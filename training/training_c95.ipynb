{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d84912c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Loading IA Weights...\n",
      "   Loaded IA for 6413 terms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START OFFICIAL TRAINING V2 (C95 MFO | Seed: 42)\n",
      "   Using EMA Decay: 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:09<00:00, 26.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss: 136.6985 | Val Loss: 587.4654 | EMA F-max: 0.0612\n",
      "MF: 0.026562336832284927 BP: 0.0501042902469635 CC: 0.10678490251302719\n",
      "    Saved Best EMA Model (proxy F-max: 0.0612)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 33.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 | Train Loss: 62.9329 | Val Loss: 336.8705 | EMA F-max: 0.3520\n",
      "MF: 0.4032348096370697 BP: 0.24300503730773926 CC: 0.4097551703453064\n",
      "    Saved Best EMA Model (proxy F-max: 0.3520)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 34.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 | Train Loss: 59.0222 | Val Loss: 173.2320 | EMA F-max: 0.3922\n",
      "MF: 0.42817485332489014 BP: 0.2857500910758972 CC: 0.46279221773147583\n",
      "    Patience: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 34.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 | Train Loss: 56.8788 | Val Loss: 100.1480 | EMA F-max: 0.4181\n",
      "MF: 0.4554789364337921 BP: 0.30779826641082764 CC: 0.4909173846244812\n",
      "    Saved Best EMA Model (proxy F-max: 0.3874)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 33.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 | Train Loss: 55.4363 | Val Loss: 72.7233 | EMA F-max: 0.4441\n",
      "MF: 0.48857492208480835 BP: 0.3270297944545746 CC: 0.5166446566581726\n",
      "    Saved Best EMA Model (proxy F-max: 0.4181)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 35.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 | Train Loss: 54.2737 | Val Loss: 62.8351 | EMA F-max: 0.4651\n",
      "MF: 0.5134793519973755 BP: 0.34440097212791443 CC: 0.5374487042427063\n",
      "    Saved Best EMA Model (proxy F-max: 0.4424)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 32.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 | Train Loss: 53.3485 | Val Loss: 58.5686 | EMA F-max: 0.4822\n",
      "MF: 0.5347944498062134 BP: 0.3583384156227112 CC: 0.5534701943397522\n",
      "    Saved Best EMA Model (proxy F-max: 0.4638)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 34.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 | Train Loss: 52.5392 | Val Loss: 56.2894 | EMA F-max: 0.4961\n",
      "MF: 0.5534444451332092 BP: 0.370866060256958 CC: 0.5639670491218567\n",
      "    Saved Best EMA Model (proxy F-max: 0.4811)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 34.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 | Train Loss: 51.6688 | Val Loss: 54.8128 | EMA F-max: 0.5065\n",
      "MF: 0.5669605731964111 BP: 0.3796241283416748 CC: 0.5730208158493042\n",
      "    Saved Best EMA Model (proxy F-max: 0.4949)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 34.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train Loss: 50.6988 | Val Loss: 53.7375 | EMA F-max: 0.5145\n",
      "MF: 0.576317548751831 BP: 0.38816016912460327 CC: 0.5790892839431763\n",
      "    Saved Best EMA Model (proxy F-max: 0.5057)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 34.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 | Train Loss: 49.7653 | Val Loss: 52.9718 | EMA F-max: 0.5221\n",
      "MF: 0.5867264270782471 BP: 0.3952581584453583 CC: 0.5843107104301453\n",
      "    Saved Best EMA Model (proxy F-max: 0.5144)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 35.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 | Train Loss: 48.7291 | Val Loss: 52.4399 | EMA F-max: 0.5285\n",
      "MF: 0.5959430932998657 BP: 0.400628924369812 CC: 0.5888416171073914\n",
      "    Saved Best EMA Model (proxy F-max: 0.5217)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 34.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 | Train Loss: 47.7368 | Val Loss: 52.1093 | EMA F-max: 0.5336\n",
      "MF: 0.6030508875846863 BP: 0.4057435393333435 CC: 0.591980516910553\n",
      "    Saved Best EMA Model (proxy F-max: 0.5281)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 34.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 | Train Loss: 46.6006 | Val Loss: 51.9380 | EMA F-max: 0.5379\n",
      "MF: 0.6084147691726685 BP: 0.4096858501434326 CC: 0.5955181121826172\n",
      "    Saved Best EMA Model (proxy F-max: 0.5333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 34.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 | Train Loss: 45.4965 | Val Loss: 51.8894 | EMA F-max: 0.5409\n",
      "MF: 0.6118411421775818 BP: 0.41235682368278503 CC: 0.5983776450157166\n",
      "    Saved Best EMA Model (proxy F-max: 0.5374)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 35.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 | Train Loss: 44.3022 | Val Loss: 51.9515 | EMA F-max: 0.5434\n",
      "MF: 0.6151878833770752 BP: 0.4148193597793579 CC: 0.600284993648529\n",
      "    Saved Best EMA Model (proxy F-max: 0.5407)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 34.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 | Train Loss: 43.1928 | Val Loss: 52.1022 | EMA F-max: 0.5453\n",
      "MF: 0.6175118684768677 BP: 0.4167115092277527 CC: 0.6015477776527405\n",
      "    Saved Best EMA Model (proxy F-max: 0.5432)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 35.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 | Train Loss: 42.0196 | Val Loss: 52.3326 | EMA F-max: 0.5463\n",
      "MF: 0.6192262172698975 BP: 0.4176892936229706 CC: 0.6018440127372742\n",
      "    Saved Best EMA Model (proxy F-max: 0.5450)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 34.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 | Train Loss: 40.8864 | Val Loss: 52.6116 | EMA F-max: 0.5472\n",
      "MF: 0.6204292178153992 BP: 0.41845881938934326 CC: 0.6026325225830078\n",
      "    Saved Best EMA Model (proxy F-max: 0.5462)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 34.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 | Train Loss: 39.7551 | Val Loss: 52.9515 | EMA F-max: 0.5479\n",
      "MF: 0.6210730671882629 BP: 0.4187500774860382 CC: 0.60383540391922\n",
      "    Saved Best EMA Model (proxy F-max: 0.5471)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 34.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 | Train Loss: 38.7544 | Val Loss: 53.3294 | EMA F-max: 0.5480\n",
      "MF: 0.6214264035224915 BP: 0.41843876242637634 CC: 0.6041554808616638\n",
      "    Saved Best EMA Model (proxy F-max: 0.5477)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 34.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 | Train Loss: 37.8160 | Val Loss: 53.7394 | EMA F-max: 0.5476\n",
      "MF: 0.6209926009178162 BP: 0.418040007352829 CC: 0.6037581562995911\n",
      "    Saved Best EMA Model (proxy F-max: 0.5478)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 34.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 | Train Loss: 36.9200 | Val Loss: 54.1825 | EMA F-max: 0.5469\n",
      "MF: 0.6197471022605896 BP: 0.4172874689102173 CC: 0.6037395596504211\n",
      "    Patience: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 34.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 | Train Loss: 36.2521 | Val Loss: 54.6315 | EMA F-max: 0.5458\n",
      "MF: 0.6183481216430664 BP: 0.41663169860839844 CC: 0.602554202079773\n",
      "    Patience: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 35.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 | Train Loss: 35.6059 | Val Loss: 55.0818 | EMA F-max: 0.5454\n",
      "MF: 0.6181322932243347 BP: 0.41574472188949585 CC: 0.6023028492927551\n",
      "    Patience: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 34.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 | Train Loss: 35.1198 | Val Loss: 55.5036 | EMA F-max: 0.5444\n",
      "MF: 0.6168670654296875 BP: 0.4144900143146515 CC: 0.6018363833427429\n",
      "    Patience: 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 258/258 [00:07<00:00, 35.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 | Train Loss: 34.7647 | Val Loss: 55.8992 | EMA F-max: 0.5437\n",
      "MF: 0.6161227226257324 BP: 0.41374918818473816 CC: 0.6012502908706665\n",
      "    Patience: 5/5\n",
      " Early Stopping Triggered!\n",
      "\n",
      "Finished Training. Best Model saved at: c95_mfo_best_ema.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "# =========================================================\n",
    "# ⚙️ CONFIG (BEST PARAMS + NEW SETTINGS)\n",
    "# =========================================================\n",
    "CONFIG = {\n",
    "    # --- Paths ---\n",
    "    'TRAIN_IDS': '/root/CAFA6data/c95/train_ids_C95_split.npy',\n",
    "    'VAL_IDS': '/root/CAFA6data/c95/val_ids_C95_split.npy',\n",
    "    'TARGETS_PKL': '/root/CAFA6data/c95/train_targets_C95.pkl',\n",
    "    'EMBED_DIR': '/root/CAFA6data/cafa6-embeds',\n",
    "    'IA_FILE': '/root/CAFA6data/IA.tsv', \n",
    "    'VOCAB_FILE': '/root/CAFA6data/c95/vocab_C95_remove.csv', \n",
    "    \n",
    "    # --- Training Hypers ---\n",
    "    'input_dim': 1280,\n",
    "    'batch_size': 256,      \n",
    "    'device': \"cuda\",\n",
    "    'epochs': 30,           \n",
    "    'lr_max': 4e-4,         \n",
    "    'seed': 42,            \n",
    "    'patience': 5,          \n",
    "    'ema_decay': 0.999,     \n",
    "      \n",
    "    'TAXON_PKL': '/root/cafa6/preprocessing/taxon_mapping_K_Species.pkl',\n",
    "    \n",
    "    # --- Best Params found ---\n",
    "    'gamma_neg': 2.5,\n",
    "    'gamma_pos': 0.0,\n",
    "    'clip': 0.01,\n",
    "    \n",
    "    # --- Output ---\n",
    "    'save_path': 'c95_mfo_best_ema.pth',\n",
    "}\n",
    "\n",
    "# =========================================================\n",
    "# 1. UTILS: SEED & EMA & IA\n",
    "# =========================================================\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Seed set to {seed}\")\n",
    "\n",
    "class ModelEMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.module = copy.deepcopy(model).eval()\n",
    "\n",
    "        self.module.load_state_dict(model.state_dict())\n",
    "\n",
    "        self.decay = decay\n",
    "\n",
    "        for p in self.module.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        msd = model.state_dict()\n",
    "        for k, ema_v in self.module.state_dict().items():\n",
    "            model_v = msd[k].detach()\n",
    "            ema_v.copy_(ema_v * self.decay + (1.0 - self.decay) * model_v)\n",
    "\n",
    "def load_ia_weights(vocab_path, ia_path):\n",
    "    print(\"Loading IA Weights...\")\n",
    "    try:\n",
    "        vocab_df = pd.read_csv(vocab_path)\n",
    "        term_list = vocab_df['term'].tolist()\n",
    "        term_to_idx = {t: i for i, t in enumerate(term_list)}\n",
    "        num_classes = len(term_list)\n",
    "        \n",
    "        ia_weights = np.ones(num_classes, dtype=np.float32)\n",
    "        if os.path.exists(ia_path):\n",
    "            with open(ia_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) >= 2:\n",
    "                        term, ia = parts[0], float(parts[1])\n",
    "                        if term in term_to_idx:\n",
    "                            ia_weights[term_to_idx[term]] = ia\n",
    "            print(f\"   Loaded IA for {num_classes} terms.\")\n",
    "        else:\n",
    "            print(f\"   IA file not found. Using weights=1.0\")\n",
    "        return ia_weights\n",
    "    except Exception as e:\n",
    "        print(f\"   Error loading IA: {e}\")\n",
    "        return np.ones(1, dtype=np.float32)\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, ids_file, targets_pkl, embed_dir, taxon_pkl):\n",
    "        self.ids = np.load(ids_file)\n",
    "        self.embeds = np.load(os.path.join(embed_dir, \"train_embeds.npy\"))\n",
    "        \n",
    "        with open(os.path.join(embed_dir, \"train_ids.txt\")) as f:\n",
    "            all_ids = [x.strip() for x in f]\n",
    "        self.id_to_idx = {pid: i for i, pid in enumerate(all_ids)}\n",
    "        \n",
    "        with open(targets_pkl, 'rb') as f:\n",
    "            self.targets = pickle.load(f)\n",
    "        \n",
    "        max_idx = 0\n",
    "        for v in self.targets.values():\n",
    "            if v: max_idx = max(max_idx, max(v))\n",
    "        self.num_classes = max_idx + 1\n",
    "        \n",
    "        with open(taxon_pkl, 'rb') as f:\n",
    "            tax_data = pickle.load(f)\n",
    "\n",
    "        self.prot_to_taxon = tax_data[\"prot_to_taxon_idx\"]\n",
    "        self.num_taxa = tax_data[\"num_taxa_classes\"]\n",
    "        self.default_tax = self.num_taxa - 1  \n",
    "\n",
    "    def __len__(self): return len(self.ids)\n",
    "    def __getitem__(self, i):\n",
    "        pid = self.ids[i]\n",
    "        \n",
    "        # Embedding\n",
    "        emb_idx = self.id_to_idx.get(pid)\n",
    "        feat = torch.tensor(self.embeds[emb_idx], dtype=torch.float32) if emb_idx is not None else torch.zeros(1280)\n",
    "        \n",
    "        target = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        if pid in self.targets:\n",
    "            inds = self.targets[pid]\n",
    "            if len(inds) > 0:\n",
    "                inds = [int(x) for x in inds]\n",
    "                target[inds] = 1.0\n",
    "                \n",
    "        tax_id = self.prot_to_taxon.get(pid, self.default_tax)\n",
    "        tax_id = torch.tensor(tax_id, dtype=torch.long)\n",
    "        \n",
    "        return feat, tax_id, target\n",
    "\n",
    "# =========================================================\n",
    "# 2. MODEL & LOSS\n",
    "# =========================================================\n",
    "class AsymmetricLossOptimized(nn.Module):\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=0, clip=0.05, eps=1e-7):\n",
    "        super().__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "        \n",
    "        x_sigmoid = torch.sigmoid(x)\n",
    "        xs_pos = x_sigmoid\n",
    "        xs_neg = 1 - x_sigmoid\n",
    "        if self.clip > 0:\n",
    "            xs_neg = (xs_neg + self.clip + self.eps).clamp(max=1.0)\n",
    "        else:\n",
    "            xs_neg = (xs_neg + self.eps).clamp(max=1.0)\n",
    "        \n",
    "        pt = y * xs_pos + (1 - y) * xs_neg\n",
    "        log_pt = torch.log(pt.clamp(min=self.eps, max=1.0))\n",
    "        \n",
    "        pos_weight = (1 - xs_pos) ** self.gamma_pos\n",
    "        neg_weight = (1 - xs_neg) ** self.gamma_neg\n",
    "        \n",
    "        weighted_loss = - (pos_weight * log_pt * y + neg_weight * log_pt * (1-y))\n",
    "        return weighted_loss.sum() / x.size(0)\n",
    "\n",
    "class WideProteinMLP_WithTaxon(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_taxa, taxon_dim=64,\n",
    "                 hidden_dims=[4096, 4096], dropout=0.4):\n",
    "        super().__init__()\n",
    "\n",
    "        # Normalize protein embedding\n",
    "        self.seq_norm = nn.LayerNorm(input_dim)\n",
    "\n",
    "        # Taxon branch\n",
    "        self.taxon_embedding = nn.Embedding(num_taxa, taxon_dim)\n",
    "        self.taxon_norm = nn.LayerNorm(taxon_dim)\n",
    "\n",
    "        # UNK taxon\n",
    "        self.unk_idx = num_taxa - 1\n",
    "        with torch.no_grad():\n",
    "            self.taxon_embedding.weight[self.unk_idx].zero_()\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"unk_vec\", \n",
    "            self.taxon_embedding.weight[self.unk_idx].clone().detach()\n",
    "        )\n",
    " \n",
    "        combined_dim = input_dim + taxon_dim\n",
    "\n",
    "        layers = []\n",
    "        prev = combined_dim\n",
    "\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        \n",
    "        layers.append(nn.Linear(prev, num_classes))\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, seq_emb, taxon_id):\n",
    "        seq = self.seq_norm(seq_emb)\n",
    "        tax = self.taxon_norm(self.taxon_embedding(taxon_id))\n",
    "        x = torch.cat([seq, tax], dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "def gpu_calculate_fmax(preds_t, targets_t, ia_weights_np, device):\n",
    "    if preds_t.numel() == 0: return 0.0\n",
    "    ia_t = torch.from_numpy(ia_weights_np.astype(np.float32)).to(device)\n",
    "    w = ia_t.unsqueeze(0)\n",
    "    true_sum = (targets_t * w).sum(dim=1)\n",
    "    valid_mask = true_sum > 0\n",
    "    if valid_mask.sum().item() == 0: return 0.0\n",
    "    \n",
    "    p_sub = preds_t[valid_mask]\n",
    "    t_sub = targets_t[valid_mask]\n",
    "    w_sub = w\n",
    "    w_true_sub = true_sum[valid_mask]\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    thresholds = torch.linspace(0.0, 1.0, 51, device=device)\n",
    "    for tau in thresholds:\n",
    "        cut = (p_sub >= tau).float()\n",
    "        tp = ((cut * t_sub) * w_sub).sum(dim=1)\n",
    "        pred_sum = (cut * w_sub).sum(dim=1)\n",
    "        prec = torch.where(pred_sum != 0, tp / pred_sum, torch.zeros_like(tp))\n",
    "        rec = torch.where(w_true_sub != 0, tp / w_true_sub, torch.zeros_like(tp))\n",
    "        avg_p = prec.mean(); avg_r = rec.mean()\n",
    "        denom = (avg_p + avg_r)\n",
    "        if denom > 0: f1 = (2.0 * avg_p * avg_r / denom).item()\n",
    "        else: f1 = 0.0\n",
    "        if f1 > best_f1: best_f1 = f1\n",
    "    return float(best_f1)\n",
    "\n",
    "def gpu_fmax_split(preds_t, targets_t, ia_t):\n",
    "    \"\"\"\n",
    "    preds_t: Tensor [N, C]\n",
    "    targets_t: Tensor [N, C]\n",
    "    ia_t: Tensor [C]\n",
    "    \"\"\"\n",
    "    device = preds_t.device\n",
    "    w = ia_t.unsqueeze(0)                # [1, C]\n",
    "    true_sum = (targets_t * w).sum(1)    # [N]\n",
    "\n",
    "    valid = true_sum > 0\n",
    "    if valid.sum() == 0:\n",
    "        return 0.0\n",
    "\n",
    "    p = preds_t[valid]     # [Nv, C]\n",
    "    t = targets_t[valid]\n",
    "    ts = true_sum[valid]\n",
    "\n",
    "    best = 0.0\n",
    "    thresholds = torch.linspace(0, 1, 101, device=device)\n",
    "\n",
    "    for tau in thresholds:\n",
    "        cut = (p >= tau).float()\n",
    "\n",
    "        tp = (cut * t * w).sum(1)\n",
    "        pred_sum = (cut * w).sum(1)\n",
    "\n",
    "        prec = torch.where(pred_sum > 0, tp / pred_sum, torch.zeros_like(tp))\n",
    "        rec = torch.where(ts > 0, tp / ts, torch.zeros_like(tp))\n",
    "\n",
    "        avg_p = prec.mean()\n",
    "        avg_r = rec.mean()\n",
    "        denom = avg_p + avg_r\n",
    "\n",
    "        f1 = torch.where(denom > 0, 2 * avg_p * avg_r / denom, torch.tensor(0., device=device))\n",
    "        best = max(best, f1.item())\n",
    "\n",
    "    return best\n",
    "\n",
    "# =========================================================\n",
    "# 3. TRAINING LOOP\n",
    "# =========================================================\n",
    "def train_epoch(model, ema_model, loader, loss_fn, optimizer, scaler, scheduler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    \n",
    "    for feats, tax_id, labels in pbar:\n",
    "        feats = feats.to(device)\n",
    "        tax_id = tax_id.to(device)     \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            logits = model(feats, tax_id)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "        if not torch.isfinite(loss):\n",
    "            print(\"!!! LOSS NaN or Inf detected.\")\n",
    "            print(\"max_logit\", logits.max().item(), \"min_logit\", logits.min().item())\n",
    "            torch.save(model.state_dict(), \"crash_before_nan.pth\")\n",
    "            raise RuntimeError(\"NaN in loss\")\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if ema_model is not None:\n",
    "            ema_model.update(model)\n",
    "        \n",
    "        running_loss += float(loss.item())\n",
    "\n",
    "        del feats, tax_id, labels, logits, loss\n",
    "        \n",
    "    return running_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_split_go_gpu(model, loader, loss_fn, device, ia_weights, vocab_df):\n",
    "    model.eval()\n",
    "\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for feats, tax_id, labels in loader:\n",
    "        feats = feats.to(device)\n",
    "        tax_id = tax_id.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(feats, tax_id)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds_list.append(torch.sigmoid(logits))\n",
    "        targets_list.append(labels)\n",
    "\n",
    "    preds_t = torch.cat(preds_list, dim=0).to(device)\n",
    "    targets_t = torch.cat(targets_list, dim=0).to(device)\n",
    "    val_loss = total_loss / len(loader)\n",
    "\n",
    "    # Convert IA to torch\n",
    "    ia_t = torch.tensor(ia_weights, dtype=torch.float32, device=device)\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    for asp in [\"MFO\", \"BPO\", \"CCO\"]:\n",
    "        cols = vocab_df.index[vocab_df[\"aspect\"] == asp].tolist()\n",
    "        if not cols:\n",
    "            continue\n",
    "\n",
    "        scores[asp] = gpu_fmax_split(\n",
    "            preds_t[:, cols],\n",
    "            targets_t[:, cols],\n",
    "            ia_t[cols]\n",
    "        )\n",
    "\n",
    "    avg_fmax = np.mean(list(scores.values()))\n",
    "\n",
    "    return val_loss, avg_fmax, scores\n",
    "\n",
    "# =========================================================\n",
    "# MAIN\n",
    "# =========================================================\n",
    "def main():\n",
    "    # 1. Init Seed\n",
    "    seed_everything(CONFIG['seed'])\n",
    "    \n",
    "    # 2. Load Data & IA\n",
    "    ia_weights = load_ia_weights(CONFIG['VOCAB_FILE'], CONFIG['IA_FILE'])\n",
    "    train_ds = ProteinDataset(CONFIG['TRAIN_IDS'], CONFIG['TARGETS_PKL'], CONFIG['EMBED_DIR'], CONFIG['TAXON_PKL'])\n",
    "    val_ds    = ProteinDataset(CONFIG['VAL_IDS'], CONFIG['TARGETS_PKL'], CONFIG['EMBED_DIR'], CONFIG['TAXON_PKL'])\n",
    "    \n",
    "    if len(ia_weights) != train_ds.num_classes:\n",
    "        print(f\"Resizing IA: {len(ia_weights)} -> {train_ds.num_classes}\")\n",
    "        new_ia = np.ones(train_ds.num_classes, dtype=np.float32)\n",
    "        min_len = min(len(ia_weights), train_ds.num_classes)\n",
    "        new_ia[:min_len] = ia_weights[:min_len]\n",
    "        ia_weights = new_ia\n",
    "\n",
    "    CONFIG['output_classes'] = train_ds.num_classes\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, \n",
    "                              num_workers=16, pin_memory=True, persistent_workers=False, prefetch_factor=2)\n",
    "    val_loader    = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False, \n",
    "                               num_workers=16, pin_memory=True, persistent_workers=False, prefetch_factor=2)\n",
    "\n",
    "    # 3. Init Model & EMA\n",
    "    print(f\"START OFFICIAL TRAINING V2 (C95 MFO | Seed: {CONFIG['seed']})\")\n",
    "    model = WideProteinMLP_WithTaxon(\n",
    "        input_dim=CONFIG['input_dim'],\n",
    "        num_classes=CONFIG['output_classes'],\n",
    "        num_taxa=train_ds.num_taxa,\n",
    "        taxon_dim=64,\n",
    "        hidden_dims=[4096, 4096],    \n",
    "        dropout=0.4\n",
    "    ).to(CONFIG['device'])\n",
    "    \n",
    "    ema_model = ModelEMA(model, decay=CONFIG['ema_decay'])\n",
    "    print(f\"   Using EMA Decay: {CONFIG['ema_decay']}\")\n",
    "    \n",
    "    loss_fn = AsymmetricLossOptimized(gamma_neg=CONFIG['gamma_neg'], gamma_pos=CONFIG['gamma_pos'], clip=CONFIG['clip'])\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr_max'], weight_decay=0.02)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=CONFIG['lr_max'], epochs=CONFIG['epochs'], \n",
    "        steps_per_epoch=len(train_loader), pct_start=0.3, div_factor=25, final_div_factor=1e4\n",
    "    )\n",
    "\n",
    "    # 4. Training Loop \n",
    "    best_proxy_fmax = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    fmax_buffer = deque(maxlen=3)   # window = 3 \n",
    "    \n",
    "    vocab_df = pd.read_csv(CONFIG['VOCAB_FILE'])\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        tr_loss = train_epoch(model, ema_model, train_loader, loss_fn, optimizer, scaler, scheduler, CONFIG['device'])\n",
    "        \n",
    "        # Validate EMA Model (Usually better)\n",
    "        val_loss, val_fmax, go_scores = validate_split_go_gpu(\n",
    "            ema_model.module,\n",
    "            val_loader,\n",
    "            loss_fn,\n",
    "            CONFIG[\"device\"],\n",
    "            ia_weights,\n",
    "            vocab_df\n",
    "        )\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{CONFIG['epochs']} | Train Loss: {tr_loss:.4f} | Val Loss: {val_loss:.4f} | EMA F-max: {val_fmax:.4f}\")\n",
    "        print(\"MF:\", go_scores[\"MFO\"], \"BP:\", go_scores[\"BPO\"], \"CC:\", go_scores[\"CCO\"])\n",
    "         #  cập nhật buffer\n",
    "        fmax_buffer.append(val_fmax)\n",
    "\n",
    "        if len(fmax_buffer) == fmax_buffer.maxlen:\n",
    "            proxy_fmax = float(np.mean(fmax_buffer))  \n",
    "        else:\n",
    "            proxy_fmax = val_fmax\n",
    "            \n",
    "        if proxy_fmax > best_proxy_fmax:\n",
    "            best_proxy_fmax = proxy_fmax\n",
    "            patience_counter = 0\n",
    "            torch.save(ema_model.module.state_dict(), CONFIG['save_path'])\n",
    "            print(f\"    Saved Best EMA Model (proxy F-max: {best_proxy_fmax:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"    Patience: {patience_counter}/{CONFIG['patience']}\")\n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(\" Early Stopping Triggered!\")\n",
    "                break\n",
    "            \n",
    "    print(f\"\\nFinished Training. Best Model saved at: {CONFIG['save_path']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a99246f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Seed set to 42\n",
      "Merging Train + Val data for FULL TRAINING...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading IA Weights...\n",
      " FINAL TRAINING START (full data, 25 epochs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:09<00:00, 33.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Loss: 122.6740 | LR: 2.76e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:10<00:00, 32.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 | Loss: 61.7423 | LR: 6.09e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:10<00:00, 29.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 | Loss: 58.1034 | LR: 1.12e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:08<00:00, 36.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 | Loss: 56.1127 | LR: 1.75e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:09<00:00, 35.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 | Loss: 54.7940 | LR: 2.41e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:08<00:00, 36.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 | Loss: 53.7238 | LR: 3.04e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:09<00:00, 35.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 | Loss: 52.8780 | LR: 3.55e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:08<00:00, 36.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 | Loss: 52.0805 | LR: 3.88e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:08<00:00, 37.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 | Loss: 51.2514 | LR: 4.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:09<00:00, 35.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Loss: 50.3157 | LR: 3.98e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:09<00:00, 35.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 | Loss: 49.3508 | LR: 3.91e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:08<00:00, 37.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 | Loss: 48.3827 | LR: 3.80e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:08<00:00, 36.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 | Loss: 47.3112 | LR: 3.65e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:08<00:00, 36.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 | Loss: 46.2920 | LR: 3.47e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:08<00:00, 36.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 | Loss: 45.1066 | LR: 3.25e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:09<00:00, 35.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 | Loss: 43.9822 | LR: 3.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:09<00:00, 35.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 | Loss: 42.8208 | LR: 2.73e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:08<00:00, 37.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 | Loss: 41.7068 | LR: 2.44e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:08<00:00, 36.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 | Loss: 40.5587 | LR: 2.15e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:08<00:00, 36.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 | Loss: 39.4975 | LR: 1.85e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:09<00:00, 34.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 | Loss: 38.4900 | LR: 1.55e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 322/322 [00:08<00:00, 35.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 | Loss: 37.5596 | LR: 1.27e-04\n",
      " Forced stop at epoch 22 (observed F-max peak)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =========================================================\n",
    "#  CONFIG (CHẾ ĐỘ FULL TRAIN)\n",
    "# =========================================================\n",
    "CONFIG = {\n",
    "    # --- Paths ---\n",
    "    'TRAIN_IDS': '/root/CAFA6data/c95/train_ids_C95_split.npy',\n",
    "    'VAL_IDS': '/root/CAFA6data/c95/val_ids_C95_split.npy',\n",
    "    'TARGETS_PKL': '/root/CAFA6data/c95/train_targets_C95.pkl',\n",
    "    'EMBED_DIR': '/root/CAFA6data/cafa6-embeds',\n",
    "    'IA_FILE': '/root/CAFA6data/IA.tsv', \n",
    "    'VOCAB_FILE': '/root/CAFA6data/c95/vocab_C95_remove.csv', \n",
    "    'TAXON_PKL': '/root/cafa6/preprocessing/taxon_mapping_K_Species.pkl',\n",
    "    \n",
    "    # --- Training Hypers ---\n",
    "    'input_dim': 1280,\n",
    "    'batch_size': 256,      \n",
    "    'device': \"cuda\",\n",
    "          \n",
    "    'lr_max': 4e-4,        \n",
    "    'seed': 42,            \n",
    "    'ema_decay': 0.999,   \n",
    "    \n",
    "    'epochs': 30,\n",
    "    'stop_epoch': 22,\n",
    "    \n",
    "    # --- Params  ---\n",
    "    'gamma_neg': 2.5,\n",
    "    'gamma_pos': 0.0,\n",
    "    'clip': 0.01,\n",
    "    \n",
    "}\n",
    "\n",
    "# =========================================================\n",
    "# 1. UTILS\n",
    "# =========================================================\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\" Seed set to {seed}\")\n",
    "\n",
    "class ModelEMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.module = copy.deepcopy(model).eval()\n",
    "        self.module.load_state_dict(model.state_dict())\n",
    "        self.decay = decay\n",
    "        for p in self.module.parameters(): p.requires_grad_(False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        msd = model.state_dict()\n",
    "        for k, ema_v in self.module.state_dict().items():\n",
    "            model_v = msd[k].detach()\n",
    "            ema_v.copy_(ema_v * self.decay + (1.0 - self.decay) * model_v)\n",
    "\n",
    "def load_ia_weights(vocab_path, ia_path):\n",
    "    print(\" Loading IA Weights...\")\n",
    "    try:\n",
    "        vocab_df = pd.read_csv(vocab_path)\n",
    "        term_list = vocab_df['term'].tolist()\n",
    "        term_to_idx = {t: i for i, t in enumerate(term_list)}\n",
    "        num_classes = len(term_list)\n",
    "        ia_weights = np.ones(num_classes, dtype=np.float32)\n",
    "        if os.path.exists(ia_path):\n",
    "            with open(ia_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) >= 2:\n",
    "                        term, ia = parts[0], float(parts[1])\n",
    "                        if term in term_to_idx:\n",
    "                            ia_weights[term_to_idx[term]] = ia\n",
    "        return ia_weights\n",
    "    except Exception:\n",
    "        return np.ones(1, dtype=np.float32)\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, ids_source, targets_pkl, embed_dir, taxon_pkl):\n",
    "        \n",
    "        if isinstance(ids_source, str):\n",
    "            self.ids = np.load(ids_source)\n",
    "        else:\n",
    "            self.ids = ids_source \n",
    "            \n",
    "        # Dùng mmap_mode='r' để tiết kiệm RAM\n",
    "        self.embeds = np.load(os.path.join(embed_dir, \"train_embeds.npy\"), mmap_mode='r')\n",
    "        \n",
    "        with open(os.path.join(embed_dir, \"train_ids.txt\")) as f:\n",
    "            all_ids = [x.strip() for x in f]\n",
    "        self.id_to_idx = {pid: i for i, pid in enumerate(all_ids)}\n",
    "        \n",
    "        with open(targets_pkl, 'rb') as f:\n",
    "            self.targets = pickle.load(f)\n",
    "        \n",
    "        max_idx = 0\n",
    "        for v in self.targets.values():\n",
    "            if v: max_idx = max(max_idx, max(v))\n",
    "        self.num_classes = max_idx + 1\n",
    "        \n",
    "        with open(taxon_pkl, 'rb') as f:\n",
    "            tax_data = pickle.load(f)\n",
    "        self.prot_to_taxon = tax_data[\"prot_to_taxon_idx\"]\n",
    "        self.num_taxa = tax_data[\"num_taxa_classes\"]\n",
    "        self.default_tax = self.num_taxa - 1\n",
    "\n",
    "    def __len__(self): return len(self.ids)\n",
    "    def __getitem__(self, i):\n",
    "        pid = self.ids[i]\n",
    "        emb_idx = self.id_to_idx.get(pid)\n",
    "        if emb_idx is not None:\n",
    "             # Copy vào RAM\n",
    "            feat = torch.tensor(self.embeds[emb_idx], dtype=torch.float32)\n",
    "        else:\n",
    "            feat = torch.zeros(1280)\n",
    "        \n",
    "        target = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        if pid in self.targets:\n",
    "            inds = self.targets[pid]\n",
    "            if len(inds) > 0:\n",
    "                target[inds] = 1.0\n",
    "                \n",
    "        tax_id = self.prot_to_taxon.get(pid, self.default_tax)\n",
    "        return feat, torch.tensor(tax_id, dtype=torch.long), target\n",
    "\n",
    "# =========================================================\n",
    "# 2. MODEL & LOSS\n",
    "# =========================================================\n",
    "class AsymmetricLossOptimized(nn.Module):\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=0, clip=0.05, eps=1e-7):\n",
    "        super().__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.eps = eps\n",
    "    def forward(self, x, y):\n",
    "        x = x.float(); y = y.float()\n",
    "        x_sigmoid = torch.sigmoid(x)\n",
    "        xs_pos = x_sigmoid\n",
    "        xs_neg = (1 - x_sigmoid + self.clip + self.eps).clamp(max=1.0)\n",
    "        pt = y * xs_pos + (1 - y) * xs_neg\n",
    "        log_pt = torch.log(pt.clamp(min=self.eps, max=1.0))\n",
    "        weight = y*(1-xs_pos)**self.gamma_pos + (1-y)*(1-xs_neg)**self.gamma_neg\n",
    "        loss = - weight * log_pt\n",
    "        return loss.sum() / x.size(0)\n",
    "\n",
    "class WideProteinMLP_WithTaxon(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_taxa, taxon_dim=64, hidden_dims=[4096, 4096], dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.seq_norm = nn.LayerNorm(input_dim)\n",
    "        self.taxon_embedding = nn.Embedding(num_taxa, taxon_dim)\n",
    "        self.taxon_norm = nn.LayerNorm(taxon_dim)\n",
    "        with torch.no_grad(): self.taxon_embedding.weight[num_taxa-1].zero_()\n",
    "        layers = []\n",
    "        prev = input_dim + taxon_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, num_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, seq, tax):\n",
    "        x = torch.cat([self.seq_norm(seq), self.taxon_norm(self.taxon_embedding(tax))], dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def train_epoch(model, ema_model, loader, loss_fn, optimizer, scaler, scheduler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(loader, desc=\"Training Full\")\n",
    "    \n",
    "    for feats, tax_id, labels in pbar:\n",
    "        feats = feats.to(device)\n",
    "        tax_id = tax_id.to(device)     \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            logits = model(feats, tax_id)\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        scaler.step(optimizer); scaler.update(); scheduler.step()\n",
    "        \n",
    "        if ema_model: ema_model.update(model)\n",
    "        \n",
    "        running_loss += float(loss.item())\n",
    "        del feats, tax_id, labels, logits, loss\n",
    "    \n",
    "    return running_loss / len(loader)\n",
    "\n",
    "# =========================================================\n",
    "# MAIN\n",
    "# =========================================================\n",
    "def main():\n",
    "    seed_everything(CONFIG['seed'])\n",
    "\n",
    "    print(\"Merging Train + Val data for FULL TRAINING...\")\n",
    "    ids_train = np.load(CONFIG['TRAIN_IDS'])\n",
    "    ids_val   = np.load(CONFIG['VAL_IDS'])\n",
    "    ids_full  = np.concatenate([ids_train, ids_val])\n",
    "\n",
    "    full_ds = ProteinDataset(ids_full, CONFIG['TARGETS_PKL'], CONFIG['EMBED_DIR'], CONFIG['TAXON_PKL'])\n",
    "    full_loader = DataLoader(full_ds, batch_size=CONFIG['batch_size'], shuffle=True,\n",
    "                            num_workers=16, pin_memory=True, persistent_workers=False)\n",
    "\n",
    "    # Load IA Weights cho metric\n",
    "    ia_weights = load_ia_weights(CONFIG['VOCAB_FILE'], CONFIG['IA_FILE'])\n",
    "    if len(ia_weights) != full_ds.num_classes:\n",
    "        new_ia = np.ones(full_ds.num_classes, dtype=np.float32)\n",
    "        min_len = min(len(ia_weights), full_ds.num_classes)\n",
    "        new_ia[:min_len] = ia_weights[:min_len]\n",
    "        ia_weights = new_ia\n",
    "\n",
    "    model = WideProteinMLP_WithTaxon(\n",
    "        input_dim=CONFIG['input_dim'],\n",
    "        num_classes=full_ds.num_classes,\n",
    "        num_taxa=full_ds.num_taxa,\n",
    "        taxon_dim=64,\n",
    "        hidden_dims=[4096, 4096],\n",
    "        dropout=0.4\n",
    "    ).to(CONFIG['device'])\n",
    "\n",
    "    ema_model = ModelEMA(model, decay=CONFIG['ema_decay'])\n",
    "    \n",
    "    loss_fn = AsymmetricLossOptimized(\n",
    "        gamma_neg=CONFIG['gamma_neg'],\n",
    "        gamma_pos=CONFIG['gamma_pos'],\n",
    "        clip=CONFIG['clip']\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr_max'], weight_decay=0.02)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=CONFIG['lr_max'], \n",
    "        epochs=CONFIG['epochs'],                \n",
    "        steps_per_epoch=len(full_loader),\n",
    "        pct_start=0.3,\n",
    "        div_factor=25,\n",
    "        final_div_factor=1e4\n",
    "    )\n",
    "\n",
    "    print(\" FINAL TRAINING START (full data, 22 epochs)\")\n",
    "\n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        tr_loss = train_epoch(\n",
    "            model, ema_model, full_loader,\n",
    "            loss_fn, optimizer, scaler,\n",
    "            scheduler, CONFIG['device']\n",
    "        )\n",
    "        \n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{CONFIG['epochs']} | \"\n",
    "            f\"Loss: {tr_loss:.4f} | \"\n",
    "            f\"LR: {current_lr:.2e}\"\n",
    "        )\n",
    "\n",
    "        # ===============================\n",
    "        #  FORCE STOP AT BEST EPOCH\n",
    "        # ===============================\n",
    "        if epoch + 1 >= CONFIG['stop_epoch']:\n",
    "            print(\n",
    "                f\" Forced stop at epoch {epoch+1} \"\n",
    "                f\"(observed F-max peak)\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "    # Save EMA as final model\n",
    "    torch.save(ema_model.module.state_dict(), \"final_cafa6_model_c95.pth\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98cce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
