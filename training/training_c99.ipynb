{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42085747",
   "metadata": {
    "id": "42085747",
    "outputId": "a88e485a-eec7-4aaa-844d-ed4038900fa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Seed set to 42\n",
      " Loading IA Weights...\n",
      "    Loaded IA for 15582 terms.\n",
      "START OFFICIAL TRAINING V2 (C95 MFO | Seed: 42)\n",
      "   Using EMA Decay: 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:17<00:00, 19.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 67.5758 | Val Loss: 314.3646 | EMA F-max: 0.1247\n",
      "MF: 0.16382519900798798 BP: 0.09105165302753448 CC: 0.1191985160112381\n",
      "    Saved Best EMA Model (proxy F-max: 0.1247)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:16<00:00, 20.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 | Train Loss: 31.9462 | Val Loss: 122.7725 | EMA F-max: 0.3524\n",
      "MF: 0.4022637903690338 BP: 0.2591497302055359 CC: 0.3959347903728485\n",
      "    Saved Best EMA Model (proxy F-max: 0.3524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:16<00:00, 20.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 | Train Loss: 29.6347 | Val Loss: 51.9845 | EMA F-max: 0.4178\n",
      "MF: 0.45802152156829834 BP: 0.30861836671829224 CC: 0.4866268038749695\n",
      "    Patience: 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:16<00:00, 20.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 | Train Loss: 28.3631 | Val Loss: 34.8537 | EMA F-max: 0.4507\n",
      "MF: 0.4955832064151764 BP: 0.3308239281177521 CC: 0.5255696177482605\n",
      "    Saved Best EMA Model (proxy F-max: 0.4070)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:16<00:00, 20.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | Train Loss: 27.3542 | Val Loss: 30.6259 | EMA F-max: 0.4744\n",
      "MF: 0.5242636203765869 BP: 0.3494601845741272 CC: 0.54942387342453\n",
      "    Saved Best EMA Model (proxy F-max: 0.4476)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:16<00:00, 20.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 | Train Loss: 26.3936 | Val Loss: 28.9254 | EMA F-max: 0.4927\n",
      "MF: 0.5488895177841187 BP: 0.3651718199253082 CC: 0.5639460682868958\n",
      "    Saved Best EMA Model (proxy F-max: 0.4726)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:17<00:00, 20.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 | Train Loss: 25.3263 | Val Loss: 27.9263 | EMA F-max: 0.5055\n",
      "MF: 0.5656755566596985 BP: 0.378062903881073 CC: 0.572760283946991\n",
      "    Saved Best EMA Model (proxy F-max: 0.4909)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:17<00:00, 19.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 | Train Loss: 24.1579 | Val Loss: 27.3184 | EMA F-max: 0.5161\n",
      "MF: 0.5797516703605652 BP: 0.3887706398963928 CC: 0.5798185467720032\n",
      "    Saved Best EMA Model (proxy F-max: 0.5048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:19<00:00, 17.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 | Train Loss: 22.9604 | Val Loss: 27.0002 | EMA F-max: 0.5238\n",
      "MF: 0.5891242623329163 BP: 0.3962215483188629 CC: 0.5859778523445129\n",
      "    Saved Best EMA Model (proxy F-max: 0.5151)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:17<00:00, 20.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Train Loss: 21.7375 | Val Loss: 26.9346 | EMA F-max: 0.5288\n",
      "MF: 0.5949710607528687 BP: 0.40131181478500366 CC: 0.5900393724441528\n",
      "    Saved Best EMA Model (proxy F-max: 0.5229)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:17<00:00, 20.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | Train Loss: 20.4698 | Val Loss: 27.1004 | EMA F-max: 0.5313\n",
      "MF: 0.5973190665245056 BP: 0.4045393466949463 CC: 0.5919551253318787\n",
      "    Saved Best EMA Model (proxy F-max: 0.5279)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:17<00:00, 20.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | Train Loss: 19.1877 | Val Loss: 27.4687 | EMA F-max: 0.5328\n",
      "MF: 0.598224401473999 BP: 0.4061540365219116 CC: 0.5939397215843201\n",
      "    Saved Best EMA Model (proxy F-max: 0.5309)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:16<00:00, 20.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | Train Loss: 17.9695 | Val Loss: 28.0098 | EMA F-max: 0.5321\n",
      "MF: 0.5978807806968689 BP: 0.4047158360481262 CC: 0.5936333537101746\n",
      "    Saved Best EMA Model (proxy F-max: 0.5320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:17<00:00, 19.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | Train Loss: 16.7903 | Val Loss: 28.7036 | EMA F-max: 0.5303\n",
      "MF: 0.5948261618614197 BP: 0.40303143858909607 CC: 0.59291672706604\n",
      "    Patience: 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:17<00:00, 19.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 | Train Loss: 15.7645 | Val Loss: 29.5324 | EMA F-max: 0.5281\n",
      "MF: 0.5918530225753784 BP: 0.4006190896034241 CC: 0.5917574763298035\n",
      "    Patience: 2/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:17<00:00, 19.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 | Train Loss: 14.8815 | Val Loss: 30.4301 | EMA F-max: 0.5257\n",
      "MF: 0.5886651873588562 BP: 0.3979606330394745 CC: 0.590548574924469\n",
      "    Patience: 3/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:18<00:00, 18.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 | Train Loss: 14.2113 | Val Loss: 31.3588 | EMA F-max: 0.5240\n",
      "MF: 0.5860811471939087 BP: 0.3961750566959381 CC: 0.5897903442382812\n",
      "    Patience: 4/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:16<00:00, 20.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 | Train Loss: 13.6950 | Val Loss: 32.2558 | EMA F-max: 0.5220\n",
      "MF: 0.5838826894760132 BP: 0.39440321922302246 CC: 0.587813675403595\n",
      "    Patience: 5/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [00:16<00:00, 20.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 | Train Loss: 13.4050 | Val Loss: 33.0437 | EMA F-max: 0.5205\n",
      "MF: 0.5818638801574707 BP: 0.39279839396476746 CC: 0.5867449045181274\n",
      "    Patience: 6/6\n",
      " Early Stopping Triggered!\n",
      "\n",
      " Finished. Best EMA proxy F-max: 0.5320\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG (BEST PARAMS + NEW SETTINGS)\n",
    "# =========================================================\n",
    "CONFIG = {\n",
    "    # --- Paths ---\n",
    "    'TRAIN_IDS': '/root/CAFA6data/c99/train_ids_C99_split.npy',\n",
    "    'VAL_IDS': '/root/CAFA6data/c99/val_ids_C99_split.npy',\n",
    "    'TARGETS_PKL': '/root/CAFA6data/c99/train_targets_C99.pkl',\n",
    "    'EMBED_DIR': '/root/CAFA6data/cafa6-embeds',\n",
    "    'IA_FILE': '/root/CAFA6data/IA.tsv',\n",
    "    'VOCAB_FILE': '/root/CAFA6data/c99/vocab_C99_remove.csv',\n",
    "\n",
    "    'TAXON_PKL': '/root/cafa6/preprocessing/taxon_mapping_K_Species.pkl',\n",
    "\n",
    "    # --- Training Hypers ---\n",
    "    'input_dim': 1280,\n",
    "    'batch_size': 192,\n",
    "    'device': \"cuda\",\n",
    "    'epochs': 20,\n",
    "    'lr_max': 4e-4,\n",
    "    'seed': 42,\n",
    "    'patience': 6,\n",
    "    'ema_decay': 0.999,\n",
    "\n",
    "    # --- Best Params found ---\n",
    "    'gamma_neg': 4,\n",
    "    'gamma_pos': 1.0,\n",
    "    'clip': 0.03,\n",
    "\n",
    "    # --- Output ---\n",
    "    'save_path': 'c99_mfo_balanced_best_ema.pth',\n",
    "}\n",
    "\n",
    "# =========================================================\n",
    "# 1. UTILS: SEED & EMA & IA\n",
    "# =========================================================\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\" Seed set to {seed}\")\n",
    "\n",
    "class ModelEMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        # Clone structure\n",
    "        self.module = copy.deepcopy(model).eval()\n",
    "\n",
    "        # Load weights\n",
    "        self.module.load_state_dict(model.state_dict())\n",
    "\n",
    "        # EMA decay\n",
    "        self.decay = decay\n",
    "\n",
    "        # Không track gradient\n",
    "        for p in self.module.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        msd = model.state_dict()\n",
    "        for k, ema_v in self.module.state_dict().items():\n",
    "            model_v = msd[k].detach()\n",
    "            ema_v.copy_(ema_v * self.decay + (1.0 - self.decay) * model_v)\n",
    "\n",
    "def load_ia_weights(vocab_path, ia_path):\n",
    "    print(\" Loading IA Weights...\")\n",
    "    try:\n",
    "        vocab_df = pd.read_csv(vocab_path)\n",
    "        term_list = vocab_df['term'].tolist()\n",
    "        term_to_idx = {t: i for i, t in enumerate(term_list)}\n",
    "        num_classes = len(term_list)\n",
    "\n",
    "        ia_weights = np.ones(num_classes, dtype=np.float32)\n",
    "        if os.path.exists(ia_path):\n",
    "            with open(ia_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) >= 2:\n",
    "                        term, ia = parts[0], float(parts[1])\n",
    "                        if term in term_to_idx:\n",
    "                            ia_weights[term_to_idx[term]] = ia\n",
    "            print(f\"    Loaded IA for {num_classes} terms.\")\n",
    "        else:\n",
    "            print(f\"    IA file not found. Using weights=1.0\")\n",
    "        return ia_weights\n",
    "    except Exception as e:\n",
    "        print(f\"    Error loading IA: {e}\")\n",
    "        return np.ones(1, dtype=np.float32)\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, ids_file, targets_pkl, embed_dir, taxon_pkl):\n",
    "        self.ids = np.load(ids_file)\n",
    "        self.embeds = np.load(os.path.join(embed_dir, \"train_embeds.npy\"))\n",
    "\n",
    "        with open(os.path.join(embed_dir, \"train_ids.txt\")) as f:\n",
    "            all_ids = [x.strip() for x in f]\n",
    "        self.id_to_idx = {pid: i for i, pid in enumerate(all_ids)}\n",
    "\n",
    "        with open(targets_pkl, 'rb') as f:\n",
    "            self.targets = pickle.load(f)\n",
    "\n",
    "        max_idx = 0\n",
    "        for v in self.targets.values():\n",
    "            if v: max_idx = max(max_idx, max(v))\n",
    "        self.num_classes = max_idx + 1\n",
    "\n",
    "        with open(taxon_pkl, 'rb') as f:\n",
    "            tax_data = pickle.load(f)\n",
    "\n",
    "        self.prot_to_taxon = tax_data[\"prot_to_taxon_idx\"]\n",
    "        self.num_taxa = tax_data[\"num_taxa_classes\"]\n",
    "        self.default_tax = self.num_taxa - 1  # UNK taxon\n",
    "\n",
    "    def __len__(self): return len(self.ids)\n",
    "    def __getitem__(self, i):\n",
    "        pid = self.ids[i]\n",
    "\n",
    "        # Embedding\n",
    "        emb_idx = self.id_to_idx.get(pid)\n",
    "        feat = torch.tensor(self.embeds[emb_idx], dtype=torch.float32) if emb_idx is not None else torch.zeros(1280)\n",
    "\n",
    "        target = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        if pid in self.targets:\n",
    "            inds = self.targets[pid]\n",
    "            if len(inds) > 0:\n",
    "                inds = [int(x) for x in inds]\n",
    "                target[inds] = 1.0\n",
    "\n",
    "        tax_id = self.prot_to_taxon.get(pid, self.default_tax)\n",
    "        tax_id = torch.tensor(tax_id, dtype=torch.long)\n",
    "\n",
    "        return feat, tax_id, target\n",
    "\n",
    "# =========================================================\n",
    "# 2. MODEL & LOSS\n",
    "# =========================================================\n",
    "class AsymmetricLossOptimized(nn.Module):\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=0, clip=0.05, eps=1e-7):\n",
    "        super().__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "\n",
    "        x_sigmoid = torch.sigmoid(x)\n",
    "        xs_pos = x_sigmoid\n",
    "        xs_neg = 1 - x_sigmoid\n",
    "        if self.clip > 0:\n",
    "            xs_neg = (xs_neg + self.clip + self.eps).clamp(max=1.0)\n",
    "        else:\n",
    "            xs_neg = (xs_neg + self.eps).clamp(max=1.0)\n",
    "\n",
    "        pt = y * xs_pos + (1 - y) * xs_neg\n",
    "        log_pt = torch.log(pt.clamp(min=self.eps, max=1.0))\n",
    "\n",
    "        pos_weight = (1 - xs_pos) ** self.gamma_pos\n",
    "        neg_weight = (1 - xs_neg) ** self.gamma_neg\n",
    "\n",
    "        weighted_loss = - (pos_weight * log_pt * y + neg_weight * log_pt * (1-y))\n",
    "        return weighted_loss.sum() / x.size(0)\n",
    "\n",
    "class WideProteinMLP_WithTaxon(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_taxa, taxon_dim=64,\n",
    "                 hidden_dims=[4096, 4096], dropout=0.4):\n",
    "        super().__init__()\n",
    "\n",
    "        # Normalize protein embedding\n",
    "        self.seq_norm = nn.LayerNorm(input_dim)\n",
    "\n",
    "        # Taxon branch\n",
    "        self.taxon_embedding = nn.Embedding(num_taxa, taxon_dim)\n",
    "        nn.init.normal_(self.taxon_embedding.weight, mean=0.0, std=0.1)\n",
    "        self.taxon_norm = nn.LayerNorm(taxon_dim)\n",
    "\n",
    "        # UNK taxon\n",
    "        self.unk_idx = num_taxa - 1\n",
    "        with torch.no_grad():\n",
    "            self.taxon_embedding.weight[self.unk_idx].zero_()\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"unk_vec\",\n",
    "            self.taxon_embedding.weight[self.unk_idx].clone().detach()\n",
    "        )\n",
    "\n",
    "        # Combined feature dimension\n",
    "        combined_dim = input_dim + taxon_dim\n",
    "\n",
    "        layers = []\n",
    "        prev = combined_dim\n",
    "\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "\n",
    "        layers.append(nn.Linear(prev, num_classes))\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, seq_emb, taxon_id):\n",
    "        seq = self.seq_norm(seq_emb)\n",
    "        tax = self.taxon_norm(self.taxon_embedding(taxon_id))\n",
    "        x = torch.cat([seq, tax], dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "def gpu_calculate_fmax(preds_t, targets_t, ia_weights_np, device):\n",
    "    if preds_t.numel() == 0: return 0.0\n",
    "    ia_t = torch.from_numpy(ia_weights_np.astype(np.float32)).to(device)\n",
    "    w = ia_t.unsqueeze(0)\n",
    "    true_sum = (targets_t * w).sum(dim=1)\n",
    "    valid_mask = true_sum > 0\n",
    "    if valid_mask.sum().item() == 0: return 0.0\n",
    "\n",
    "    p_sub = preds_t[valid_mask]\n",
    "    t_sub = targets_t[valid_mask]\n",
    "    w_sub = w\n",
    "    w_true_sub = true_sum[valid_mask]\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    thresholds = torch.linspace(0.0, 1.0, 51, device=device)\n",
    "    for tau in thresholds:\n",
    "        cut = (p_sub >= tau).float()\n",
    "        tp = ((cut * t_sub) * w_sub).sum(dim=1)\n",
    "        pred_sum = (cut * w_sub).sum(dim=1)\n",
    "        prec = torch.where(pred_sum != 0, tp / pred_sum, torch.zeros_like(tp))\n",
    "        rec = torch.where(w_true_sub != 0, tp / w_true_sub, torch.zeros_like(tp))\n",
    "        avg_p = prec.mean(); avg_r = rec.mean()\n",
    "        denom = (avg_p + avg_r)\n",
    "        if denom > 0: f1 = (2.0 * avg_p * avg_r / denom).item()\n",
    "        else: f1 = 0.0\n",
    "        if f1 > best_f1: best_f1 = f1\n",
    "    return float(best_f1)\n",
    "\n",
    "def gpu_fmax_split(preds_t, targets_t, ia_t):\n",
    "    \"\"\"\n",
    "    preds_t: Tensor [N, C]\n",
    "    targets_t: Tensor [N, C]\n",
    "    ia_t: Tensor [C]\n",
    "    \"\"\"\n",
    "    device = preds_t.device\n",
    "    w = ia_t.unsqueeze(0)\n",
    "    true_sum = (targets_t * w).sum(1)\n",
    "\n",
    "    valid = true_sum > 0\n",
    "    if valid.sum() == 0:\n",
    "        return 0.0\n",
    "\n",
    "    p = preds_t[valid]\n",
    "    t = targets_t[valid]\n",
    "    ts = true_sum[valid]\n",
    "\n",
    "    best = 0.0\n",
    "    thresholds = torch.linspace(0, 1, 101, device=device)\n",
    "\n",
    "    for tau in thresholds:\n",
    "        cut = (p >= tau).float()\n",
    "\n",
    "        tp = (cut * t * w).sum(1)\n",
    "        pred_sum = (cut * w).sum(1)\n",
    "\n",
    "        prec = torch.where(pred_sum > 0, tp / pred_sum, torch.zeros_like(tp))\n",
    "        rec = torch.where(ts > 0, tp / ts, torch.zeros_like(tp))\n",
    "\n",
    "        avg_p = prec.mean()\n",
    "        avg_r = rec.mean()\n",
    "        denom = avg_p + avg_r\n",
    "\n",
    "        f1 = torch.where(denom > 0, 2 * avg_p * avg_r / denom, torch.tensor(0., device=device))\n",
    "        best = max(best, f1.item())\n",
    "\n",
    "    return best\n",
    "\n",
    "# =========================================================\n",
    "# 3. TRAINING LOOP\n",
    "# =========================================================\n",
    "def train_epoch(model, ema_model, loader, loss_fn, optimizer, scaler, scheduler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "\n",
    "    for feats, tax_id, labels in pbar:\n",
    "        feats = feats.to(device)\n",
    "        tax_id = tax_id.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            logits = model(feats, tax_id)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        # immediate check (loss may be a tensor on cuda)\n",
    "        if not torch.isfinite(loss):\n",
    "            print(\"!!! LOSS NaN or Inf detected.\")\n",
    "            print(\"max_logit\", logits.max().item(), \"min_logit\", logits.min().item())\n",
    "            torch.save(model.state_dict(), \"crash_before_nan.pth\")\n",
    "            raise RuntimeError(\"NaN in loss\")\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update EMA\n",
    "        if ema_model is not None:\n",
    "            ema_model.update(model)\n",
    "\n",
    "        running_loss += float(loss.item())\n",
    "        # ==========================================\n",
    "        # GPU MEMORY CLEANUP — PREVENT MEMORY CREEP\n",
    "        # ==========================================\n",
    "        del feats, tax_id, labels, logits, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "    return running_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_split_go_gpu(model, loader, loss_fn, device, ia_weights, vocab_df):\n",
    "    model.eval()\n",
    "\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for feats, tax_id, labels in loader:\n",
    "        feats = feats.to(device)\n",
    "        tax_id = tax_id.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(feats, tax_id)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds_list.append(torch.sigmoid(logits))\n",
    "        targets_list.append(labels)\n",
    "\n",
    "    preds_t = torch.cat(preds_list, dim=0).to(device)\n",
    "    targets_t = torch.cat(targets_list, dim=0).to(device)\n",
    "    val_loss = total_loss / len(loader)\n",
    "\n",
    "    # Convert IA to torch\n",
    "    ia_t = torch.tensor(ia_weights, dtype=torch.float32, device=device)\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    for asp in [\"MFO\", \"BPO\", \"CCO\"]:\n",
    "        cols = vocab_df.index[vocab_df[\"aspect\"] == asp].tolist()\n",
    "        if not cols:\n",
    "            continue\n",
    "\n",
    "        scores[asp] = gpu_fmax_split(\n",
    "            preds_t[:, cols],\n",
    "            targets_t[:, cols],\n",
    "            ia_t[cols]\n",
    "        )\n",
    "\n",
    "    avg_fmax = np.mean(list(scores.values()))\n",
    "\n",
    "    return val_loss, avg_fmax, scores\n",
    "\n",
    "# =========================================================\n",
    "# MAIN\n",
    "# =========================================================\n",
    "def main():\n",
    "    # 1. Init Seed\n",
    "    seed_everything(CONFIG['seed'])\n",
    "\n",
    "    # 2. Load Data & IA\n",
    "    ia_weights = load_ia_weights(CONFIG['VOCAB_FILE'], CONFIG['IA_FILE'])\n",
    "    train_ds = ProteinDataset(CONFIG['TRAIN_IDS'], CONFIG['TARGETS_PKL'], CONFIG['EMBED_DIR'], CONFIG['TAXON_PKL'])\n",
    "    val_ds    = ProteinDataset(CONFIG['VAL_IDS'], CONFIG['TARGETS_PKL'], CONFIG['EMBED_DIR'], CONFIG['TAXON_PKL'])\n",
    "\n",
    "    if len(ia_weights) != train_ds.num_classes:\n",
    "        print(f\"Resizing IA: {len(ia_weights)} -> {train_ds.num_classes}\")\n",
    "        new_ia = np.ones(train_ds.num_classes, dtype=np.float32)\n",
    "        min_len = min(len(ia_weights), train_ds.num_classes)\n",
    "        new_ia[:min_len] = ia_weights[:min_len]\n",
    "        ia_weights = new_ia\n",
    "\n",
    "    CONFIG['output_classes'] = train_ds.num_classes\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True,\n",
    "                              num_workers=16, pin_memory=True, persistent_workers=False, prefetch_factor=2)\n",
    "    val_loader    = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False,\n",
    "                               num_workers=16, pin_memory=True, persistent_workers=False, prefetch_factor=2)\n",
    "\n",
    "    # 3. Init Model & EMA\n",
    "    print(f\"START OFFICIAL TRAINING V2 (C95 MFO | Seed: {CONFIG['seed']})\")\n",
    "    model = WideProteinMLP_WithTaxon(\n",
    "        input_dim=CONFIG['input_dim'],\n",
    "        num_classes=CONFIG['output_classes'],\n",
    "        num_taxa=train_ds.num_taxa,\n",
    "        taxon_dim=64,\n",
    "        hidden_dims=[4096, 4096],\n",
    "        dropout=0.25\n",
    "    ).to(CONFIG['device'])\n",
    "\n",
    "    #  Initialize EMA Model\n",
    "    ema_model = ModelEMA(model, decay=CONFIG['ema_decay'])\n",
    "    print(f\"   Using EMA Decay: {CONFIG['ema_decay']}\")\n",
    "\n",
    "    loss_fn = AsymmetricLossOptimized(gamma_neg=CONFIG['gamma_neg'], gamma_pos=CONFIG['gamma_pos'], clip=CONFIG['clip'])\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr_max'], weight_decay=0.01)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=CONFIG['lr_max'], epochs=CONFIG['epochs'],\n",
    "        steps_per_epoch=len(train_loader), pct_start=0.3, div_factor=25, final_div_factor=1e4\n",
    "    )\n",
    "\n",
    "    # 4. Training Loop\n",
    "    best_proxy_fmax = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    fmax_buffer = deque(maxlen=3)   # window = 3\n",
    "\n",
    "    vocab_df = pd.read_csv(CONFIG['VOCAB_FILE'])\n",
    "\n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        tr_loss = train_epoch(model, ema_model, train_loader, loss_fn, optimizer, scaler, scheduler, CONFIG['device'])\n",
    "\n",
    "        #  Validate EMA Model (\n",
    "        val_loss, val_fmax, go_scores = validate_split_go_gpu(\n",
    "            ema_model.module,\n",
    "            val_loader,\n",
    "            loss_fn,\n",
    "            CONFIG[\"device\"],\n",
    "            ia_weights,\n",
    "            vocab_df\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{CONFIG['epochs']} | Train Loss: {tr_loss:.4f} | Val Loss: {val_loss:.4f} | EMA F-max: {val_fmax:.4f}\")\n",
    "        print(\"MF:\", go_scores[\"MFO\"], \"BP:\", go_scores[\"BPO\"], \"CC:\", go_scores[\"CCO\"])\n",
    "        #  cập nhật buffer\n",
    "        fmax_buffer.append(val_fmax)\n",
    "\n",
    "        if len(fmax_buffer) == fmax_buffer.maxlen:\n",
    "            proxy_fmax = float(np.mean(fmax_buffer))\n",
    "        else:\n",
    "            proxy_fmax = val_fmax\n",
    "\n",
    "        if proxy_fmax > best_proxy_fmax:\n",
    "            best_proxy_fmax = proxy_fmax\n",
    "            patience_counter = 0\n",
    "            torch.save(ema_model.module.state_dict(), CONFIG['save_path'])\n",
    "            print(f\"    Saved Best EMA Model (proxy F-max: {best_proxy_fmax:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"    Patience: {patience_counter}/{CONFIG['patience']}\")\n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(\" Early Stopping Triggered!\")\n",
    "                break\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "    print(f\"\\n Finished. Best EMA proxy F-max: {best_proxy_fmax:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1054ac9",
   "metadata": {
    "id": "a1054ac9",
    "outputId": "b6634318-1de1-482f-d6db-b50971b8dd2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Seed set to 42\n",
      " Merging Train + Val data for FULL TRAINING...\n",
      "Loading IA Weights...\n",
      "FINAL TRAINING START (full data, 20 epochs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 430/430 [00:16<00:00, 26.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full] Epoch 1/20 | Loss: 60.7082 | LR: 4.17e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 430/430 [00:16<00:00, 26.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full] Epoch 2/20 | Loss: 31.3414 | LR: 1.12e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 430/430 [00:16<00:00, 26.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full] Epoch 3/20 | Loss: 29.2087 | LR: 2.08e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 430/430 [00:16<00:00, 26.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full] Epoch 4/20 | Loss: 27.9943 | LR: 3.04e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 430/430 [00:16<00:00, 26.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full] Epoch 5/20 | Loss: 27.0426 | LR: 3.74e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 430/430 [00:16<00:00, 26.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full] Epoch 6/20 | Loss: 26.1325 | LR: 4.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 430/430 [00:16<00:00, 26.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full] Epoch 7/20 | Loss: 25.0885 | LR: 3.95e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 430/430 [00:16<00:00, 26.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full] Epoch 8/20 | Loss: 23.9432 | LR: 3.80e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 430/430 [00:16<00:00, 26.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full] Epoch 9/20 | Loss: 22.7908 | LR: 3.56e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 430/430 [00:16<00:00, 25.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full] Epoch 10/20 | Loss: 21.7979 | LR: 3.25e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 430/430 [00:16<00:00, 26.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full] Epoch 11/20 | Loss: 20.2878 | LR: 2.87e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 430/430 [00:16<00:00, 25.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full] Epoch 12/20 | Loss: 19.0122 | LR: 2.44e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Full: 100%|██████████| 430/430 [00:16<00:00, 26.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full] Epoch 13/20 | Loss: 17.7908 | LR: 2.00e-04\n",
      " Forced stop at epoch 13 (best F-max observed)\n",
      " FINAL MODEL SAVED: final_cafa6_model_c99.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG (CHẾ ĐỘ FULL TRAIN)\n",
    "# =========================================================\n",
    "CONFIG = {\n",
    "    # --- Paths ---\n",
    "    'TRAIN_IDS': '/root/CAFA6data/c99/train_ids_C99_split.npy',\n",
    "    'VAL_IDS': '/root/CAFA6data/c99/val_ids_C99_split.npy',\n",
    "    'TARGETS_PKL': '/root/CAFA6data/c99/train_targets_C99.pkl',\n",
    "    'EMBED_DIR': '/root/CAFA6data/cafa6-embeds',\n",
    "    'IA_FILE': '/root/CAFA6data/IA.tsv',\n",
    "    'VOCAB_FILE': '/root/CAFA6data/c99/vocab_C99_remove.csv',\n",
    "    'TAXON_PKL': '/root/cafa6/preprocessing/taxon_mapping_K_Species.pkl',\n",
    "\n",
    "    # --- Training Hypers ---\n",
    "    'input_dim': 1280,\n",
    "    'batch_size': 192,\n",
    "    'device': \"cuda\",\n",
    "    'lr_max': 4e-4,\n",
    "    'seed': 42,\n",
    "    'ema_decay': 0.999,\n",
    "\n",
    "    'epochs': 20,\n",
    "    'stop_epoch': 13,\n",
    "\n",
    "    # --- Best Params found ---\n",
    "    'gamma_neg': 4,\n",
    "    'gamma_pos': 1.0,\n",
    "    'clip': 0.03,\n",
    "\n",
    "}\n",
    "\n",
    "# =========================================================\n",
    "# 1. UTILS\n",
    "# =========================================================\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\" Seed set to {seed}\")\n",
    "\n",
    "class ModelEMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.module = copy.deepcopy(model).eval()\n",
    "        self.module.load_state_dict(model.state_dict())\n",
    "        self.decay = decay\n",
    "        for p in self.module.parameters(): p.requires_grad_(False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        msd = model.state_dict()\n",
    "        for k, ema_v in self.module.state_dict().items():\n",
    "            model_v = msd[k].detach()\n",
    "            ema_v.copy_(ema_v * self.decay + (1.0 - self.decay) * model_v)\n",
    "\n",
    "def load_ia_weights(vocab_path, ia_path):\n",
    "    print(\"Loading IA Weights...\")\n",
    "    try:\n",
    "        vocab_df = pd.read_csv(vocab_path)\n",
    "        term_list = vocab_df['term'].tolist()\n",
    "        term_to_idx = {t: i for i, t in enumerate(term_list)}\n",
    "        num_classes = len(term_list)\n",
    "        ia_weights = np.ones(num_classes, dtype=np.float32)\n",
    "        if os.path.exists(ia_path):\n",
    "            with open(ia_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) >= 2:\n",
    "                        term, ia = parts[0], float(parts[1])\n",
    "                        if term in term_to_idx:\n",
    "                            ia_weights[term_to_idx[term]] = ia\n",
    "        return ia_weights\n",
    "    except Exception:\n",
    "        return np.ones(1, dtype=np.float32)\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, ids_source, targets_pkl, embed_dir, taxon_pkl):\n",
    "\n",
    "        if isinstance(ids_source, str):\n",
    "            self.ids = np.load(ids_source)\n",
    "        else:\n",
    "            self.ids = ids_source\n",
    "\n",
    "        # Dùng mmap_mode='r' để tiết kiệm RAM\n",
    "        self.embeds = np.load(os.path.join(embed_dir, \"train_embeds.npy\"), mmap_mode='r')\n",
    "\n",
    "        with open(os.path.join(embed_dir, \"train_ids.txt\")) as f:\n",
    "            all_ids = [x.strip() for x in f]\n",
    "        self.id_to_idx = {pid: i for i, pid in enumerate(all_ids)}\n",
    "\n",
    "        with open(targets_pkl, 'rb') as f:\n",
    "            self.targets = pickle.load(f)\n",
    "\n",
    "        max_idx = 0\n",
    "        for v in self.targets.values():\n",
    "            if v: max_idx = max(max_idx, max(v))\n",
    "        self.num_classes = max_idx + 1\n",
    "\n",
    "        with open(taxon_pkl, 'rb') as f:\n",
    "            tax_data = pickle.load(f)\n",
    "        self.prot_to_taxon = tax_data[\"prot_to_taxon_idx\"]\n",
    "        self.num_taxa = tax_data[\"num_taxa_classes\"]\n",
    "        self.default_tax = self.num_taxa - 1\n",
    "\n",
    "    def __len__(self): return len(self.ids)\n",
    "    def __getitem__(self, i):\n",
    "        pid = self.ids[i]\n",
    "        emb_idx = self.id_to_idx.get(pid)\n",
    "        if emb_idx is not None:\n",
    "             # Copy vào RAM\n",
    "            feat = torch.tensor(self.embeds[emb_idx], dtype=torch.float32)\n",
    "        else:\n",
    "            feat = torch.zeros(1280)\n",
    "\n",
    "        target = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        if pid in self.targets:\n",
    "            inds = self.targets[pid]\n",
    "            if len(inds) > 0:\n",
    "                target[inds] = 1.0\n",
    "\n",
    "        tax_id = self.prot_to_taxon.get(pid, self.default_tax)\n",
    "        return feat, torch.tensor(tax_id, dtype=torch.long), target\n",
    "\n",
    "# =========================================================\n",
    "# 2. MODEL & LOSS\n",
    "# =========================================================\n",
    "class AsymmetricLossOptimized(nn.Module):\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=0, clip=0.05, eps=1e-7):\n",
    "        super().__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.eps = eps\n",
    "    def forward(self, x, y):\n",
    "        x = x.float(); y = y.float()\n",
    "        x_sigmoid = torch.sigmoid(x)\n",
    "        xs_pos = x_sigmoid\n",
    "        xs_neg = (1 - x_sigmoid + self.clip + self.eps).clamp(max=1.0)\n",
    "        pt = y * xs_pos + (1 - y) * xs_neg\n",
    "        log_pt = torch.log(pt.clamp(min=self.eps, max=1.0))\n",
    "        weight = y*(1-xs_pos)**self.gamma_pos + (1-y)*(1-xs_neg)**self.gamma_neg\n",
    "        loss = - weight * log_pt\n",
    "        return loss.sum() / x.size(0)\n",
    "\n",
    "class WideProteinMLP_WithTaxon(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_taxa, taxon_dim=64, hidden_dims=[4096, 4096], dropout=0.25):\n",
    "        super().__init__()\n",
    "        self.seq_norm = nn.LayerNorm(input_dim)\n",
    "        self.taxon_embedding = nn.Embedding(num_taxa, taxon_dim)\n",
    "        self.taxon_norm = nn.LayerNorm(taxon_dim)\n",
    "        with torch.no_grad(): self.taxon_embedding.weight[num_taxa-1].zero_()\n",
    "        layers = []\n",
    "        prev = input_dim + taxon_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, num_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, seq, tax):\n",
    "        x = torch.cat([self.seq_norm(seq), self.taxon_norm(self.taxon_embedding(tax))], dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "# =========================================================\n",
    "# METRICS & TRAINING\n",
    "# =========================================================\n",
    "def train_epoch(model, ema_model, loader, loss_fn, optimizer, scaler, scheduler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(loader, desc=\"Training Full\")\n",
    "\n",
    "    for feats, tax_id, labels in pbar:\n",
    "        feats = feats.to(device)\n",
    "        tax_id = tax_id.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            logits = model(feats, tax_id)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        scaler.step(optimizer); scaler.update(); scheduler.step()\n",
    "\n",
    "        if ema_model: ema_model.update(model)\n",
    "\n",
    "        running_loss += float(loss.item())\n",
    "        del feats, tax_id, labels, logits, loss\n",
    "\n",
    "    return running_loss / len(loader)\n",
    "\n",
    "# =========================================================\n",
    "# MAIN\n",
    "# =========================================================\n",
    "def main():\n",
    "    seed_everything(CONFIG['seed'])\n",
    "\n",
    "    print(\" Merging Train + Val data for FULL TRAINING...\")\n",
    "    ids_train = np.load(CONFIG['TRAIN_IDS'])\n",
    "    ids_val   = np.load(CONFIG['VAL_IDS'])\n",
    "    ids_full  = np.concatenate([ids_train, ids_val])\n",
    "\n",
    "    full_ds = ProteinDataset(ids_full, CONFIG['TARGETS_PKL'], CONFIG['EMBED_DIR'], CONFIG['TAXON_PKL'])\n",
    "    full_loader = DataLoader(full_ds, batch_size=CONFIG['batch_size'], shuffle=True,\n",
    "                            num_workers=16, pin_memory=True, persistent_workers=False)\n",
    "\n",
    "    # Load IA Weights cho metric\n",
    "    ia_weights = load_ia_weights(CONFIG['VOCAB_FILE'], CONFIG['IA_FILE'])\n",
    "    if len(ia_weights) != full_ds.num_classes:\n",
    "        new_ia = np.ones(full_ds.num_classes, dtype=np.float32)\n",
    "        min_len = min(len(ia_weights), full_ds.num_classes)\n",
    "        new_ia[:min_len] = ia_weights[:min_len]\n",
    "        ia_weights = new_ia\n",
    "\n",
    "    model = WideProteinMLP_WithTaxon(\n",
    "        input_dim=CONFIG['input_dim'],\n",
    "        num_classes=full_ds.num_classes,\n",
    "        num_taxa=full_ds.num_taxa,\n",
    "        taxon_dim=64,\n",
    "        hidden_dims=[4096, 4096],\n",
    "        dropout=0.25\n",
    "    ).to(CONFIG['device'])\n",
    "\n",
    "    ema_model = ModelEMA(model, decay=CONFIG['ema_decay'])\n",
    "\n",
    "    loss_fn = AsymmetricLossOptimized(\n",
    "        gamma_neg=CONFIG['gamma_neg'],\n",
    "        gamma_pos=CONFIG['gamma_pos'],\n",
    "        clip=CONFIG['clip']\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr_max'], weight_decay=0.01)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=CONFIG['lr_max'],  # same LR as training\n",
    "        epochs=CONFIG['epochs'],                # BEST EPOCH FOUND\n",
    "        steps_per_epoch=len(full_loader),\n",
    "        pct_start=0.3,\n",
    "        div_factor=25,\n",
    "        final_div_factor=1e4\n",
    "    )\n",
    "\n",
    "    print(\"FINAL TRAINING START (full data, 20 epochs)\")\n",
    "\n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        tr_loss = train_epoch(\n",
    "        model, ema_model, full_loader,\n",
    "        loss_fn, optimizer, scaler,\n",
    "        scheduler, CONFIG['device']\n",
    "    )\n",
    "\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"[Full] Epoch {epoch+1}/{CONFIG['epochs']} | Loss: {tr_loss:.4f} | LR: {current_lr:.2e}\")\n",
    "\n",
    "        # ===============================\n",
    "        # FORCE STOP AT EPOCH 13\n",
    "        # ===============================\n",
    "        if epoch + 1 >= CONFIG['stop_epoch']:\n",
    "            print(f\" Forced stop at epoch {epoch+1} (best F-max observed)\")\n",
    "            break\n",
    "\n",
    "    torch.save(ema_model.module.state_dict(), \"final_cafa6_model_c99.pth\")\n",
    "    print(\" FINAL MODEL SAVED: final_cafa6_model_c99.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
