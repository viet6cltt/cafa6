{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f57b22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T11:06:25.326135Z",
     "iopub.status.busy": "2025-12-16T11:06:25.325748Z",
     "iopub.status.idle": "2025-12-16T11:10:31.752613Z",
     "shell.execute_reply": "2025-12-16T11:10:31.750820Z"
    },
    "papermill": {
     "duration": 246.507698,
     "end_time": "2025-12-16T11:10:31.755900",
     "exception": false,
     "start_time": "2025-12-16T11:06:25.248202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " STARTING SAFE OFFLINE REPAIR PIPELINE...\n",
      " Loading Vocab & IA...\n",
      "Parsing OBO from /kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo...\n",
      "Parsing OBO from /kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo...\n",
      "Parsed 16788 parents with children.\n",
      "\n",
      ">>> Loading GOA Database from /kaggle/input/protein-go-annotations/goa_uniprot_all.csv...\n",
      "Loaded 2583077 annotations.\n",
      "Propagating Negatives...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3863/3863 [00:00<00:00, 16451.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 238553 negative Protein-Term pairs.\n",
      "Identified 2464255 positive pairs to inject (Score = 1.0).\n",
      "Loading KNN Rescue Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1360373/1360373 [00:02<00:00, 510077.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing Proteins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 113it [03:52,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Rescuing missing proteins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Applying Logic...\n",
      "1. Removing Negatives...\n",
      "   Removed 6875 negative predictions.\n",
      "2. Injecting Ground Truth...\n",
      "   Boosted 1931323 existing predictions to 1.0\n",
      "   Adding 382835 completely new ground-truth rows...\n",
      "\n",
      " DONE! Predictions: 33,521,389\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# UTILS\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "# =========================================================\n",
    "# CONFIGURATION\n",
    "# =========================================================\n",
    "CONFIG_POST = {\n",
    "    # --- INPUT FILES ---\n",
    "    'RAW_FILE': \"submission_c95_c99_final.tsv\",                                     \n",
    "    'VOCAB_FILE': \"/kaggle/input/c99-cafa6/vocab_C99_remove.csv\",            \n",
    "    \n",
    "    # --- EXTERNAL DATA ---\n",
    "    'GOA_FILE': \"/kaggle/input/protein-go-annotations/goa_uniprot_all.csv\",  \n",
    "    'OBO_FILE': \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\", \n",
    "    'IA_FILE': \"/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\", \n",
    "    \n",
    "    'OUTPUT_FILE': \"submission.tsv\",\n",
    "    \n",
    "    # --- PARAMETERS ---\n",
    "    'FINAL_THRESHOLD': 0.35,  \n",
    "    'FINAL_CAP': 250,         \n",
    "    \n",
    "    'PROP_MIN_SCORE': 0.50,   \n",
    "}\n",
    "\n",
    "# =========================================================\n",
    "# UTILS\n",
    "# =========================================================\n",
    "def parse_obo_parents(obo_path):\n",
    "    print(f\"Parsing OBO from {obo_path}...\")\n",
    "    term_to_parents = defaultdict(set)\n",
    "    if not os.path.exists(obo_path): return term_to_parents\n",
    "    with open(obo_path, \"r\") as f:\n",
    "        cur_id = None\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"[Term]\": cur_id = None\n",
    "            elif line.startswith(\"id: \"): cur_id = line.split(\"id: \")[1].strip()\n",
    "            elif line.startswith(\"is_a: \") and cur_id: term_to_parents[cur_id].add(line.split()[1].strip())\n",
    "            elif line.startswith(\"relationship: part_of \") and cur_id:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 3: term_to_parents[cur_id].add(parts[2].strip())\n",
    "    return term_to_parents\n",
    "\n",
    "def get_descendants(term, children_map):\n",
    "    out = set()\n",
    "    stack = [term]\n",
    "    while stack:\n",
    "        t = stack.pop()\n",
    "        for c in children_map.get(t, []):\n",
    "            if c not in out:\n",
    "                out.add(c)\n",
    "                stack.append(c)\n",
    "    return out\n",
    "\n",
    "def parse_obo_children(obo_path):\n",
    "    print(f\"Parsing OBO from {obo_path}...\")\n",
    "    children_map = defaultdict(set)\n",
    "\n",
    "    with open(obo_path, \"r\") as f:\n",
    "        cur_id = None\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"[Term]\":\n",
    "                cur_id = None\n",
    "            elif line.startswith(\"id: \"):\n",
    "                cur_id = line.split(\"id: \")[1].strip()\n",
    "            elif line.startswith(\"is_a: \") and cur_id:\n",
    "                parent = line.split()[1].strip()\n",
    "                children_map[parent].add(cur_id)\n",
    "            elif line.startswith(\"relationship: part_of \") and cur_id:\n",
    "                parent = line.split()[2].strip()\n",
    "                children_map[parent].add(cur_id)\n",
    "\n",
    "    print(f\"Parsed {len(children_map)} parents with children.\")\n",
    "    return children_map\n",
    "\n",
    "\n",
    "def load_uniprot_data(goa_path, children_map):\n",
    "    print(f\"\\n>>> Loading GOA Database from {goa_path}...\")\n",
    "    df = pd.read_csv(goa_path, usecols=['protein_id','go_term','qualifier'], dtype=str)\n",
    "    print(f\"Loaded {len(df)} annotations.\")\n",
    "\n",
    "    neg_df = df[df['qualifier'].str.contains('NOT', na=False)]\n",
    "    neg_map = neg_df.groupby('protein_id')['go_term'].apply(list).to_dict()\n",
    "\n",
    "    neg_keys = set()\n",
    "    print(\"Propagating Negatives...\")\n",
    "    for pid, terms in tqdm(neg_map.items()):\n",
    "        for t in terms:\n",
    "            neg_keys.add(f\"{pid}_{t}\")\n",
    "            for c in get_descendants(t, children_map):\n",
    "                neg_keys.add(f\"{pid}_{c}\")\n",
    "\n",
    "    print(f\"Identified {len(neg_keys)} negative Protein-Term pairs.\")\n",
    "\n",
    "    pos_df = df[~df['qualifier'].str.contains('NOT', na=False)]\n",
    "    pos_map = defaultdict(set)\n",
    "    for pid, term in zip(pos_df['protein_id'], pos_df['go_term']):\n",
    "        pos_map[pid].add(term)\n",
    "\n",
    "    print(f\"Identified {sum(len(v) for v in pos_map.values())} positive pairs to inject (Score = 1.0).\")\n",
    "\n",
    "    del df, neg_df, pos_df\n",
    "    gc.collect()\n",
    "    return pos_map, neg_keys\n",
    "# =========================================================\n",
    "# CORE PROCESSING\n",
    "# =========================================================\n",
    "def run_offline_repair_safe():\n",
    "    print(\" STARTING SAFE OFFLINE REPAIR PIPELINE...\")\n",
    "    \n",
    "    # --- A. LOAD RESOURCES ---\n",
    "    print(\" Loading Vocab & IA...\")\n",
    "    vocab_df = pd.read_csv(CONFIG_POST['VOCAB_FILE'])\n",
    "    term_to_idx = {t: i for i, t in enumerate(vocab_df['term'])}\n",
    "    idx_to_term = {i: t for i, t in enumerate(vocab_df['term'])}\n",
    "    \n",
    "    # Load IA Map (Cho Logic 2)\n",
    "    ia_df = pd.read_csv(CONFIG_POST['IA_FILE'], sep='\\t', header=None, names=['term', 'ia'])\n",
    "    term_to_ia = dict(zip(ia_df.term, ia_df.ia))\n",
    "    \n",
    "    # Load Parents\n",
    "    obo_parents = parse_obo_parents(CONFIG_POST['OBO_FILE'])\n",
    "    parent_map_idx = defaultdict(list)\n",
    "    for t_str, parents in obo_parents.items():\n",
    "        if t_str in term_to_idx:\n",
    "            c_idx = term_to_idx[t_str]\n",
    "            for p_str in parents:\n",
    "                if p_str in term_to_idx: parent_map_idx[c_idx].append(term_to_idx[p_str])\n",
    "    \n",
    "    children_map = parse_obo_children(CONFIG_POST['OBO_FILE'])\n",
    "    gt_pos_map, gt_neg_keys = load_uniprot_data(CONFIG_POST['GOA_FILE'], children_map)\n",
    "    \n",
    "    print(\"Loading KNN Rescue Data...\")\n",
    "    knn_rescue_map = defaultdict(list)\n",
    "    if os.path.exists(CONFIG_POST['KNN_FILE']):\n",
    "        knn_df = pd.read_csv(CONFIG_POST['KNN_FILE'], sep='\\t', names=['PID', 'Term', 'Score'])\n",
    "        for pid, term, score in tqdm(zip(knn_df.PID, knn_df.Term, knn_df.Score), total=len(knn_df)):\n",
    "            if term in term_to_idx:\n",
    "                knn_rescue_map[pid].append((term_to_idx[term], float(score)))\n",
    "        del knn_df; gc.collect()\n",
    "\n",
    "    # --- B. PROCESSING ---\n",
    "    print(\" Processing Proteins...\")\n",
    "    f_out = open(CONFIG_POST['OUTPUT_FILE'], \"w\")\n",
    "    reader = pd.read_csv(CONFIG_POST['RAW_FILE'], sep='\\t', names=['PID', 'Term', 'Score'], \n",
    "                         chunksize=1000000, dtype={'PID': str, 'Term': str, 'Score': float})\n",
    "    \n",
    "    current_pid = None; current_scores = {}; total_written = 0\n",
    "    processed_pids = set()\n",
    "\n",
    "    # === GLOBAL STATS ===\n",
    "    STAT_NEG_REMOVED = 0\n",
    "    STAT_POS_BOOSTED = 0\n",
    "    STAT_POS_ADDED   = 0\n",
    "    \n",
    "    def process_and_write(pid, scores_dict):\n",
    "        nonlocal STAT_NEG_REMOVED, STAT_POS_BOOSTED, STAT_POS_ADDED\n",
    "\n",
    "        original_keys = set(scores_dict.keys())\n",
    "        # (2) FILTER KNN HEAD & (1) NO OVERWRITE\n",
    "        if pid in knn_rescue_map:\n",
    "            for t_idx, k_score in knn_rescue_map[pid]:\n",
    "        \n",
    "                # KhÃ´ng ghi Ä‘Ã¨ DL\n",
    "                if t_idx in scores_dict:\n",
    "                    continue\n",
    "        \n",
    "                t_str = idx_to_term[t_idx]\n",
    "                ia = term_to_ia.get(t_str, 0.0)\n",
    "        \n",
    "                #  CHá»ˆ Cá»¨U IA Ráº¤T CAO\n",
    "                if ia < 4.0:\n",
    "                    continue\n",
    "        \n",
    "                #  CHá»ˆ NHáº¬N SCORE KNN CHáº®C\n",
    "                if k_score < 0.25:\n",
    "                    continue\n",
    "        \n",
    "                # Clamp cá»©ng\n",
    "                scores_dict[t_idx] = 0.4\n",
    "    \n",
    "        # --- UNIPROT INJECTION ---\n",
    "        if pid in gt_pos_map:\n",
    "            for term in gt_pos_map[pid]:\n",
    "                if term in term_to_idx:\n",
    "                    t_idx = term_to_idx[term]\n",
    "                    if t_idx in scores_dict:\n",
    "                        if scores_dict[t_idx] < 1.0:\n",
    "                            STAT_POS_BOOSTED += 1\n",
    "                    else:\n",
    "                        STAT_POS_ADDED += 1\n",
    "                    scores_dict[t_idx] = max(scores_dict.get(t_idx, 0.0), 1.0)\n",
    "\n",
    "        # NEGATIVE FILTER\n",
    "        # --- NEGATIVE FILTER ---\n",
    "        keys_to_remove = []\n",
    "        for k in scores_dict:\n",
    "            if f\"{pid}_{idx_to_term[k]}\" in gt_neg_keys:\n",
    "                keys_to_remove.append(k)\n",
    "    \n",
    "        STAT_NEG_REMOVED += len(keys_to_remove)\n",
    "        for k in keys_to_remove:\n",
    "            del scores_dict[k]\n",
    "\n",
    "        # FINAL FILTER & CAP\n",
    "        final_items = [(t, s) for t, s in scores_dict.items() if s >= CONFIG_POST['FINAL_THRESHOLD']]\n",
    "        final_items.sort(key=lambda x: x[1], reverse=True)\n",
    "        if len(final_items) > CONFIG_POST['FINAL_CAP']: final_items = final_items[:CONFIG_POST['FINAL_CAP']]\n",
    "            \n",
    "        lines = [f\"{pid}\\t{idx_to_term[t]}\\t{s:.3f}\\n\" for t, s in final_items]\n",
    "        f_out.write(\"\".join(lines))\n",
    "        return len(lines)\n",
    "\n",
    "    # --- MAIN LOOP ---\n",
    "    for chunk in tqdm(reader, desc=\"Processing Batches\"):\n",
    "        chunk_pids = chunk['PID'].values; chunk_terms = chunk['Term'].values; chunk_scores = chunk['Score'].values\n",
    "        for i in range(len(chunk)):\n",
    "            p = chunk_pids[i]; t_str = chunk_terms[i]; s = chunk_scores[i]\n",
    "            if t_str not in term_to_idx: continue\n",
    "            \n",
    "            if p != current_pid:\n",
    "                if current_pid: \n",
    "                    total_written += process_and_write(current_pid, current_scores)\n",
    "                    processed_pids.add(current_pid)\n",
    "                current_pid = p; current_scores = {}\n",
    "            current_scores[term_to_idx[t_str]] = s\n",
    "            \n",
    "    if current_pid: \n",
    "        total_written += process_and_write(current_pid, current_scores)\n",
    "        processed_pids.add(current_pid)\n",
    "\n",
    "    # --- RESCUE MISSING ---\n",
    "    print(\"ðŸ”„ Rescuing missing proteins...\")\n",
    "    missing_pids = (set(knn_rescue_map.keys()) | set(gt_pos_map.keys())) - processed_pids\n",
    "    for pid in tqdm(missing_pids): total_written += process_and_write(pid, {})\n",
    "        \n",
    "    f_out.close()\n",
    "    print(\"\\n>>> Applying Logic...\")\n",
    "    print(f\"1. Removing Negatives...\")\n",
    "    print(f\"   Removed {STAT_NEG_REMOVED} negative predictions.\")\n",
    "    \n",
    "    print(\"2. Injecting Ground Truth...\")\n",
    "    print(f\"   Boosted {STAT_POS_BOOSTED} existing predictions to 1.0\")\n",
    "    print(f\"   Adding {STAT_POS_ADDED} completely new ground-truth rows...\")\n",
    "    print(f\"\\n DONE! Predictions: {total_written:,}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_offline_repair_safe()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14875579,
     "sourceId": 116062,
     "sourceType": "competition"
    },
    {
     "datasetId": 8699749,
     "sourceId": 13680623,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8916743,
     "sourceId": 14073266,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8917158,
     "sourceId": 14114790,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8917141,
     "sourceId": 14114804,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8908959,
     "sourceId": 14152266,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8919436,
     "sourceId": 14169699,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 493.449262,
   "end_time": "2025-12-16T11:10:33.243540",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-16T11:02:19.794278",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
