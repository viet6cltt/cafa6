{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea3cd0ab",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-08T19:21:39.521692Z",
     "iopub.status.busy": "2025-12-08T19:21:39.521394Z",
     "iopub.status.idle": "2025-12-08T19:21:58.918733Z",
     "shell.execute_reply": "2025-12-08T19:21:58.916953Z"
    },
    "papermill": {
     "duration": 19.407114,
     "end_time": "2025-12-08T19:21:58.921197",
     "exception": false,
     "start_time": "2025-12-08T19:21:39.514083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫ÆT ƒê·∫¶U T·∫†O METADATA (FULL & CORRECTED)...\n",
      "\n",
      "[1/4] Loading Graph & Vocab...\n",
      "   - Graph nodes: 40,122\n",
      "   - Vocab size: 15,582\n",
      "\n",
      "[2/4] Calculating Depth Norm...\n",
      "   - Max Depth: 11.0\n",
      "\n",
      "[3/4] Building Parent Map (Transitive - Fix Broken Chains)...\n",
      "   - Building full is_a graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Transitive: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15582/15582 [00:00<00:00, 29222.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Mapped 130,171 transitive relationships (Bridged gaps).\n",
      "\n",
      "[4/4] Calculating Information Content (IC)...\n",
      "   - IC Max Value: 9.3735\n",
      "   - IC Norm Shape: (15582,)\n",
      "\n",
      ">>> Saving to hierarchy_metadata.pkl...\n",
      "‚úÖ DONE! Metadata V2 (Final) Created Successfully.\n"
     ]
    }
   ],
   "source": [
    "!pip install obonet networkx --quiet\n",
    "\n",
    "import obonet\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict, deque\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================================\n",
    "# 1. C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N (CHECK K·ª∏)\n",
    "# ============================================================================\n",
    "CONFIG = {\n",
    "    # File OBO g·ªëc\n",
    "    'OBO_FILE': \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\",\n",
    "    \n",
    "    'VOCAB_FILE': \"/kaggle/input/c99-cafa6/vocab_C99_remove.csv\",\n",
    "    'TARGET_FILE': \"/kaggle/input/c99-cafa6/train_targets_C99.pkl\",\n",
    "    \n",
    "    # File ƒë·∫ßu ra\n",
    "    'OUTPUT_PKL': \"hierarchy_metadata.pkl\"\n",
    "}\n",
    "\n",
    "def build_metadata_v2_final():\n",
    "    print(\"üöÄ B·∫ÆT ƒê·∫¶U T·∫†O METADATA (FULL & CORRECTED)...\")\n",
    "    \n",
    "    # --- A. LOAD D·ªÆ LI·ªÜU ---\n",
    "    print(\"\\n[1/4] Loading Graph & Vocab...\")\n",
    "    graph = obonet.read_obo(CONFIG['OBO_FILE'])\n",
    "    vocab_df = pd.read_csv(CONFIG['VOCAB_FILE'])\n",
    "    vocab_terms = vocab_df['term'].tolist()\n",
    "    vocab_size = len(vocab_terms)\n",
    "    term_to_idx = {t: i for i, t in enumerate(vocab_terms)}\n",
    "    \n",
    "    print(f\"   - Graph nodes: {len(graph):,}\")\n",
    "    print(f\"   - Vocab size: {vocab_size:,}\")\n",
    "\n",
    "    # --- B. T√çNH DEPTH NORM (BFS) ---\n",
    "    print(\"\\n[2/4] Calculating Depth Norm...\")\n",
    "    # (Ph·∫ßn n√†y code c≈© ƒë√£ ƒë√∫ng, gi·ªØ nguy√™n logic BFS)\n",
    "    roots = [\"GO:0008150\", \"GO:0003674\", \"GO:0005575\"]\n",
    "    depth_dict = {node: float('inf') for node in graph.nodes()}\n",
    "    queue = deque()\n",
    "    \n",
    "    for root in roots:\n",
    "        if root in graph:\n",
    "            depth_dict[root] = 0\n",
    "            queue.append(root)\n",
    "            \n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        d = depth_dict[node]\n",
    "        # Predecessors = CON (Edge ƒëi t·ª´ con -> cha)\n",
    "        for child in graph.predecessors(node):\n",
    "            if depth_dict[child] > d + 1:\n",
    "                depth_dict[child] = d + 1\n",
    "                queue.append(child)\n",
    "                \n",
    "    depth_arr = np.array([depth_dict.get(t, 0) for t in vocab_terms], dtype=np.float32)\n",
    "    depth_arr[depth_arr == float('inf')] = 0.0\n",
    "    \n",
    "    if depth_arr.max() > 0:\n",
    "        depth_norm = depth_arr / depth_arr.max()\n",
    "    else:\n",
    "        depth_norm = depth_arr\n",
    "    print(f\"   - Max Depth: {depth_arr.max()}\")\n",
    "\n",
    "    # --- C. T√çNH PARENT MAP (TRANSITIVE CLOSURE) - [N√ÇNG C·∫§P] ---\n",
    "    print(\"\\n[3/4] Building Parent Map (Transitive - Fix Broken Chains)...\")\n",
    "    \n",
    "    # 1. X√¢y d·ª±ng ƒë·ªì th·ªã 'is_a' to√†n v·∫πn t·ª´ file OBO g·ªëc\n",
    "    # (ƒê·ªÉ t√¨m ƒë∆∞·ªùng ƒëi ngay c·∫£ khi node trung gian b·ªã c·∫Øt)\n",
    "    full_isa_graph = nx.DiGraph()\n",
    "    \n",
    "    print(\"   - Building full is_a graph...\")\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        if \"is_a\" in data:\n",
    "            for p_str in data[\"is_a\"]:\n",
    "                p_id = p_str.split(\" ! \")[0]\n",
    "                # Th√™m c·∫°nh Con -> Cha\n",
    "                full_isa_graph.add_edge(node, p_id)\n",
    "                \n",
    "    # 2. Map Con -> T·∫•t c·∫£ T·ªï ti√™n (Ancestors) c√≥ trong Vocab\n",
    "    child_to_parent = defaultdict(list)\n",
    "    count_edges = 0\n",
    "    vocab_set = set(vocab_terms) # ƒê·ªÉ tra c·ª©u nhanh\n",
    "    \n",
    "    for term, idx in tqdm(term_to_idx.items(), desc=\"Mapping Transitive\"):\n",
    "        if term not in full_isa_graph: continue\n",
    "        \n",
    "        # T√¨m t·∫•t c·∫£ t·ªï ti√™n trong ƒë·ªì th·ªã g·ªëc (bao g·ªìm c·∫£ cha, √¥ng, c·ª•...)\n",
    "        # nx.descendants trong DiGraph(Con->Cha) s·∫Ω tr·∫£ v·ªÅ t·∫•t c·∫£ Ancestors\n",
    "        try:\n",
    "            all_ancestors = nx.descendants(full_isa_graph, term)\n",
    "        except:\n",
    "            continue # Ph√≤ng tr∆∞·ªùng h·ª£p l·ªói graph\n",
    "            \n",
    "        # Ch·ªâ gi·ªØ l·∫°i nh·ªØng t·ªï ti√™n C√ì M·∫∂T trong Vocab\n",
    "        valid_ancestors = []\n",
    "        for anc in all_ancestors:\n",
    "            if anc in vocab_set:\n",
    "                p_idx = term_to_idx[anc]\n",
    "                valid_ancestors.append(p_idx)\n",
    "                \n",
    "        if valid_ancestors:\n",
    "            child_to_parent[idx] = valid_ancestors\n",
    "            count_edges += len(valid_ancestors)\n",
    "\n",
    "    print(f\"   - Mapped {count_edges:,} transitive relationships (Bridged gaps).\")\n",
    "\n",
    "    # --- D. T√çNH IC NORM (INFORMATION CONTENT) - [S·ª¨A QUAN TR·ªåNG] ---\n",
    "    print(\"\\n[4/4] Calculating Information Content (IC)...\")\n",
    "    \n",
    "    # Load Targets\n",
    "    with open(CONFIG['TARGET_FILE'], 'rb') as f:\n",
    "        labels_dict = pickle.load(f)\n",
    "    \n",
    "    # ƒê·∫øm t·∫ßn su·∫•t\n",
    "    term_counts = np.zeros(vocab_size, dtype=np.float32)\n",
    "    total_samples = len(labels_dict)\n",
    "    \n",
    "    for indices in labels_dict.values():\n",
    "        if len(indices) > 0:\n",
    "            term_counts[indices] += 1\n",
    "            \n",
    "    # T√≠nh Frequency (C·ªông epsilon nh·ªè)\n",
    "    freq = (term_counts + 1e-9) / total_samples\n",
    "    \n",
    "    # IC = -log(Freq)\n",
    "    ic_values = -np.log(freq)\n",
    "    \n",
    "    # Normalize IC [0, 1]\n",
    "    ic_max = ic_values.max()\n",
    "    if ic_max > 0:\n",
    "        ic_norm = ic_values / ic_max\n",
    "    else:\n",
    "        ic_norm = np.zeros_like(ic_values)\n",
    "        \n",
    "    print(f\"   - IC Max Value: {ic_max:.4f}\")\n",
    "    print(f\"   - IC Norm Shape: {ic_norm.shape}\")\n",
    "\n",
    "    # --- E. L∆ØU FILE ---\n",
    "    print(f\"\\n>>> Saving to {CONFIG['OUTPUT_PKL']}...\")\n",
    "    \n",
    "    save_data = {\n",
    "        'depth_norm': depth_norm,\n",
    "        'ic_norm': ic_norm,          \n",
    "        'child_to_parent': dict(child_to_parent), # ƒê√£ fix logic is_a\n",
    "        'term_to_idx': term_to_idx\n",
    "    }\n",
    "    \n",
    "    with open(CONFIG['OUTPUT_PKL'], 'wb') as f:\n",
    "        pickle.dump(save_data, f)\n",
    "        \n",
    "    print(\"‚úÖ DONE! Metadata V2 (Final) Created Successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_metadata_v2_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4426563f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T19:21:58.931746Z",
     "iopub.status.busy": "2025-12-08T19:21:58.931446Z",
     "iopub.status.idle": "2025-12-08T19:21:58.937040Z",
     "shell.execute_reply": "2025-12-08T19:21:58.936014Z"
    },
    "papermill": {
     "duration": 0.013197,
     "end_time": "2025-12-08T19:21:58.939191",
     "exception": false,
     "start_time": "2025-12-08T19:21:58.925994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # --- N√äN ƒê·ªîI T√äN FILE CHO ƒê√öNG ---\n",
    "# # base_path   = \"/kaggle/input/sample-submission/0.275_submission.tsv\"\n",
    "# # boost350    = \"/kaggle/input/sample-submission/0.229_submission.tsv\"\n",
    "# # boost50     = \"/kaggle/input/sample-submission/0.233_submission.tsv\"\n",
    "# # low     = \"/kaggle/input/sample-submission/0.266_submission.tsv\"\n",
    "\n",
    "# submit = \"/kaggle/working/submission.tsv\"\n",
    "\n",
    "# def load_sub(path):\n",
    "#     return pd.read_csv(path, sep='\\t', names=['protein','term','score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac3ae90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T19:21:58.949747Z",
     "iopub.status.busy": "2025-12-08T19:21:58.949414Z",
     "iopub.status.idle": "2025-12-08T19:21:58.956390Z",
     "shell.execute_reply": "2025-12-08T19:21:58.955190Z"
    },
    "papermill": {
     "duration": 0.014411,
     "end_time": "2025-12-08T19:21:58.958099",
     "exception": false,
     "start_time": "2025-12-08T19:21:58.943688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # File b·∫°n v·ª´a t·∫°o\n",
    "# SUBMISSION_FILE = \"/kaggle/input/sample-submission/0.275_submission.tsv\"\n",
    "\n",
    "# def analyze_threshold_impact():\n",
    "#     print(f\"üìÇ ƒêang ƒë·ªçc file {SUBMISSION_FILE}...\")\n",
    "#     # ƒê·ªçc file (c·ªôt: Protein, Term, Score)\n",
    "#     df = pd.read_csv(SUBMISSION_FILE, sep='\\t', names=['Protein', 'Term', 'Score'])\n",
    "    \n",
    "#     total_rows = len(df)\n",
    "#     total_proteins = df['Protein'].nunique()\n",
    "    \n",
    "#     print(f\"‚úÖ ƒê√£ t·∫£i xong!\")\n",
    "#     print(f\"   - T·ªïng d√≤ng hi·ªán t·∫°i: {total_rows:,}\")\n",
    "#     print(f\"   - T·ªïng Protein: {total_proteins:,}\")\n",
    "#     print(f\"   - Trung b√¨nh nh√£n/Protein (Hi·ªán t·∫°i): {total_rows / total_proteins:.1f}\")\n",
    "    \n",
    "#     # ‚úÖ TH·ªêNG K√ä MIN / MAX LABELS PER PROTEIN\n",
    "#     labels_per_protein = df.groupby('Protein').size()\n",
    "#     min_labels = labels_per_protein.min()\n",
    "#     max_labels = labels_per_protein.max()\n",
    "    \n",
    "#     print(f\"\\nüìå TH·ªêNG K√ä LABELS / PROTEIN:\")\n",
    "#     print(f\"   - Min labels / protein : {min_labels}\")\n",
    "#     print(f\"   - Max labels / protein : {max_labels}\")\n",
    "    \n",
    "#     # --- TH·ªêNG K√ä NG∆Ø·ª†NG GI·∫¢ L·∫¨P ---\n",
    "#     print(\"\\nüìä N·∫æU TƒÇNG THRESHOLD TH√å SAO?\")\n",
    "#     print(\"-\" * 50)\n",
    "#     print(f\"{'Threshold':<10} | {'S·ªë d√≤ng c√≤n l·∫°i':<15} | {'Nh√£n/Protein':<15} | {'% Gi·ªØ l·∫°i':<10}\")\n",
    "#     print(\"-\" * 50)\n",
    "    \n",
    "#     # C√°c m·ªëc threshold mu·ªën th·ª≠\n",
    "#     thresholds = [0.01, 0.05, 0.08, 0.10, 0.12, 0.15, 0.35, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    \n",
    "#     for thr in thresholds:\n",
    "#         # L·ªçc gi·∫£ l·∫≠p\n",
    "#         filtered_count = (df['Score'] >= thr).sum()\n",
    "#         avg_labels = filtered_count / total_proteins\n",
    "#         percent_kept = (filtered_count / total_rows) * 100\n",
    "        \n",
    "#         print(f\"{thr:<10.2f} | {filtered_count:<15,} | {avg_labels:<15.1f} | {percent_kept:<9.1f}%\")\n",
    "        \n",
    "#     print(\"-\" * 50)\n",
    "    \n",
    "#     # --- V·∫º BI·ªÇU ƒê·ªí PH√ÇN B·ªê SCORE ---\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     sns.histplot(df['Score'], bins=100, color='teal', kde=False)\n",
    "#     plt.axvline(0.08, color='red', linestyle='--', label='Current Threshold (0.08)')\n",
    "#     plt.title(f'Ph√¢n b·ªë ƒëi·ªÉm s·ªë trong file {SUBMISSION_FILE}')\n",
    "#     plt.xlabel('Score')\n",
    "#     plt.ylabel('S·ªë l∆∞·ª£ng')\n",
    "#     plt.legend()\n",
    "#     plt.yscale('log') # D√πng log scale ƒë·ªÉ nh√¨n r√µ v√πng th·∫•p\n",
    "#     plt.show()\n",
    "\n",
    "# analyze_threshold_impact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573ea837",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T19:21:58.968115Z",
     "iopub.status.busy": "2025-12-08T19:21:58.967757Z",
     "iopub.status.idle": "2025-12-08T19:21:58.975263Z",
     "shell.execute_reply": "2025-12-08T19:21:58.974200Z"
    },
    "papermill": {
     "duration": 0.015023,
     "end_time": "2025-12-08T19:21:58.977298",
     "exception": false,
     "start_time": "2025-12-08T19:21:58.962275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # ========= FILE PATH =========\n",
    "# submission_path = \"submission.tsv\"\n",
    "# ia_path = \"/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\"\n",
    "\n",
    "# # ========= ROOT TERMS (B·ªé KH·ªéI TH·ªêNG K√ä) =========\n",
    "# ROOT_TERMS = {\"GO:0003674\", \"GO:0005575\", \"GO:0008150\"}\n",
    "\n",
    "# # ========= LOAD IA =========\n",
    "# df_ia = pd.read_csv(\n",
    "#     ia_path,\n",
    "#     sep=\"\\t\",\n",
    "#     header=None,\n",
    "#     names=[\"go_term\", \"ia\"]\n",
    "# )\n",
    "# ia_map = dict(zip(df_ia[\"go_term\"], df_ia[\"ia\"]))\n",
    "# print(\"‚úÖ S·ªë GO term c√≥ IA:\", len(ia_map))\n",
    "\n",
    "# # ========= IA BUCKET =========\n",
    "# ia_labels = [\"IA = 0\", \"0 < IA < 1\", \"1 ‚Üí 2\", \"2 ‚Üí 4\", \"‚â• 4\"]\n",
    "\n",
    "# # ========= SCORE BUCKET =========\n",
    "# score_bins = [(i/10, (i+1)/10) for i in range(0, 10)]\n",
    "# score_bins.append((\"==1.0\", \"==1.0\"))  # ‚úÖ bucket ƒë·∫∑c bi·ªát\n",
    "\n",
    "# results = {b: {k: 0 for k in ia_labels} for b in score_bins}\n",
    "# totals = {b: 0 for b in score_bins}\n",
    "\n",
    "# missing_ia = 0\n",
    "# skipped_root = 0\n",
    "\n",
    "# # ========= STREAM SUBMISSION =========\n",
    "# for chunk in pd.read_csv(\n",
    "#     submission_path,\n",
    "#     sep=\"\\t\",\n",
    "#     header=None,\n",
    "#     names=[\"protein_id\", \"go_term\", \"score\"],\n",
    "#     chunksize=2_000_000\n",
    "# ):\n",
    "#     for row in chunk.itertuples(index=False):\n",
    "#         pid, go, sc = row\n",
    "\n",
    "#         # ‚úÖ B·ªé ROOT\n",
    "#         if go in ROOT_TERMS:\n",
    "#             skipped_root += 1\n",
    "#             continue\n",
    "\n",
    "#         ia = ia_map.get(go, None)\n",
    "#         if ia is None:\n",
    "#             missing_ia += 1\n",
    "#             continue\n",
    "\n",
    "#         # ========= X√ÅC ƒê·ªäNH SCORE BUCKET =========\n",
    "#         if sc == 1.0:\n",
    "#             sb = (\"==1.0\", \"==1.0\")\n",
    "#         else:\n",
    "#             sb = None\n",
    "#             for (s_lo, s_hi) in score_bins[:-1]:\n",
    "#                 if s_lo <= sc < s_hi:\n",
    "#                     sb = (s_lo, s_hi)\n",
    "#                     break\n",
    "\n",
    "#             if sb is None:\n",
    "#                 continue\n",
    "\n",
    "#         # ========= IA BUCKET LOGIC =========\n",
    "#         if ia == 0:\n",
    "#             results[sb][\"IA = 0\"] += 1\n",
    "#         elif 0 < ia < 1:\n",
    "#             results[sb][\"0 < IA < 1\"] += 1\n",
    "#         elif 1 <= ia < 2:\n",
    "#             results[sb][\"1 ‚Üí 2\"] += 1\n",
    "#         elif 2 <= ia < 4:\n",
    "#             results[sb][\"2 ‚Üí 4\"] += 1\n",
    "#         else:\n",
    "#             results[sb][\"‚â• 4\"] += 1\n",
    "\n",
    "#         totals[sb] += 1\n",
    "\n",
    "# # ========= IN K·∫æT QU·∫¢ =========\n",
    "# for b in score_bins:\n",
    "#     if b == (\"==1.0\", \"==1.0\"):\n",
    "#         print(f\"\\nüî• PH√ÇN B·ªê IA (score == 1.0) [ƒê√É B·ªé ROOT]:\")\n",
    "#     else:\n",
    "#         print(f\"\\nüìä PH√ÇN B·ªê IA ({b[0]:.1f} ‚Üí {b[1]:.1f}) [ƒê√É B·ªé ROOT]:\")\n",
    "\n",
    "#     total = totals[b]\n",
    "#     if total == 0:\n",
    "#         print(\"  (Tr·ªëng)\")\n",
    "#         continue\n",
    "\n",
    "#     for k in ia_labels:\n",
    "#         cnt = results[b][k]\n",
    "#         pct = cnt / total * 100\n",
    "#         print(f\"{k:6s}: {cnt:>10,}  |  {pct:6.2f} %\")\n",
    "\n",
    "# print(\"\\n‚ö†Ô∏è S·ªë d√≤ng b·ªã thi·∫øu IA:\", missing_ia)\n",
    "# print(\"üö´ S·ªë d√≤ng b·ªã lo·∫°i v√¨ l√† ROOT:\", skipped_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65cda9a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T19:21:58.988529Z",
     "iopub.status.busy": "2025-12-08T19:21:58.988207Z",
     "iopub.status.idle": "2025-12-08T19:21:58.995197Z",
     "shell.execute_reply": "2025-12-08T19:21:58.994096Z"
    },
    "papermill": {
     "duration": 0.015325,
     "end_time": "2025-12-08T19:21:58.996916",
     "exception": false,
     "start_time": "2025-12-08T19:21:58.981591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # =========================================================\n",
    "# # C·∫§U H√åNH & THAM S·ªê\n",
    "# # =========================================================\n",
    "# SUB_FILE = \"submission.tsv\"\n",
    "# IA_FILE  = \"/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\"\n",
    "# CHUNK_SIZE = 5_000_000 \n",
    "# ROOT_TERMS = [\"GO:0003674\", \"GO:0005575\", \"GO:0008150\"]\n",
    "# SCORE_BINS = np.linspace(0.0, 1.0, 11) # T·∫°o 11 m·ªëc: 0.0, 0.1, 0.2, ..., 1.0\n",
    "\n",
    "# def analyze_ia_extremes_fixed():\n",
    "#     # 1. Load submission (Ph√°t hi·ªán c·ªôt)\n",
    "#     print(\"1. Reading submission to identify columns...\")\n",
    "#     df_head = pd.read_csv(SUB_FILE, sep=None, engine=\"python\", header=None, nrows=1000)\n",
    "\n",
    "#     go_col, score_col = None, None\n",
    "#     for col in df_head.columns:\n",
    "#         sample = df_head[col].astype(str)\n",
    "#         if sample.str.startswith(\"GO:\").any(): go_col = col\n",
    "#         if pd.to_numeric(sample, errors=\"coerce\").notna().mean() > 0.9: score_col = col\n",
    "\n",
    "#     if go_col is None or score_col is None:\n",
    "#         raise ValueError(\"Kh√¥ng t√¨m th·∫•y c·ªôt GO-term ho·∫∑c Score. Ki·ªÉm tra l·∫°i SUB_FILE.\")\n",
    "\n",
    "#     # 2. Load IA Map\n",
    "#     ia_df = pd.read_csv(IA_FILE, sep=\"\\t\", header=None, names=[\"go\", \"ia\"])\n",
    "#     ia_map = dict(zip(ia_df.go, ia_df.ia))\n",
    "#     print(f\"2. IA Map Loaded ({len(ia_map):,} terms).\")\n",
    "\n",
    "#     # 3. PH√ÇN T√çCH STREAMING\n",
    "#     ia_bin_stats = defaultdict(lambda: {'total_ia': 0.0, 'count': 0})\n",
    "\n",
    "#     print(\"\\n3. Starting Streaming Analysis (Binning Mean IA)...\")\n",
    "#     reader = pd.read_csv(SUB_FILE, sep=None, engine=\"python\", header=None, chunksize=CHUNK_SIZE)\n",
    "\n",
    "#     for chunk in tqdm(reader, desc=\"Processing Chunks\"):\n",
    "#         # ƒê·ªïi t√™n c·ªôt\n",
    "#         df_chunk = chunk.rename(columns={go_col: \"go\", score_col: \"score\"})\n",
    "        \n",
    "#         # L·ªçc 3 nh√£n g·ªëc\n",
    "#         df_chunk = df_chunk[~df_chunk['go'].isin(ROOT_TERMS)].copy()\n",
    "        \n",
    "#         # G√°n IA (Fill NaN = 0 cho an to√†n)\n",
    "#         df_chunk[\"ia\"] = df_chunk[\"go\"].map(ia_map).fillna(0.0)\n",
    "        \n",
    "#         # L·ªçc ƒëi·ªÉm s·ªë h·ª£p l·ªá\n",
    "#         df_chunk = df_chunk[(df_chunk['score'] >= 0) & (df_chunk['score'] <= 1.0)].copy()\n",
    "\n",
    "#         # T√≠nh to√°n BINS\n",
    "#         df_chunk['bin_index'] = np.digitize(df_chunk['score'], bins=SCORE_BINS) - 1 # ƒê·ªïi v·ªÅ index 0-9\n",
    "        \n",
    "#         # Nh√≥m theo index bin v√† t√≠nh t·ªïng IA v√† Count\n",
    "#         chunk_stats = df_chunk.groupby('bin_index').agg(\n",
    "#             total_ia=('ia', 'sum'),\n",
    "#             count=('ia', 'count')\n",
    "#         ).reset_index()\n",
    "\n",
    "#         # C·∫≠p nh·∫≠t global stats\n",
    "#         for _, row in chunk_stats.iterrows():\n",
    "#             # [FIX L·ªñI] √âp ki·ªÉu v·ªÅ int ti√™u chu·∫©n\n",
    "#             bin_idx = int(row['bin_index']) \n",
    "            \n",
    "#             # Ki·ªÉm tra l·ªói ngo√†i bi√™n (do np.digitize ƒë√¥i khi tr·∫£ v·ªÅ 10)\n",
    "#             if bin_idx >= len(SCORE_BINS) - 1 or bin_idx < 0:\n",
    "#                 continue \n",
    "            \n",
    "#             bin_min = SCORE_BINS[bin_idx]\n",
    "#             bin_max = SCORE_BINS[bin_idx + 1]\n",
    "            \n",
    "#             key = (bin_min, bin_max)\n",
    "#             ia_bin_stats[key]['total_ia'] += row['total_ia']\n",
    "#             ia_bin_stats[key]['count'] += row['count']\n",
    "\n",
    "\n",
    "#     # --- K·∫æT QU·∫¢ CU·ªêI C√ôNG ---\n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(f\"üìä PH√ÇN T√çCH MEAN IA THEO KHO·∫¢NG ƒêI·ªÇM S·ªê\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     total_all_counts = sum(s['count'] for s in ia_bin_stats.values())\n",
    "    \n",
    "#     print(f\"{'Score Range':<15} | {'Mean IA':<10} | {'Total Count':<15} | {'Density (%)':<10}\")\n",
    "#     print(\"-\" * 70)\n",
    "\n",
    "#     for (bin_min, bin_max), stats in sorted(ia_bin_stats.items()):\n",
    "#         count = stats['count']\n",
    "#         mean_ia = stats['total_ia'] / count if count > 0 else 0.0\n",
    "#         density = (count / total_all_counts) * 100 if total_all_counts > 0 else 0.0\n",
    "        \n",
    "#         range_str = f\"[{bin_min:.2f} - {bin_max:.2f})\"\n",
    "        \n",
    "#         print(f\"{range_str:<15} | {mean_ia:<10.4f} | {count:<15,} | {density:<9.1f}%\")\n",
    "\n",
    "#     print(\"=\"*70)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     analyze_ia_extremes_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cd7711",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T19:21:59.008397Z",
     "iopub.status.busy": "2025-12-08T19:21:59.008036Z",
     "iopub.status.idle": "2025-12-08T19:21:59.018773Z",
     "shell.execute_reply": "2025-12-08T19:21:59.017656Z"
    },
    "papermill": {
     "duration": 0.01902,
     "end_time": "2025-12-08T19:21:59.020565",
     "exception": false,
     "start_time": "2025-12-08T19:21:59.001545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from torch import amp\n",
    "\n",
    "# =========================================================\n",
    "# C·∫§U H√åNH DUMP \n",
    "# =========================================================\n",
    "CONFIG = {\n",
    "    'MODEL1_PATH': \"/kaggle/input/model-cafa6/best_model_wide_0.63.pth\",     \n",
    "    'MODEL2_PATH': \"/kaggle/input/model-cafa6/model_v3_hmc_warmup.pth\",  \n",
    "    'MODEL3_PATH': \"/kaggle/input/model-cafa6/best_model_c99_wide.pth\",  \n",
    "    \n",
    "    'EMBED_DIR': \"/kaggle/input/cafa6-embeds\",\n",
    "    'VOCAB_C95': \"/kaggle/input/c95-cafa6/vocab_C95_remove.csv\",\n",
    "    'VOCAB_C99': \"/kaggle/input/c99-cafa6/vocab_C99_remove.csv\",\n",
    "    'IA_FILE': \"/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\",\n",
    "    \n",
    "    # Weights & Gating\n",
    "    'BASE_W1': 0.6, 'BASE_W2': 0.1, 'BASE_W3': 0.3,\n",
    "    'IA_HIGH_THRESHOLD': 2.0,\n",
    "    'C99_CONFIDENCE_MIN': 0.12, \n",
    "    'C99_DELTA_MIN': 0.02,\n",
    "    \n",
    "    # DUMP CONFIG\n",
    "    'DUMP_THRESHOLD': 0.01, # L·∫•y h·∫øt t√≠n hi·ªáu > 1% ƒë·ªÉ v·ªÅ nh√† l·ªçc sau\n",
    "    \n",
    "    'device': 'cuda', 'batch_size': 128, 'num_workers': 4\n",
    "}\n",
    "\n",
    "class WideProteinMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden_dims=[2048, 4096], dropout=0.3):\n",
    "        super().__init__()\n",
    "        layers = [nn.LayerNorm(input_dim)]\n",
    "        prev = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers += [nn.Linear(prev, h), nn.GELU(), nn.Dropout(dropout)]\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, num_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, embed_dir):\n",
    "        with open(os.path.join(embed_dir, \"test_ids.txt\")) as f: self.ids = [line.strip() for line in f]\n",
    "        self.embed_matrix = np.load(os.path.join(embed_dir, \"test_embeds.npy\"), mmap_mode=\"r\")\n",
    "    def __len__(self): return len(self.ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.embed_matrix[idx].copy()).float(), self.ids[idx]\n",
    "\n",
    "def load_and_clean_model(path, num_classes, device):\n",
    "    m = WideProteinMLP(1280, num_classes).to(device)\n",
    "    try:\n",
    "        ckpt = torch.load(path, map_location='cpu')\n",
    "        sd = ckpt['model_state'] if isinstance(ckpt, dict) and 'model_state' in ckpt else ckpt\n",
    "        new_sd = OrderedDict()\n",
    "        for k, v in sd.items():\n",
    "            if k.startswith('module.'): new_sd[k[7:]] = v\n",
    "            else: new_sd[k] = v\n",
    "        m.load_state_dict(new_sd)\n",
    "        m.eval()\n",
    "        return m\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_fast_dump_v2():\n",
    "    print(\"üöÄ PHASE 1: GPU-ACCELERATED RAW DUMP...\")\n",
    "    device = CONFIG['device']\n",
    "    \n",
    "    # 1. Load Vocabs & Indexing\n",
    "    df_c95 = pd.read_csv(CONFIG['VOCAB_C95'])\n",
    "    df_c99 = pd.read_csv(CONFIG['VOCAB_C99'])\n",
    "    c95_terms = df_c95['term'].tolist()\n",
    "    c99_terms = np.array(df_c99['term'].tolist())\n",
    "    \n",
    "    # Mapping Indices\n",
    "    c99_term_to_idx = {t: i for i, t in enumerate(c99_terms)}\n",
    "    c95_to_c99_indices = []\n",
    "    valid_c95_indices = []\n",
    "    for i, t in enumerate(c95_terms):\n",
    "        if t in c99_term_to_idx:\n",
    "            c95_to_c99_indices.append(c99_term_to_idx[t])\n",
    "            valid_c95_indices.append(i)\n",
    "    c95_to_c99_indices = torch.tensor(c95_to_c99_indices, device=device, dtype=torch.long)\n",
    "    valid_c95_indices = torch.tensor(valid_c95_indices, device=device, dtype=torch.long)\n",
    "    \n",
    "    # IA Vector\n",
    "    try:\n",
    "        ia_df = pd.read_csv(CONFIG['IA_FILE'], sep='\\t', names=['term', 'ia'], header=None)\n",
    "        ia_map = dict(zip(ia_df.term, ia_df.ia))\n",
    "        ia_vector = np.array([ia_map.get(t, 0.0) for t in c99_terms], dtype=np.float32)\n",
    "        ia_tensor = torch.tensor(ia_vector, device=device)\n",
    "    except:\n",
    "        ia_tensor = torch.zeros(len(c99_terms), device=device)\n",
    "\n",
    "    # 2. Load Models \n",
    "    m1 = load_and_clean_model(CONFIG['MODEL1_PATH'], len(c95_terms), device)\n",
    "    m2 = load_and_clean_model(CONFIG['MODEL2_PATH'], len(c95_terms), device)\n",
    "    m3 = load_and_clean_model(CONFIG['MODEL3_PATH'], len(c99_terms), device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        m1 = nn.DataParallel(m1); m2 = nn.DataParallel(m2); m3 = nn.DataParallel(m3)\n",
    "\n",
    "    # 3. Inference Loop (GPU ACCELERATED)\n",
    "    ds = TestDataset(CONFIG['EMBED_DIR'])\n",
    "    dl = DataLoader(ds, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4)\n",
    "    \n",
    "    f_out = open(\"raw_predictions.tsv\", \"w\")\n",
    "    \n",
    "    DUMP_K = 500      \n",
    "    DUMP_THR = CONFIG['DUMP_THRESHOLD'] \n",
    "    \n",
    "    print(\">>> Streaming & Dumping (GPU TopK Mode)...\")\n",
    "    with torch.no_grad():\n",
    "        for features, prot_ids in tqdm(dl):\n",
    "            features = features.to(device)\n",
    "            with amp.autocast(device_type='cuda'):\n",
    "                p1_raw = torch.sigmoid(m1(features))\n",
    "                p2_raw = torch.sigmoid(m2(features))\n",
    "                p3 = torch.sigmoid(m3(features))\n",
    "            \n",
    "            # --- BLENDING ON GPU ---\n",
    "            final_probs = torch.zeros_like(p3)\n",
    "            # Map C95 -> C99\n",
    "            p1_mapped = torch.zeros_like(p3)\n",
    "            p2_mapped = torch.zeros_like(p3)\n",
    "            p1_mapped.index_add_(1, c95_to_c99_indices, p1_raw[:, valid_c95_indices])\n",
    "            p2_mapped.index_add_(1, c95_to_c99_indices, p2_raw[:, valid_c95_indices])\n",
    "            \n",
    "            # Gating Logic\n",
    "            high_ia = ia_tensor >= CONFIG['IA_HIGH_THRESHOLD']\n",
    "            conf_c99 = p3 > CONFIG['C99_CONFIDENCE_MIN']\n",
    "            better_c99 = p3 > (p1_mapped + CONFIG['C99_DELTA_MIN'])\n",
    "            override_mask = high_ia & conf_c99 & better_c99\n",
    "            \n",
    "            base_prob = (CONFIG['BASE_W1'] * p1_mapped) + (CONFIG['BASE_W2'] * p2_mapped) + (CONFIG['BASE_W3'] * p3)\n",
    "            override_prob = (0.2 * p1_mapped) + (0.8 * p3)\n",
    "            final_probs = torch.where(override_mask, override_prob, base_prob)\n",
    "            \n",
    "            # --- [GPU ACCELERATION START] ---\n",
    "            \n",
    "            # 1. Mask c√°c gi√° tr·ªã d∆∞·ªõi threshold th√†nh -1 ƒë·ªÉ TopK kh√¥ng l·∫•y nh·∫ßm\n",
    "            mask_low = final_probs < DUMP_THR\n",
    "            final_probs.masked_fill_(mask_low, -1.0)\n",
    "            \n",
    "            # 2. L·∫•y Top K tr√™n GPU (C·ª±c nhanh)\n",
    "            topk_vals, topk_inds = torch.topk(final_probs, k=DUMP_K, dim=1)\n",
    "            \n",
    "            # 3. Chuy·ªÉn k·∫øt qu·∫£ nh·ªè g·ªçn v·ªÅ CPU\n",
    "            vals_np = topk_vals.float().cpu().numpy()\n",
    "            inds_np = topk_inds.cpu().numpy()\n",
    "            \n",
    "            # --- [WRITING BUFFER] ---\n",
    "            batch_lines = []\n",
    "            for i, pid in enumerate(prot_ids):\n",
    "                # Duy·ªát qua Top K c·ªßa protein i\n",
    "                for j in range(DUMP_K):\n",
    "                    score = vals_np[i, j]\n",
    "                    idx = inds_np[i, j]\n",
    "                    \n",
    "                    # N·∫øu ƒëi·ªÉm < Threshold (do b·ªã fill -1 ho·∫∑c K qu√° l·ªõn), d·ª´ng l·∫°i\n",
    "                    if score < DUMP_THR: \n",
    "                        continue\n",
    "                        \n",
    "                    term = c99_terms[idx]\n",
    "                    batch_lines.append(f\"{pid}\\t{term}\\t{score:.4f}\\n\")\n",
    "\n",
    "            if batch_lines:\n",
    "                f_out.write(\"\".join(batch_lines))\n",
    "\n",
    "    f_out.close()\n",
    "    print(\"‚úÖ PHASE 1 DONE! Saved 'raw_predictions.tsv'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_fast_dump_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf217542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T19:21:59.030954Z",
     "iopub.status.busy": "2025-12-08T19:21:59.030570Z",
     "iopub.status.idle": "2025-12-08T19:28:46.383569Z",
     "shell.execute_reply": "2025-12-08T19:28:46.382397Z"
    },
    "papermill": {
     "duration": 407.36048,
     "end_time": "2025-12-08T19:28:46.385275",
     "exception": false,
     "start_time": "2025-12-08T19:21:59.024795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ PHASE 2: OFFLINE REPAIR & FILTER...\n",
      "   Loading Metadata...\n",
      "   Reading & Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 113it [06:47,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ REPAIR DONE! Rows: 39,543,118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# =========================================================\n",
    "# C·∫§U H√åNH POST-PROCESS\n",
    "# =========================================================\n",
    "CONFIG_POST = {\n",
    "    'RAW_FILE': \"/kaggle/input/sample-submission/raw_predictions_500.tsv\",\n",
    "    'OUTPUT_FILE': \"submission.tsv\",\n",
    "    'METADATA_PKL': \"hierarchy_metadata.pkl\",\n",
    "    \n",
    "    # --- LOGIC L·ªåC CU·ªêI C√ôNG ---\n",
    "    'FINAL_THRESHOLD': 0.20, \n",
    "    'FINAL_CAP': 180,        # C·∫Øt ƒëu√¥i\n",
    "    'MIN_LABELS': 1,         # Safety\n",
    "    'ROOT_TERMS': [\"GO:0003674\", \"GO:0005575\", \"GO:0008150\"]\n",
    "}\n",
    "\n",
    "def run_offline_repair():\n",
    "    print(\"üöÄ PHASE 2: OFFLINE REPAIR & FILTER...\")\n",
    "    \n",
    "    # 1. Load Metadata\n",
    "    print(\"   Loading Metadata...\")\n",
    "    with open(CONFIG_POST['METADATA_PKL'], 'rb') as f: meta = pickle.load(f)\n",
    "    if 'child_to_parent' in meta: parent_map = meta['child_to_parent']\n",
    "    else: parent_map = meta.get('child_to_parent_idx', {})\n",
    "    \n",
    "   \n",
    "    vocab_df = pd.read_csv(\"/kaggle/input/c99-cafa6/vocab_C99_remove.csv\") \n",
    "    term_to_idx = {t: i for i, t in enumerate(vocab_df['term'])}\n",
    "    idx_to_term = {i: t for i, t in enumerate(vocab_df['term'])}\n",
    "    \n",
    "    # 2. Process File (Group by Protein)\n",
    "    print(\"   Reading & Processing...\")\n",
    "    \n",
    "    f_out = open(CONFIG_POST['OUTPUT_FILE'], \"w\")\n",
    "    \n",
    "    # ƒê·ªçc file raw\n",
    "    reader = pd.read_csv(CONFIG_POST['RAW_FILE'], sep='\\t', names=['PID', 'Term', 'Score'], \n",
    "                         chunksize=1000000)\n",
    "    \n",
    "    \n",
    "    current_pid = None\n",
    "    current_data = [] \n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    # H√†m x·ª≠ l√Ω 1 protein\n",
    "    def process_protein(pid, data_list):\n",
    "        if not data_list: return 0\n",
    "        \n",
    "        # 1. Convert to Dict\n",
    "        scores = {}\n",
    "        for t_idx, s in data_list:\n",
    "            scores[t_idx] = s\n",
    "            \n",
    "        # 2. Global Repair (BFS)\n",
    "        queue = list(scores.keys())\n",
    "        processed = set(queue)\n",
    "        idx_ptr = 0\n",
    "    \n",
    "        \n",
    "        while idx_ptr < len(queue):\n",
    "            c_idx = queue[idx_ptr]; idx_ptr += 1\n",
    "            c_score = scores[c_idx]\n",
    "            \n",
    "            for p_idx in parent_map.get(c_idx, []):\n",
    "                p_prev = scores.get(p_idx, 0.0) # 0.0 n·∫øu ch∆∞a c√≥ (v√¨ raw ƒë√£ c·∫Øt 0.01)\n",
    "                \n",
    "                new_score = max(p_prev, c_score)\n",
    "                if new_score > p_prev + 1e-6:\n",
    "                    scores[p_idx] = new_score\n",
    "                    if p_idx not in processed:\n",
    "                        queue.append(p_idx); processed.add(p_idx)\n",
    "        \n",
    "        # 3. Filter & Cap\n",
    "        sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Threshold\n",
    "        filtered = [(idx, s) for idx, s in sorted_items if s >= CONFIG_POST['FINAL_THRESHOLD']]\n",
    "        \n",
    "        # Min Label Fallback\n",
    "        if len(filtered) < CONFIG_POST['MIN_LABELS']:\n",
    "            filtered = sorted_items[:CONFIG_POST['MIN_LABELS']]\n",
    "            \n",
    "        # Cap\n",
    "        if len(filtered) > CONFIG_POST['FINAL_CAP']:\n",
    "            filtered = filtered[:CONFIG_POST['FINAL_CAP']]\n",
    "            \n",
    "        # Write\n",
    "        lines = []\n",
    "        for idx, s in filtered:\n",
    "            t_str = idx_to_term[idx]\n",
    "            lines.append(f\"{pid}\\t{t_str}\\t{s:.3f}\\n\")\n",
    "        \n",
    "        f_out.write(\"\".join(lines))\n",
    "        return len(lines)\n",
    "\n",
    "    # LOOP CH√çNH\n",
    "    for chunk in tqdm(reader, desc=\"Processing Chunks\"):\n",
    "        for row in chunk.itertuples(index=False):\n",
    "            pid = row.PID\n",
    "            term = row.Term\n",
    "            score = row.Score\n",
    "            \n",
    "            # Map term string -> int index \n",
    "            if term not in term_to_idx: continue \n",
    "            t_idx = term_to_idx[term]\n",
    "            \n",
    "            if pid != current_pid:\n",
    "                if current_pid is not None:\n",
    "                    count += process_protein(current_pid, current_data)\n",
    "                \n",
    "                # Reset\n",
    "                current_pid = pid\n",
    "                current_data = []\n",
    "            \n",
    "            current_data.append((t_idx, score))\n",
    "            \n",
    "    # X·ª≠ l√Ω protein cu·ªëi c√πng\n",
    "    if current_pid is not None:\n",
    "        count += process_protein(current_pid, current_data)\n",
    "        \n",
    "    f_out.close()\n",
    "    print(f\"\\n‚úÖ REPAIR DONE! Rows: {count:,}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_offline_repair()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8335f71c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T19:28:46.408434Z",
     "iopub.status.busy": "2025-12-08T19:28:46.407919Z",
     "iopub.status.idle": "2025-12-08T19:28:46.416779Z",
     "shell.execute_reply": "2025-12-08T19:28:46.414551Z"
    },
    "papermill": {
     "duration": 0.023791,
     "end_time": "2025-12-08T19:28:46.418964",
     "exception": false,
     "start_time": "2025-12-08T19:28:46.395173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "# # =========================================================\n",
    "# # C·∫§U H√åNH KI·ªÇM TRA\n",
    "# # =========================================================\n",
    "# SUB_FILE = \"submission.tsv\"\n",
    "# TEST_IDS_FILE = \"/kaggle/input/cafa6-embeds/test_ids.txt\" # Thay b·∫±ng ƒë∆∞·ªùng d·∫´n file ID th·ª±c t·∫ø\n",
    "# CHUNK_SIZE = 5_000_000 \n",
    "\n",
    "# # =========================================================\n",
    "# # H√ÄM KI·ªÇM TRA\n",
    "# # =========================================================\n",
    "\n",
    "# def check_protein_coverage(submission_path, test_ids_path):\n",
    "#     print(f\"üî¨ B·∫Øt ƒë·∫ßu ki·ªÉm tra file: {submission_path}\")\n",
    "    \n",
    "#     if not os.path.exists(submission_path):\n",
    "#         print(f\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file submission t·∫°i: {submission_path}\")\n",
    "#         return False\n",
    "        \n",
    "#     # 1. Load danh s√°ch ID B·∫ÆT BU·ªòC (224,309 ID)\n",
    "#     try:\n",
    "#         with open(test_ids_path, 'r') as f:\n",
    "#             required_pids = set(line.strip() for line in f)\n",
    "        \n",
    "#         TOTAL_REQUIRED = len(required_pids)\n",
    "#         print(f\"‚úÖ ƒê√£ load {TOTAL_REQUIRED:,} Protein ID b·∫Øt bu·ªôc.\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå L·ªñI: Kh√¥ng th·ªÉ ƒë·ªçc danh s√°ch ID test t·ª´ {test_ids_path}. L·ªói: {e}\")\n",
    "#         return False\n",
    "\n",
    "#     # 2. ƒê·ªçc file submission theo chunks v√† thu th·∫≠p ID\n",
    "#     found_pids = set()\n",
    "#     total_submission_rows = 0\n",
    "    \n",
    "#     try:\n",
    "#         # S·ª≠ d·ª•ng engine='python' v√¨ file TSV c√≥ th·ªÉ c√≥ l·ªói ƒë·ªãnh d·∫°ng nh·∫π\n",
    "#         reader = pd.read_csv(submission_path, sep='\\t', header=None, \n",
    "#                               usecols=[0], names=['Protein'], \n",
    "#                               chunksize=CHUNK_SIZE, engine='python')\n",
    "        \n",
    "#         # Qu√©t qua t·ª´ng chunk\n",
    "#         for chunk in tqdm(reader, desc=\"Scanning PIDs in Submission\"):\n",
    "#             total_submission_rows += len(chunk)\n",
    "#             # Th√™m c√°c ID duy nh·∫•t t·ª´ chunk v√†o t·∫≠p h·ª£p\n",
    "#             found_pids.update(chunk['Protein'].astype(str).unique())\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå L·ªñI: L·ªói khi ƒë·ªçc file submission theo chunk. L·ªói: {e}\")\n",
    "#         print(\"   => Vui l√≤ng ki·ªÉm tra l·∫°i c·∫•u tr√∫c file submission (TAB separator).\")\n",
    "#         return False\n",
    "\n",
    "#     # 3. Ph√¢n t√≠ch k·∫øt qu·∫£\n",
    "#     TOTAL_FOUND = len(found_pids)\n",
    "    \n",
    "#     # Ki·ªÉm tra s·ªë l∆∞·ª£ng Protein ID b·ªã thi·∫øu\n",
    "#     missing_pids = required_pids - found_pids\n",
    "#     TOTAL_MISSING = len(missing_pids)\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*50)\n",
    "#     print(f\"üìä K·∫æT QU·∫¢ KI·ªÇM TRA PH·ª¶ S√ìNG PROTEIN\")\n",
    "#     print(\"=\"*50)\n",
    "#     print(f\"1. T·ªïng ID c·∫ßn thi·∫øt: {TOTAL_REQUIRED:,}\")\n",
    "#     print(f\"2. T·ªïng ID t√¨m th·∫•y: {TOTAL_FOUND:,}\")\n",
    "#     print(f\"3. T·ªïng d√≤ng Submission: {total_submission_rows:,}\")\n",
    "    \n",
    "#     if TOTAL_MISSING == 0:\n",
    "#         print(\"‚úÖ TH√ÄNH C√îNG: File submission ƒë√£ bao g·ªìm ƒê·∫¶Y ƒê·ª¶ 224,309 Protein ID.\")\n",
    "#     else:\n",
    "#         print(f\"‚ùå THI·∫æU {TOTAL_MISSING:,} Protein ID!\")\n",
    "#         # In ra 5 ID b·ªã thi·∫øu ƒë·∫ßu ti√™n l√†m v√≠ d·ª•\n",
    "#         print(f\"   => 5 ID b·ªã thi·∫øu l√†m v√≠ d·ª•: {list(missing_pids)[:5]}\")\n",
    "        \n",
    "#     print(\"=\"*50)\n",
    "#     return TOTAL_MISSING == 0\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # CH·∫†Y H√ÄM KI·ªÇM TRA (B·∫°n c·∫ßn thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n)\n",
    "#     # V√≠ d·ª• m·∫´u:\n",
    "#     # check_protein_coverage(\"submission.tsv\", \"/kaggle/input/cafa6-embeds/test_ids.txt\")\n",
    "    \n",
    "#     # Gi·∫£ s·ª≠ file ID test l√† 224,309 ID\n",
    "#     # T·ª∞ CH·∫†Y H√ÄM N√ÄY SAU KHI THAY ƒê·ªîI ƒê∆Ø·ªúNG D·∫™N\n",
    "    \n",
    "    \n",
    "#     check_protein_coverage(SUB_FILE, TEST_IDS_FILE)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9c9ae",
   "metadata": {
    "papermill": {
     "duration": 0.009506,
     "end_time": "2025-12-08T19:28:46.438842",
     "exception": false,
     "start_time": "2025-12-08T19:28:46.429336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bdaa0b",
   "metadata": {
    "papermill": {
     "duration": 0.009443,
     "end_time": "2025-12-08T19:28:46.457822",
     "exception": false,
     "start_time": "2025-12-08T19:28:46.448379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14084779,
     "isSourceIdPinned": false,
     "sourceId": 116062,
     "sourceType": "competition"
    },
    {
     "datasetId": 8916743,
     "sourceId": 13989811,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8917141,
     "sourceId": 13990998,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8917158,
     "sourceId": 13991073,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8919436,
     "sourceId": 14054871,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8908959,
     "sourceId": 14064668,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 433.235826,
   "end_time": "2025-12-08T19:28:47.089406",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-08T19:21:33.853580",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
