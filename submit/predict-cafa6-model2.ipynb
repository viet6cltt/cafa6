{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e2144b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-06T20:23:35.190825Z",
     "iopub.status.busy": "2025-12-06T20:23:35.190613Z",
     "iopub.status.idle": "2025-12-06T20:23:35.196629Z",
     "shell.execute_reply": "2025-12-06T20:23:35.195960Z"
    },
    "papermill": {
     "duration": 0.011374,
     "end_time": "2025-12-06T20:23:35.197773",
     "exception": false,
     "start_time": "2025-12-06T20:23:35.186399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install obonet networkx --quiet\n",
    "\n",
    "# import obonet\n",
    "# import networkx as nx\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# from collections import defaultdict, deque\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # ============================================================================\n",
    "# # 1. C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N (CHECK K·ª∏)\n",
    "# # ============================================================================\n",
    "# CONFIG = {\n",
    "#     # File OBO g·ªëc\n",
    "#     'OBO_FILE': \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\",\n",
    "    \n",
    "#     'VOCAB_FILE': \"/kaggle/input/c95-cafa6/vocab_C95_remove.csv\",\n",
    "#     'TARGET_FILE': \"/kaggle/input/c95-cafa6/train_targets_C95.pkl\",\n",
    "    \n",
    "#     # File ƒë·∫ßu ra\n",
    "#     'OUTPUT_PKL': \"hierarchy_metadata_v2.pkl\"\n",
    "# }\n",
    "\n",
    "# def build_metadata_v2_final():\n",
    "#     print(\"üöÄ B·∫ÆT ƒê·∫¶U T·∫†O METADATA V2 (FULL & CORRECTED)...\")\n",
    "    \n",
    "#     # --- A. LOAD D·ªÆ LI·ªÜU ---\n",
    "#     print(\"\\n[1/4] Loading Graph & Vocab...\")\n",
    "#     graph = obonet.read_obo(CONFIG['OBO_FILE'])\n",
    "#     vocab_df = pd.read_csv(CONFIG['VOCAB_FILE'])\n",
    "#     vocab_terms = vocab_df['term'].tolist()\n",
    "#     vocab_size = len(vocab_terms)\n",
    "#     term_to_idx = {t: i for i, t in enumerate(vocab_terms)}\n",
    "    \n",
    "#     print(f\"   - Graph nodes: {len(graph):,}\")\n",
    "#     print(f\"   - Vocab size: {vocab_size:,}\")\n",
    "\n",
    "#     # --- B. T√çNH DEPTH NORM (BFS) ---\n",
    "#     print(\"\\n[2/4] Calculating Depth Norm...\")\n",
    "#     # (Ph·∫ßn n√†y code c≈© ƒë√£ ƒë√∫ng, gi·ªØ nguy√™n logic BFS)\n",
    "#     roots = [\"GO:0008150\", \"GO:0003674\", \"GO:0005575\"]\n",
    "#     depth_dict = {node: float('inf') for node in graph.nodes()}\n",
    "#     queue = deque()\n",
    "    \n",
    "#     for root in roots:\n",
    "#         if root in graph:\n",
    "#             depth_dict[root] = 0\n",
    "#             queue.append(root)\n",
    "            \n",
    "#     while queue:\n",
    "#         node = queue.popleft()\n",
    "#         d = depth_dict[node]\n",
    "#         # Predecessors = CON (Edge ƒëi t·ª´ con -> cha)\n",
    "#         for child in graph.predecessors(node):\n",
    "#             if depth_dict[child] > d + 1:\n",
    "#                 depth_dict[child] = d + 1\n",
    "#                 queue.append(child)\n",
    "                \n",
    "#     depth_arr = np.array([depth_dict.get(t, 0) for t in vocab_terms], dtype=np.float32)\n",
    "#     depth_arr[depth_arr == float('inf')] = 0.0\n",
    "    \n",
    "#     if depth_arr.max() > 0:\n",
    "#         depth_norm = depth_arr / depth_arr.max()\n",
    "#     else:\n",
    "#         depth_norm = depth_arr\n",
    "#     print(f\"   - Max Depth: {depth_arr.max()}\")\n",
    "\n",
    "#     # --- C. T√çNH PARENT MAP (TRANSITIVE CLOSURE) - [N√ÇNG C·∫§P] ---\n",
    "#     print(\"\\n[3/4] Building Parent Map (Transitive - Fix Broken Chains)...\")\n",
    "    \n",
    "#     # 1. X√¢y d·ª±ng ƒë·ªì th·ªã 'is_a' to√†n v·∫πn t·ª´ file OBO g·ªëc\n",
    "#     # (ƒê·ªÉ t√¨m ƒë∆∞·ªùng ƒëi ngay c·∫£ khi node trung gian b·ªã c·∫Øt)\n",
    "#     full_isa_graph = nx.DiGraph()\n",
    "    \n",
    "#     print(\"   - Building full is_a graph...\")\n",
    "#     for node, data in graph.nodes(data=True):\n",
    "#         if \"is_a\" in data:\n",
    "#             for p_str in data[\"is_a\"]:\n",
    "#                 p_id = p_str.split(\" ! \")[0]\n",
    "#                 # Th√™m c·∫°nh Con -> Cha\n",
    "#                 full_isa_graph.add_edge(node, p_id)\n",
    "                \n",
    "#     # 2. Map Con -> T·∫•t c·∫£ T·ªï ti√™n (Ancestors) c√≥ trong Vocab\n",
    "#     child_to_parent = defaultdict(list)\n",
    "#     count_edges = 0\n",
    "#     vocab_set = set(vocab_terms) # ƒê·ªÉ tra c·ª©u nhanh\n",
    "    \n",
    "#     for term, idx in tqdm(term_to_idx.items(), desc=\"Mapping Transitive\"):\n",
    "#         if term not in full_isa_graph: continue\n",
    "        \n",
    "#         # T√¨m t·∫•t c·∫£ t·ªï ti√™n trong ƒë·ªì th·ªã g·ªëc (bao g·ªìm c·∫£ cha, √¥ng, c·ª•...)\n",
    "#         # nx.descendants trong DiGraph(Con->Cha) s·∫Ω tr·∫£ v·ªÅ t·∫•t c·∫£ Ancestors\n",
    "#         try:\n",
    "#             all_ancestors = nx.descendants(full_isa_graph, term)\n",
    "#         except:\n",
    "#             continue # Ph√≤ng tr∆∞·ªùng h·ª£p l·ªói graph\n",
    "            \n",
    "#         # Ch·ªâ gi·ªØ l·∫°i nh·ªØng t·ªï ti√™n C√ì M·∫∂T trong Vocab\n",
    "#         valid_ancestors = []\n",
    "#         for anc in all_ancestors:\n",
    "#             if anc in vocab_set:\n",
    "#                 p_idx = term_to_idx[anc]\n",
    "#                 valid_ancestors.append(p_idx)\n",
    "                \n",
    "#         if valid_ancestors:\n",
    "#             child_to_parent[idx] = valid_ancestors\n",
    "#             count_edges += len(valid_ancestors)\n",
    "\n",
    "#     print(f\"   - Mapped {count_edges:,} transitive relationships (Bridged gaps).\")\n",
    "\n",
    "#     # --- D. T√çNH IC NORM (INFORMATION CONTENT) - [S·ª¨A QUAN TR·ªåNG] ---\n",
    "#     print(\"\\n[4/4] Calculating Information Content (IC)...\")\n",
    "    \n",
    "#     # Load Targets\n",
    "#     with open(CONFIG['TARGET_FILE'], 'rb') as f:\n",
    "#         labels_dict = pickle.load(f)\n",
    "    \n",
    "#     # ƒê·∫øm t·∫ßn su·∫•t\n",
    "#     term_counts = np.zeros(vocab_size, dtype=np.float32)\n",
    "#     total_samples = len(labels_dict)\n",
    "    \n",
    "#     for indices in labels_dict.values():\n",
    "#         if len(indices) > 0:\n",
    "#             term_counts[indices] += 1\n",
    "            \n",
    "#     # T√≠nh Frequency (C·ªông epsilon nh·ªè)\n",
    "#     freq = (term_counts + 1e-9) / total_samples\n",
    "    \n",
    "#     # IC = -log(Freq)\n",
    "#     ic_values = -np.log(freq)\n",
    "    \n",
    "#     # Normalize IC [0, 1]\n",
    "#     ic_max = ic_values.max()\n",
    "#     if ic_max > 0:\n",
    "#         ic_norm = ic_values / ic_max\n",
    "#     else:\n",
    "#         ic_norm = np.zeros_like(ic_values)\n",
    "        \n",
    "#     print(f\"   - IC Max Value: {ic_max:.4f}\")\n",
    "#     print(f\"   - IC Norm Shape: {ic_norm.shape}\")\n",
    "\n",
    "#     # --- E. L∆ØU FILE ---\n",
    "#     print(f\"\\n>>> Saving to {CONFIG['OUTPUT_PKL']}...\")\n",
    "    \n",
    "#     save_data = {\n",
    "#         'depth_norm': depth_norm,\n",
    "#         'ic_norm': ic_norm,           # ƒê√£ c√≥ IC cho V4\n",
    "#         'child_to_parent': dict(child_to_parent), # ƒê√£ fix logic is_a\n",
    "#         'term_to_idx': term_to_idx\n",
    "#     }\n",
    "    \n",
    "#     with open(CONFIG['OUTPUT_PKL'], 'wb') as f:\n",
    "#         pickle.dump(save_data, f)\n",
    "        \n",
    "#     print(\"‚úÖ DONE! Metadata V2 (Final) Created Successfully.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     build_metadata_v2_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ad40a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T20:23:35.204117Z",
     "iopub.status.busy": "2025-12-06T20:23:35.203869Z",
     "iopub.status.idle": "2025-12-06T20:23:35.212508Z",
     "shell.execute_reply": "2025-12-06T20:23:35.211959Z"
    },
    "papermill": {
     "duration": 0.012952,
     "end_time": "2025-12-06T20:23:35.213466",
     "exception": false,
     "start_time": "2025-12-06T20:23:35.200514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from tqdm import tqdm\n",
    "# import pickle\n",
    "# import gc\n",
    "# from collections import OrderedDict\n",
    "# from torch.cuda.amp import autocast\n",
    "\n",
    "# # ============================================================================\n",
    "# # 1. C·∫§U H√åNH SUPER-BOOST V4\n",
    "# # ============================================================================\n",
    "# CONFIG = {\n",
    "#     # --- Model ---\n",
    "#     'input_dim': 1280,\n",
    "#     'hidden_dims': [2048, 4096],\n",
    "#     'dropout': 0.3,\n",
    "#     'num_classes': 6413, # C95_remove\n",
    "\n",
    "#     # --- System ---\n",
    "#     'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "#     'batch_size': 128, \n",
    "#     'num_workers': 4,\n",
    "\n",
    "#     # --- Paths ---\n",
    "#     'MODEL_PATH': \"/kaggle/input/model-cafa6/best_model_wide_0.63.pth\",\n",
    "#     'EMBED_DIR': \"/kaggle/input/cafa6-embeds\",\n",
    "#     'METADATA_PKL': \"/kaggle/working/hierarchy_metadata_v2.pkl\", # File V2 v·ª´a t·∫°o\n",
    "#     'VOCAB_FILE': \"/kaggle/input/c95-cafa6/vocab_C95_remove.csv\",\n",
    "\n",
    "#     # --- THAM S·ªê V4 (TUNED BY TOP KAGGLERS) ---\n",
    "#     'alpha': 0.15,       # Depth Boost\n",
    "#     'beta':  0.35,       # IC Boost (Quan tr·ªçng nh·∫•t)\n",
    "#     'gamma': 0.05,       # Adaptive Score Boost\n",
    "    \n",
    "#     'threshold': 0.01,   # Ng∆∞·ª°ng th·∫•p ƒë·ªÉ ƒë√≥n nh√£n s√¢u\n",
    "#     'top_k_leaf': 120,    \n",
    "#     'final_cap': 200,   \n",
    "    \n",
    "#     # --- Root Terms ---\n",
    "#     'ROOT_TERMS': [\"GO:0003674\", \"GO:0005575\", \"GO:0008150\"]\n",
    "# }\n",
    "\n",
    "# print(f\"üöÄ Config V4: Alpha={CONFIG['alpha']} | Beta={CONFIG['beta']} | Gamma={CONFIG['gamma']}\")\n",
    "\n",
    "# # ============================================================================\n",
    "# # 2. CLASS MODEL & DATASET\n",
    "# # ============================================================================\n",
    "# class WideProteinMLP(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes, hidden_dims=[2048, 4096], dropout=0.3):\n",
    "#         super().__init__()\n",
    "#         layers = [nn.LayerNorm(input_dim)]\n",
    "#         prev = input_dim\n",
    "#         for h in hidden_dims:\n",
    "#             layers += [nn.Linear(prev, h), nn.GELU(), nn.Dropout(dropout)]\n",
    "#             prev = h\n",
    "#         layers.append(nn.Linear(prev, num_classes))\n",
    "#         self.net = nn.Sequential(*layers)\n",
    "#     def forward(self, x): return self.net(x)\n",
    "\n",
    "# class TestDataset(Dataset):\n",
    "#     def __init__(self, embed_dir):\n",
    "#         with open(os.path.join(embed_dir, \"test_ids.txt\")) as f:\n",
    "#             self.ids = [line.strip() for line in f]\n",
    "#         self.embed_matrix = np.load(os.path.join(embed_dir, \"test_embeds.npy\"), mmap_mode=\"r\")\n",
    "#     def __len__(self): return len(self.ids)\n",
    "#     def __getitem__(self, idx):\n",
    "#         return torch.from_numpy(self.embed_matrix[idx].copy()).float(), self.ids[idx]\n",
    "\n",
    "# # ============================================================================\n",
    "# # 3. LOGIC X·ª¨ L√ù V4 (CORE)\n",
    "# # ============================================================================\n",
    "\n",
    "# def process_single_protein_v4_fixed(scores, depth_norm, ic_norm, parent_map, vocab_terms):\n",
    "#     \"\"\"\n",
    "#     Fixed & Robust Version:\n",
    "#     1. Select Candidates t·ª´ Base Scores (Tr√°nh boost r√°c).\n",
    "#     2. Robust Index Mapping (Tr√°nh l·ªói sai ƒëi·ªÉm).\n",
    "#     3. Safe Boosting (Clip 1.5 & Protect High Scores).\n",
    "#     4. Standard Max Propagation.\n",
    "#     5. Root Fallback.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # 1. Select Candidates t·ª´ Base Scores\n",
    "#     # Ch·ªâ xem x√©t Top-K l√° c√≥ ƒëi·ªÉm th√¥ cao nh·∫•t ƒë·ªÉ ti·∫øt ki·ªám t√≠nh to√°n v√† l·ªçc nhi·ªÖu\n",
    "#     n_terms = len(scores)\n",
    "#     if CONFIG['top_k_leaf'] >= n_terms:\n",
    "#         cand_idx = np.arange(n_terms)\n",
    "#     else:\n",
    "#         # L·∫•y top K ƒëi·ªÉm cao nh·∫•t (Raw)\n",
    "#         # argsort tr·∫£ v·ªÅ index tƒÉng d·∫ßn -> l·∫•y ƒëu√¥i\n",
    "#         cand_idx = np.argsort(scores)[-CONFIG['top_k_leaf']:]\n",
    "        \n",
    "#     # 2. Compute Boost (Ch·ªâ tr√™n t·∫≠p Candidate)\n",
    "#     cand_scores = scores[cand_idx]\n",
    "#     cand_depth = depth_norm[cand_idx]\n",
    "#     cand_ic = ic_norm[cand_idx]\n",
    "    \n",
    "#     # C√¥ng th·ª©c V4\n",
    "#     bf = 1.0 + CONFIG['alpha'] * cand_depth + CONFIG['beta'] * cand_ic + CONFIG['gamma'] * (cand_depth * cand_scores)\n",
    "    \n",
    "#     # Safety Clip: Boost t·ªëi ƒëa 1.5 l·∫ßn (V7 Gentle)\n",
    "#     MAX_BOOST = 1.5\n",
    "#     bf = np.minimum(bf, MAX_BOOST)\n",
    "    \n",
    "#     # High score protection: Score g·ªëc > 0.8 gi·ªØ nguy√™n\n",
    "#     high_mask = cand_scores > 0.8\n",
    "#     bf[high_mask] = 1.0\n",
    "    \n",
    "#     boosted_cand = cand_scores * bf\n",
    "#     boosted_cand = np.clip(boosted_cand, 0.0, 1.0)\n",
    "    \n",
    "#     # 3. Filter Threshold\n",
    "#     # Ch·ªâ gi·ªØ l·∫°i nh·ªØng nh√£n ƒë√£ boost v∆∞·ª£t qua ng∆∞·ª°ng\n",
    "#     keep_mask = boosted_cand >= CONFIG['threshold']\n",
    "    \n",
    "#     # [ROBUST MAPPING] T·∫°o map t·ª´ Global Index -> Boosted Score\n",
    "#     # ƒê·ªÉ tra c·ª©u nhanh v√† ch√≠nh x√°c\n",
    "#     # cand_idx: Global Indices c·ªßa c√°c candidate\n",
    "#     # boosted_cand: Scores t∆∞∆°ng ·ª©ng\n",
    "#     score_lookup = dict(zip(cand_idx, boosted_cand))\n",
    "    \n",
    "#     # L·∫•y danh s√°ch Global Index v∆∞·ª£t qua ng∆∞·ª°ng\n",
    "#     kept_global_idx = cand_idx[keep_mask]\n",
    "    \n",
    "#     # Fallback: N·∫øu l·ªçc xong kh√¥ng c√≤n g√¨, l·∫•y Top 1 t·ªët nh·∫•t (ƒë·ªÉ kh√¥ng r·ªóng)\n",
    "#     if len(kept_global_idx) == 0:\n",
    "#         top1_local = np.argsort(boosted_cand)[-1]\n",
    "#         kept_global_idx = np.array([cand_idx[top1_local]])\n",
    "    \n",
    "#     # 4. Hierarchy Repair (Max Propagation)\n",
    "#     final_scores = {}\n",
    "    \n",
    "#     # Kh·ªüi t·∫°o ƒëi·ªÉm cho c√°c node l√° ƒë∆∞·ª£c ch·ªçn\n",
    "#     for idx in kept_global_idx:\n",
    "#         final_scores[idx] = score_lookup[idx]\n",
    "        \n",
    "#     queue = list(final_scores.keys())\n",
    "#     processed = set(queue)\n",
    "#     idx_ptr = 0\n",
    "    \n",
    "#     while idx_ptr < len(queue):\n",
    "#         cur_idx = queue[idx_ptr]; idx_ptr += 1\n",
    "#         cur_score = final_scores[cur_idx]\n",
    "        \n",
    "#         parents = parent_map.get(cur_idx, [])\n",
    "#         for p_idx in parents:\n",
    "#             # L·∫•y ƒëi·ªÉm g·ªëc c·ªßa cha (n·∫øu c√≥ trong candidate th√¨ l·∫•y boosted, ko th√¨ l·∫•y raw)\n",
    "#             # Logic: N·∫øu cha n·∫±m trong candidate list th√¨ d√πng ƒëi·ªÉm ƒë√£ boost, \n",
    "#             # n·∫øu kh√¥ng (cha ƒëi·ªÉm th·∫•p qu√° ko l·ªçt top) th√¨ d√πng ƒëi·ªÉm raw g·ªëc.\n",
    "#             if p_idx in score_lookup:\n",
    "#                 p_base = score_lookup[p_idx]\n",
    "#             else:\n",
    "#                 p_base = float(scores[p_idx]) if p_idx < n_terms else 0.0\n",
    "                \n",
    "#             p_prev = final_scores.get(p_idx, p_base)\n",
    "            \n",
    "#             # QUY T·∫ÆC MAX CHU·∫®N: Cha = Max(Cha c≈©, Con)\n",
    "#             new_score = max(p_prev, cur_score)\n",
    "            \n",
    "#             # Ch·ªâ lan truy·ªÅn n·∫øu ƒëi·ªÉm tƒÉng ƒë√°ng k·ªÉ (> 1e-6)\n",
    "#             if new_score > p_prev + 1e-6:\n",
    "#                 final_scores[p_idx] = new_score\n",
    "#                 if p_idx not in processed:\n",
    "#                     queue.append(p_idx)\n",
    "#                     processed.add(p_idx)\n",
    "#             # ƒê·∫£m b·∫£o cha lu√¥n c√≥ m·∫∑t (Consistency)\n",
    "#             elif p_idx not in final_scores:\n",
    "#                 final_scores[p_idx] = p_prev\n",
    "#                 if p_idx not in processed:\n",
    "#                     queue.append(p_idx)\n",
    "#                     processed.add(p_idx)\n",
    "\n",
    "#     # 5. Final Cap & Format\n",
    "#     # Sort gi·∫£m d·∫ßn\n",
    "#     sorted_items = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "#     # C·∫Øt Top N (Final Cap)\n",
    "#     if len(sorted_items) > CONFIG['final_cap']:\n",
    "#         sorted_items = sorted_items[:CONFIG['final_cap']]\n",
    "    \n",
    "#     results = []\n",
    "#     for idx, score in sorted_items:\n",
    "#         t_id = vocab_terms[idx]\n",
    "#         results.append((t_id, float(score)))\n",
    "        \n",
    "#     # 6. Root Fallback (Ch·ªâ th√™m n·∫øu danh s√°ch r·ªóng)\n",
    "#     existing_terms = set([t for t,_ in results])\n",
    "#     for root in CONFIG['ROOT_TERMS']:\n",
    "#         if root not in existing_terms:\n",
    "#             results.append((root, 0.03))  # score th·∫•p an to√†n\n",
    "#             existing_terms.add(root)\n",
    "            \n",
    "#     return results, existing_terms\n",
    "\n",
    "# # ============================================================================\n",
    "# # 4. MAIN INFERENCE\n",
    "# # ============================================================================\n",
    "# def run_inference_v4():\n",
    "#     print(\">>> 1. Loading Metadata V2...\")\n",
    "#     vocab_df = pd.read_csv(CONFIG['VOCAB_FILE'])\n",
    "#     vocab_terms = np.array(vocab_df['term'].tolist())\n",
    "    \n",
    "#     # Load Metadata c√≥ IC\n",
    "#     with open(CONFIG['METADATA_PKL'], 'rb') as f:\n",
    "#         meta = pickle.load(f)\n",
    "#         depth_norm = meta['depth_norm']\n",
    "#         ic_norm = meta['ic_norm']          # <--- Key m·ªõi\n",
    "#         parent_map = meta['child_to_parent']\n",
    "        \n",
    "#     print(f\"   - Metadata loaded. IC shape: {ic_norm.shape}\")\n",
    "\n",
    "#     print(\">>> 2. Loading Model...\")\n",
    "#     model = WideProteinMLP(CONFIG['input_dim'], CONFIG['num_classes'], CONFIG['hidden_dims'], CONFIG['dropout'])\n",
    "    \n",
    "#     # Load weights\n",
    "#     ckpt = torch.load(CONFIG['MODEL_PATH'], map_location=\"cpu\")\n",
    "#     sd = ckpt['model_state'] if isinstance(ckpt, dict) and 'model_state' in ckpt else ckpt\n",
    "#     new_sd = OrderedDict()\n",
    "#     for k, v in sd.items():\n",
    "#         new_sd[k.replace(\"module.\", \"\")] = v\n",
    "#     model.load_state_dict(new_sd)\n",
    "\n",
    "#     # 2 GPU setup\n",
    "#     model.to(CONFIG['device'])\n",
    "#     if torch.cuda.device_count() > 1:\n",
    "#         print(f\"üî• Active GPUs: {torch.cuda.device_count()}\")\n",
    "#         model = nn.DataParallel(model)\n",
    "#     model.eval()\n",
    "\n",
    "#     ds = TestDataset(CONFIG['EMBED_DIR'])\n",
    "#     dl = DataLoader(ds, batch_size=CONFIG['batch_size'], shuffle=False, \n",
    "#                     num_workers=4, pin_memory=True)\n",
    "\n",
    "#     OUT_FILE = \"submission_v1.tsv\"\n",
    "#     f_out = open(OUT_FILE, \"w\")\n",
    "#     print(f\">>> 3. Streaming Inference to {OUT_FILE}...\")\n",
    "    \n",
    "#     count = 0\n",
    "#     with torch.no_grad():\n",
    "#         for features, prot_ids in tqdm(dl):\n",
    "#             features = features.to(CONFIG['device'])\n",
    "            \n",
    "#             with autocast():\n",
    "#                 logits = model(features)\n",
    "#                 probs = torch.sigmoid(logits)\n",
    "\n",
    "        \n",
    "#             probs_np = probs.float().cpu().numpy()\n",
    "            \n",
    "#             for i, pid in enumerate(prot_ids):\n",
    "#                 results, existing_terms = process_single_protein_v4_fixed(\n",
    "#                     probs_np[i], depth_norm, ic_norm, parent_map, vocab_terms\n",
    "#                 )\n",
    "\n",
    "#                 # Ensure root terms present (already handled in function, but double-check harmless)\n",
    "#                 for root in CONFIG[\"ROOT_TERMS\"]:\n",
    "#                     if root not in existing_terms:\n",
    "#                         results.append((root, 0.03))\n",
    "#                         existing_terms.add(root)\n",
    "\n",
    "#                 for term, score in results:\n",
    "#                     f_out.write(f\"{pid}\\t{term}\\t{score:.3f}\\n\")\n",
    "#                     count += 1\n",
    "                        \n",
    "#     f_out.close()\n",
    "    \n",
    "#     size_mb = os.path.getsize(OUT_FILE) / (1024**2)\n",
    "#     print(f\"\\n‚úÖ DONE! Submission V4 Created.\")\n",
    "#     print(f\"   - Total rows: {count:,}\")\n",
    "#     print(f\"   - File size: {size_mb:.2f} MB\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     run_inference_v4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7269b626",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T20:23:35.219411Z",
     "iopub.status.busy": "2025-12-06T20:23:35.219035Z",
     "iopub.status.idle": "2025-12-06T20:23:35.222625Z",
     "shell.execute_reply": "2025-12-06T20:23:35.222150Z"
    },
    "papermill": {
     "duration": 0.007585,
     "end_time": "2025-12-06T20:23:35.223603",
     "exception": false,
     "start_time": "2025-12-06T20:23:35.216018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# INPUT_FILE = \"submission_v1.tsv\"  # File k·∫øt qu·∫£ t·ª´ config Strong Boost\n",
    "# OUTPUT_FILE = \"submission.tsv\"\n",
    "\n",
    "# # C·∫§U H√åNH C·∫ÆT G·ªåT D·ª∞A TR√äN B·∫¢NG PH√ÇN T√çCH\n",
    "# CUT_THRESHOLD = 0.10  # B·∫Øt ƒë·∫ßu l·∫•y t·ª´ v√πng IA=1.48\n",
    "# CUT_LIMIT = 150       # Gi·ªØ Top 150 (Thay v√¨ 200) ƒë·ªÉ tƒÉng Precision\n",
    "\n",
    "# def optimize_submission():\n",
    "#     print(f\"üî• Optimizing {INPUT_FILE}...\")\n",
    "#     if os.path.exists(OUTPUT_FILE): os.remove(OUTPUT_FILE)\n",
    "    \n",
    "#     chunksize = 5_000_000\n",
    "#     total_kept = 0\n",
    "    \n",
    "#     with pd.read_csv(INPUT_FILE, sep='\\t', names=['P', 'T', 'S'], chunksize=chunksize) as reader:\n",
    "#         for chunk in tqdm(reader):\n",
    "#             # 1. L·ªçc ng∆∞·ª°ng 0.10\n",
    "#             filtered = chunk[chunk['S'] >= CUT_THRESHOLD]\n",
    "            \n",
    "#             # 2. Top 150\n",
    "#             filtered = filtered.sort_values(['P', 'S'], ascending=[True, False])\n",
    "#             filtered = filtered.groupby('P').head(CUT_LIMIT)\n",
    "            \n",
    "#             if len(filtered) > 0:\n",
    "#                 filtered.to_csv(OUTPUT_FILE, sep='\\t', header=False, index=False, mode='a')\n",
    "#                 total_kept += len(filtered)\n",
    "\n",
    "#     print(f\"‚úÖ DONE! New file: {OUTPUT_FILE}\")\n",
    "#     print(f\"   Rows: {total_kept:,}\")\n",
    "#     print(f\"   Density: {total_kept/224309:.1f} labels/protein\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     optimize_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae9877be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T20:23:35.229410Z",
     "iopub.status.busy": "2025-12-06T20:23:35.229222Z",
     "iopub.status.idle": "2025-12-06T20:23:35.232184Z",
     "shell.execute_reply": "2025-12-06T20:23:35.231680Z"
    },
    "papermill": {
     "duration": 0.007061,
     "end_time": "2025-12-06T20:23:35.233184",
     "exception": false,
     "start_time": "2025-12-06T20:23:35.226123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # --- N√äN ƒê·ªîI T√äN FILE CHO ƒê√öNG ---\n",
    "# # base_path   = \"/kaggle/input/sample-submission/0.275_submission.tsv\"\n",
    "# # boost350    = \"/kaggle/input/sample-submission/0.229_submission.tsv\"\n",
    "# # boost50     = \"/kaggle/input/sample-submission/0.233_submission.tsv\"\n",
    "# # low     = \"/kaggle/input/sample-submission/0.266_submission.tsv\"\n",
    "\n",
    "# submit = \"/kaggle/working/submission.tsv\"\n",
    "\n",
    "# def load_sub(path):\n",
    "#     return pd.read_csv(path, sep='\\t', names=['protein','term','score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7245e705",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T20:23:35.238988Z",
     "iopub.status.busy": "2025-12-06T20:23:35.238755Z",
     "iopub.status.idle": "2025-12-06T20:23:35.243230Z",
     "shell.execute_reply": "2025-12-06T20:23:35.242647Z"
    },
    "papermill": {
     "duration": 0.0087,
     "end_time": "2025-12-06T20:23:35.244308",
     "exception": false,
     "start_time": "2025-12-06T20:23:35.235608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # =========================================================\n",
    "# # C·∫§U H√åNH & THAM S·ªê\n",
    "# # =========================================================\n",
    "# SUB_FILE = \"submission.tsv\"\n",
    "# IA_FILE  = \"/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\"\n",
    "# CHUNK_SIZE = 5_000_000 \n",
    "# ROOT_TERMS = [\"GO:0003674\", \"GO:0005575\", \"GO:0008150\"]\n",
    "# SCORE_BINS = np.linspace(0.0, 1.0, 11) # T·∫°o 11 m·ªëc: 0.0, 0.1, 0.2, ..., 1.0\n",
    "\n",
    "# def analyze_ia_extremes_fixed():\n",
    "#     # 1. Load submission (Ph√°t hi·ªán c·ªôt)\n",
    "#     print(\"1. Reading submission to identify columns...\")\n",
    "#     df_head = pd.read_csv(SUB_FILE, sep=None, engine=\"python\", header=None, nrows=1000)\n",
    "\n",
    "#     go_col, score_col = None, None\n",
    "#     for col in df_head.columns:\n",
    "#         sample = df_head[col].astype(str)\n",
    "#         if sample.str.startswith(\"GO:\").any(): go_col = col\n",
    "#         if pd.to_numeric(sample, errors=\"coerce\").notna().mean() > 0.9: score_col = col\n",
    "\n",
    "#     if go_col is None or score_col is None:\n",
    "#         raise ValueError(\"Kh√¥ng t√¨m th·∫•y c·ªôt GO-term ho·∫∑c Score. Ki·ªÉm tra l·∫°i SUB_FILE.\")\n",
    "\n",
    "#     # 2. Load IA Map\n",
    "#     ia_df = pd.read_csv(IA_FILE, sep=\"\\t\", header=None, names=[\"go\", \"ia\"])\n",
    "#     ia_map = dict(zip(ia_df.go, ia_df.ia))\n",
    "#     print(f\"2. IA Map Loaded ({len(ia_map):,} terms).\")\n",
    "\n",
    "#     # 3. PH√ÇN T√çCH STREAMING\n",
    "#     ia_bin_stats = defaultdict(lambda: {'total_ia': 0.0, 'count': 0})\n",
    "\n",
    "#     print(\"\\n3. Starting Streaming Analysis (Binning Mean IA)...\")\n",
    "#     reader = pd.read_csv(SUB_FILE, sep=None, engine=\"python\", header=None, chunksize=CHUNK_SIZE)\n",
    "\n",
    "#     for chunk in tqdm(reader, desc=\"Processing Chunks\"):\n",
    "#         # ƒê·ªïi t√™n c·ªôt\n",
    "#         df_chunk = chunk.rename(columns={go_col: \"go\", score_col: \"score\"})\n",
    "        \n",
    "#         # L·ªçc 3 nh√£n g·ªëc\n",
    "#         df_chunk = df_chunk[~df_chunk['go'].isin(ROOT_TERMS)].copy()\n",
    "        \n",
    "#         # G√°n IA (Fill NaN = 0 cho an to√†n)\n",
    "#         df_chunk[\"ia\"] = df_chunk[\"go\"].map(ia_map).fillna(0.0)\n",
    "        \n",
    "#         # L·ªçc ƒëi·ªÉm s·ªë h·ª£p l·ªá\n",
    "#         df_chunk = df_chunk[(df_chunk['score'] >= 0) & (df_chunk['score'] <= 1.0)].copy()\n",
    "\n",
    "#         # T√≠nh to√°n BINS\n",
    "#         df_chunk['bin_index'] = np.digitize(df_chunk['score'], bins=SCORE_BINS) - 1 # ƒê·ªïi v·ªÅ index 0-9\n",
    "        \n",
    "#         # Nh√≥m theo index bin v√† t√≠nh t·ªïng IA v√† Count\n",
    "#         chunk_stats = df_chunk.groupby('bin_index').agg(\n",
    "#             total_ia=('ia', 'sum'),\n",
    "#             count=('ia', 'count')\n",
    "#         ).reset_index()\n",
    "\n",
    "#         # C·∫≠p nh·∫≠t global stats\n",
    "#         for _, row in chunk_stats.iterrows():\n",
    "#             # [FIX L·ªñI] √âp ki·ªÉu v·ªÅ int ti√™u chu·∫©n\n",
    "#             bin_idx = int(row['bin_index']) \n",
    "            \n",
    "#             # Ki·ªÉm tra l·ªói ngo√†i bi√™n (do np.digitize ƒë√¥i khi tr·∫£ v·ªÅ 10)\n",
    "#             if bin_idx >= len(SCORE_BINS) - 1 or bin_idx < 0:\n",
    "#                 continue \n",
    "            \n",
    "#             bin_min = SCORE_BINS[bin_idx]\n",
    "#             bin_max = SCORE_BINS[bin_idx + 1]\n",
    "            \n",
    "#             key = (bin_min, bin_max)\n",
    "#             ia_bin_stats[key]['total_ia'] += row['total_ia']\n",
    "#             ia_bin_stats[key]['count'] += row['count']\n",
    "\n",
    "\n",
    "#     # --- K·∫æT QU·∫¢ CU·ªêI C√ôNG ---\n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(f\"üìä PH√ÇN T√çCH MEAN IA THEO KHO·∫¢NG ƒêI·ªÇM S·ªê\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     total_all_counts = sum(s['count'] for s in ia_bin_stats.values())\n",
    "    \n",
    "#     print(f\"{'Score Range':<15} | {'Mean IA':<10} | {'Total Count':<15} | {'Density (%)':<10}\")\n",
    "#     print(\"-\" * 70)\n",
    "\n",
    "#     for (bin_min, bin_max), stats in sorted(ia_bin_stats.items()):\n",
    "#         count = stats['count']\n",
    "#         mean_ia = stats['total_ia'] / count if count > 0 else 0.0\n",
    "#         density = (count / total_all_counts) * 100 if total_all_counts > 0 else 0.0\n",
    "        \n",
    "#         range_str = f\"[{bin_min:.2f} - {bin_max:.2f})\"\n",
    "        \n",
    "#         print(f\"{range_str:<15} | {mean_ia:<10.4f} | {count:<15,} | {density:<9.1f}%\")\n",
    "\n",
    "#     print(\"=\"*70)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     analyze_ia_extremes_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71c85334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T20:23:35.250012Z",
     "iopub.status.busy": "2025-12-06T20:23:35.249804Z",
     "iopub.status.idle": "2025-12-06T20:23:35.253334Z",
     "shell.execute_reply": "2025-12-06T20:23:35.252796Z"
    },
    "papermill": {
     "duration": 0.007692,
     "end_time": "2025-12-06T20:23:35.254336",
     "exception": false,
     "start_time": "2025-12-06T20:23:35.246644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # File b·∫°n v·ª´a t·∫°o\n",
    "# SUBMISSION_FILE = \"submission.tsv\"\n",
    "\n",
    "# def analyze_threshold_impact():\n",
    "#     print(f\"üìÇ ƒêang ƒë·ªçc file {SUBMISSION_FILE}...\")\n",
    "#     # ƒê·ªçc file (c·ªôt: Protein, Term, Score)\n",
    "#     df = pd.read_csv(SUBMISSION_FILE, sep='\\t', names=['Protein', 'Term', 'Score'])\n",
    "    \n",
    "#     total_rows = len(df)\n",
    "#     total_proteins = df['Protein'].nunique()\n",
    "    \n",
    "#     print(f\"‚úÖ ƒê√£ t·∫£i xong!\")\n",
    "#     print(f\"   - T·ªïng d√≤ng hi·ªán t·∫°i: {total_rows:,}\")\n",
    "#     print(f\"   - T·ªïng Protein: {total_proteins:,}\")\n",
    "#     print(f\"   - Trung b√¨nh nh√£n/Protein (Hi·ªán t·∫°i): {total_rows / total_proteins:.1f}\")\n",
    "    \n",
    "#     # --- TH·ªêNG K√ä NG∆Ø·ª†NG GI·∫¢ L·∫¨P ---\n",
    "#     print(\"\\nüìä N·∫æU TƒÇNG THRESHOLD TH√å SAO?\")\n",
    "#     print(\"-\" * 50)\n",
    "#     print(f\"{'Threshold':<10} | {'S·ªë d√≤ng c√≤n l·∫°i':<15} | {'Nh√£n/Protein':<15} | {'% Gi·ªØ l·∫°i':<10}\")\n",
    "#     print(\"-\" * 50)\n",
    "    \n",
    "#     # C√°c m·ªëc threshold mu·ªën th·ª≠\n",
    "#     thresholds = [0.01, 0.05, 0.08, 0.10, 0.12, 0.15, 0.20]\n",
    "    \n",
    "#     for thr in thresholds:\n",
    "#         # L·ªçc gi·∫£ l·∫≠p\n",
    "#         filtered_count = (df['Score'] >= thr).sum()\n",
    "#         avg_labels = filtered_count / total_proteins\n",
    "#         percent_kept = (filtered_count / total_rows) * 100\n",
    "        \n",
    "#         print(f\"{thr:<10.2f} | {filtered_count:<15,} | {avg_labels:<15.1f} | {percent_kept:<9.1f}%\")\n",
    "        \n",
    "#     print(\"-\" * 50)\n",
    "    \n",
    "#     # --- V·∫º BI·ªÇU ƒê·ªí PH√ÇN B·ªê SCORE ---\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     sns.histplot(df['Score'], bins=100, color='teal', kde=False)\n",
    "#     plt.axvline(0.08, color='red', linestyle='--', label='Current Threshold (0.05)')\n",
    "#     plt.title(f'Ph√¢n b·ªë ƒëi·ªÉm s·ªë trong file {SUBMISSION_FILE}')\n",
    "#     plt.xlabel('Score')\n",
    "#     plt.ylabel('S·ªë l∆∞·ª£ng')\n",
    "#     plt.legend()\n",
    "#     plt.yscale('log') # D√πng log scale ƒë·ªÉ nh√¨n r√µ v√πng th·∫•p\n",
    "#     plt.show()\n",
    "\n",
    "# analyze_threshold_impact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d4d20f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T20:23:35.260142Z",
     "iopub.status.busy": "2025-12-06T20:23:35.259837Z",
     "iopub.status.idle": "2025-12-06T20:25:11.236687Z",
     "shell.execute_reply": "2025-12-06T20:25:11.235988Z"
    },
    "papermill": {
     "duration": 95.981168,
     "end_time": "2025-12-06T20:25:11.237769",
     "exception": false,
     "start_time": "2025-12-06T20:23:35.256601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ C·∫•u h√¨nh Inference: Device=cuda | Threshold=0.2\n",
      "Loading vocab from /kaggle/input/c95-cafa6/vocab_C95_remove.csv...\n",
      "Loading model from /kaggle/input/model-cafa6/best_model_wide_0.63.pth...\n",
      "üî• K√≠ch ho·∫°t 2 GPUs!\n",
      "Streaming predictions to submission.tsv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3505/3505 [01:28<00:00, 39.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DONE! Created submission.tsv with 32,761,135 rows.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from torch import amp\n",
    "\n",
    "class WideProteinMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden_dims=[2048, 4096], dropout=0.3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        # Input Norm: Gi√∫p ·ªïn ƒë·ªãnh ƒë·∫ßu v√†o t·ª´ Embeddings\n",
    "        layers.append(nn.LayerNorm(input_dim)) \n",
    "        \n",
    "        prev = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.GELU())   # Activation hi·ªán ƒë·∫°i\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "            \n",
    "        layers.append(nn.Linear(prev, num_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. C·∫§U H√åNH INFERENCE\n",
    "# ============================================================================\n",
    "CONFIG = {\n",
    "    # --- Model Architecture (Ph·∫£i kh·ªõp v·ªõi l√∫c Train) ---\n",
    "    'input_dim': 1280,             # ESM2-t33 embeddings\n",
    "    'hidden_dims': [2048, 4096],   # Wide MLP layers\n",
    "    'dropout': 0.3,                # Dropout \n",
    "    'num_classes': 6413,           # S·ªë l∆∞·ª£ng nh√£n c·ªßa b·ªô C95_remove \n",
    "    \n",
    "    # --- System ---\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'batch_size': 64,            \n",
    "    'num_workers': 2,\n",
    "    \n",
    "    # --- Paths ---\n",
    "    'MODEL_PATH': \"/kaggle/input/model-cafa6/best_model_wide_0.63.pth\",  # ƒê∆∞·ªùng d·∫´n file checkpoint ƒë√£ l∆∞u\n",
    "    'EMBED_DIR': \"/kaggle/input/cafa6-embeds\", \n",
    "    'VOCAB_FILE': \"/kaggle/input/c95-cafa6/vocab_C95_remove.csv\", # File vocab kh·ªõp v·ªõi model\n",
    "    \n",
    "    # --- Post-processing ---\n",
    "    'min_score_threshold': 0.2,   # Ch·ªâ l·∫•y c√°c d·ª± ƒëo√°n c√≥ x√°c su·∫•t >= 1%\n",
    "    'submission_limit': 150       # Gi·ªõi h·∫°n t·ªëi ƒëa 1500 nh√£n/protein\n",
    "}\n",
    "\n",
    "print(f\"üöÄ C·∫•u h√¨nh Inference: Device={CONFIG['device']} | Threshold={CONFIG['min_score_threshold']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ƒê·ªäNH NGHƒ®A DATASET CHO T·∫¨P TEST\n",
    "# ============================================================================\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, embed_dir):\n",
    "        with open(os.path.join(embed_dir, \"test_ids.txt\")) as f:\n",
    "            self.ids = [line.strip() for line in f]\n",
    "        # mmap l√† chu·∫©n ƒë·ªÉ ti·∫øt ki·ªám RAM h·ªá th·ªëng\n",
    "        self.embed_matrix = np.load(os.path.join(embed_dir, \"test_embeds.npy\"), mmap_mode=\"r\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # N√äN d√πng .copy() ·ªü ƒë√¢y. \n",
    "        # L√Ω do: Batch size 64 ch·ªâ t·ªën v√†i KB RAM, nh∆∞ng gi√∫p GPU kh√¥ng ph·∫£i ch·ªù ·ªï c·ª©ng ƒë·ªçc d·ªØ li·ªáu.\n",
    "        feat = torch.from_numpy(self.embed_matrix[idx].copy()).float()\n",
    "        return feat, self.ids[idx]\n",
    "\n",
    "# ============================================================================\n",
    "# 3. H√ÄM CH·∫†Y D·ª∞ ƒêO√ÅN (INFERENCE LOOP)\n",
    "# ============================================================================\n",
    "def run_inference_gpu_stream():\n",
    "    # 1. Setup\n",
    "    print(f\"Loading vocab from {CONFIG['VOCAB_FILE']}...\")\n",
    "    vocab_terms = pd.read_csv(CONFIG['VOCAB_FILE'])['term'].values\n",
    "    \n",
    "    # TƒÉng num_workers l√™n 4 n·∫øu d√πng 2 GPU ƒë·ªÉ ƒë·∫©y d·ªØ li·ªáu nhanh h∆°n\n",
    "    ds = TestDataset(CONFIG['EMBED_DIR'])\n",
    "    dl = DataLoader(ds, batch_size=CONFIG['batch_size'], shuffle=False, \n",
    "                    num_workers=4, pin_memory=True) # num_workers=4\n",
    "\n",
    "    model = WideProteinMLP(CONFIG['input_dim'], CONFIG['num_classes'], \n",
    "                           CONFIG['hidden_dims'], CONFIG['dropout'])\n",
    "    \n",
    "    # 2. Load Weights (Clean 'module.' prefix)\n",
    "    print(f\"Loading model from {CONFIG['MODEL_PATH']}...\")\n",
    "    ckpt = torch.load(CONFIG['MODEL_PATH'], map_location=\"cpu\")\n",
    "    sd = ckpt['model_state'] if isinstance(ckpt, dict) and 'model_state' in ckpt else ckpt\n",
    "    clean_sd = {k.replace(\"module.\", \"\"): v for k, v in sd.items()}\n",
    "    model.load_state_dict(clean_sd)\n",
    "    \n",
    "    # 3. K√çCH HO·∫†T 2 GPU (QUAN TR·ªåNG)\n",
    "    model.to(CONFIG['device'])\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"üî• K√≠ch ho·∫°t {torch.cuda.device_count()} GPUs!\")\n",
    "        model = nn.DataParallel(model) # <--- D√≤ng n√†y gi√∫p ch·∫°y tr√™n c·∫£ 2 GPU\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # 4. Inference Loop\n",
    "    out_file = \"submission.tsv\"\n",
    "    print(f\"Streaming predictions to {out_file}...\")\n",
    "    f = open(out_file, \"w\")\n",
    "    \n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for features, prot_ids in tqdm(dl):\n",
    "            features = features.to(CONFIG['device'])\n",
    "            \n",
    "            # Autocast: Gi√∫p gi·∫£m VRAM, tƒÉng t·ªëc ƒë·ªô tr√™n T4\n",
    "            with amp.autocast('cuda'):\n",
    "                logits = model(features)\n",
    "                probs = torch.sigmoid(logits)\n",
    "            \n",
    "            # Chuy·ªÉn v·ªÅ CPU ngay l·∫≠p t·ª©c ƒë·ªÉ gi·∫£i ph√≥ng GPU\n",
    "            probs = probs.float().cpu().numpy()\n",
    "            \n",
    "            # X·ª≠ l√Ω k·∫øt qu·∫£ (CPU l√†m vi·ªác n√†y trong khi GPU t√≠nh batch ti·∫øp theo)\n",
    "            for i, pid in enumerate(prot_ids):\n",
    "                p = probs[i]\n",
    "                \n",
    "                # L·ªçc nhanh b·∫±ng numpy\n",
    "                idxs = np.where(p >= CONFIG['min_score_threshold'])[0]\n",
    "                if len(idxs) == 0: continue\n",
    "                \n",
    "                scores = p[idxs]\n",
    "                \n",
    "                # C·∫Øt Top K\n",
    "                if len(idxs) > CONFIG['submission_limit']:\n",
    "                    # partition nhanh h∆°n argsort to√†n b·ªô\n",
    "                    top_k_indices = np.argpartition(scores, -CONFIG['submission_limit'])[-CONFIG['submission_limit']:]\n",
    "                    idxs = idxs[top_k_indices]\n",
    "                    scores = scores[top_k_indices]\n",
    "                \n",
    "                # Ghi file\n",
    "                batch_terms = vocab_terms[idxs]\n",
    "                for term, sc in zip(batch_terms, scores):\n",
    "                    f.write(f\"{pid}\\t{term}\\t{sc:.3f}\\n\")\n",
    "                    count += 1\n",
    "            \n",
    "            # D·ªçn d·∫πp th·ªß c√¥ng (Optional, Python t·ª± lo ƒë∆∞·ª£c nh∆∞ng th√™m cho ch·∫Øc)\n",
    "            del features, logits, probs\n",
    "            \n",
    "    f.close()\n",
    "    print(f\"‚úÖ DONE! Created submission.tsv with {count:,} rows.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_inference_gpu_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec47e4e",
   "metadata": {
    "papermill": {
     "duration": 0.031251,
     "end_time": "2025-12-06T20:25:11.301117",
     "exception": false,
     "start_time": "2025-12-06T20:25:11.269866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14084779,
     "sourceId": 116062,
     "sourceType": "competition"
    },
    {
     "datasetId": 8916743,
     "sourceId": 13989811,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8919436,
     "sourceId": 14031626,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8917141,
     "sourceId": 13990998,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 100.650328,
   "end_time": "2025-12-06T20:25:12.553402",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-06T20:23:31.903074",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
