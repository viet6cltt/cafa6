{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98653891",
   "metadata": {
    "id": "98653891",
    "outputId": "992f8fd6-03f0-41af-9b58-17c13359f680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ete3\n",
      "  Downloading ete3-3.1.3.tar.gz (4.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: ete3\n",
      "\u001b[33m  DEPRECATION: Building 'ete3' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'ete3'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for ete3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ete3: filename=ete3-3.1.3-py3-none-any.whl size=2273900 sha256=ae0595b80237e85b8ae6dd0e774b8255d65cde2648ba397436dd6c1f33bb4ec1\n",
      "  Stored in directory: /root/.cache/pip/wheels/4f/18/8d/3800b8b1dc7a8c1954eaa48424f639b2cfc760922cc3cee479\n",
      "Successfully built ete3\n",
      "Installing collected packages: ete3\n",
      "Successfully installed ete3-3.1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ete3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba3b796",
   "metadata": {
    "id": "9ba3b796",
    "outputId": "170ce5e1-e5d2-432d-f88c-4f304f8c383d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PH√ÇN T√çCH TAXONOMY (HIERARCHICAL ROLL-UP VERSION)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local taxdump.tar.gz seems up-to-date\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading node names...\n",
      "2712337 names loaded.\n",
      "425670 synonyms loaded.\n",
      "Loading nodes...\n",
      "2712337 nodes loaded.\n",
      "Linking nodes...\n",
      "Tree is loaded.\n",
      "Updating database: /root/.etetoolkit/taxa.sqlite ...\n",
      " 2712000 generating entries... \n",
      "Uploading to /root/.etetoolkit/taxa.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting synonyms:      30000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting taxid merges:  45000  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting taxids:       30000  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting taxids:       2710000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ƒêang map 1381 taxon ID v·ªÅ c·∫•p Species...\n",
      " ƒê√£ quy ho·∫°ch t·ª´ 1381 ID g·ªëc xu·ªëng c√≤n 1297 ID (ch·ªß y·∫øu l√† Species).\n",
      "\n",
      "--- T·∫†O MAPPING ---\n",
      "K_FINAL (Mapped Species ‚â• 10) = 134\n",
      "Taxonomy coverage after roll-up: 96.86%\n",
      "UNK proteins: 2,587 / 82,404\n",
      "\n",
      "--- HI·ªÜU QU·∫¢ C·ª¶A ROLL-UP ---\n",
      "S·ªë l∆∞·ª£ng ID b·ªã lo·∫°i b·ªè (Dead Zone) TR∆Ø·ªöC khi g·ªôp: 1241\n",
      "S·ªë l∆∞·ª£ng ID b·ªã lo·∫°i b·ªè (Dead Zone) SAU khi g·ªôp   : 1163\n",
      "üëâ Ch√∫ng ta ƒë√£ c·ª©u ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ c√°c taxon hi·∫øm b·∫±ng c√°ch g·ªôp v·ªÅ cha!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/ete3/ncbi_taxonomy/ncbiquery.py:243: UserWarning: taxid 61267 was translated into 1538424\n",
      "  warnings.warn(\"taxid %s was translated into %s\" %(taxid, merged_conversion[taxid]))\n",
      "/venv/main/lib/python3.12/site-packages/ete3/ncbi_taxonomy/ncbiquery.py:243: UserWarning: taxid 34647 was translated into 1879292\n",
      "  warnings.warn(\"taxid %s was translated into %s\" %(taxid, merged_conversion[taxid]))\n",
      "/venv/main/lib/python3.12/site-packages/ete3/ncbi_taxonomy/ncbiquery.py:243: UserWarning: taxid 184540 was translated into 8732\n",
      "  warnings.warn(\"taxid %s was translated into %s\" %(taxid, merged_conversion[taxid]))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from ete3 import NCBITaxa\n",
    "import os\n",
    "\n",
    "# ================================\n",
    "# CONFIG\n",
    "# ================================\n",
    "CONFIG = {\n",
    "    \"TAXON_FILE\": \"/root/CAFA6data/Train/train_taxonomy.tsv\",\n",
    "    \"MIN_SAMPLES_REQUIRED\": 10,\n",
    "    \"OUTPUT_PKL\": \"taxon_mapping_K_Species.pkl\",\n",
    "    \"DB_PATH\": \"taxon.sqlite\" # ete3 file\n",
    "}\n",
    "\n",
    "def get_species_level_mapping(unique_taxids):\n",
    "\n",
    "    ncbi = NCBITaxa()\n",
    "    if not os.path.exists(CONFIG[\"DB_PATH\"]):\n",
    "      ncbi.update_taxonomy_database()\n",
    "\n",
    "    mapping = {}\n",
    "    print(f\" ƒêang map {len(unique_taxids)} taxon ID v·ªÅ c·∫•p Species...\")\n",
    "\n",
    "    valid_ids = [int(tid) for tid in unique_taxids if str(tid).isdigit()]\n",
    "\n",
    "    try:\n",
    "        # L·∫•y rank c·ªßa t·∫•t c·∫£ ID\n",
    "        ranks = ncbi.get_rank(valid_ids)\n",
    "\n",
    "        for tid in valid_ids:\n",
    "            current_id = tid\n",
    "\n",
    "            # N·∫øu b·∫£n th√¢n n√≥ l√† Species r·ªìi th√¨ gi·ªØ nguy√™n\n",
    "            if ranks.get(current_id) == 'species':\n",
    "                mapping[str(tid)] = str(current_id)\n",
    "                continue\n",
    "\n",
    "            # N·∫øu kh√¥ng ph·∫£i Species, leo c√¢y t√¨m cha\n",
    "            try:\n",
    "                lineage = ncbi.get_lineage(tid) # Tr·∫£ v·ªÅ list [root, ..., genus, species, strain]\n",
    "                # L·∫•y rank c·ªßa c·∫£ d√≤ng h·ªç\n",
    "                lineage_ranks = ncbi.get_rank(lineage)\n",
    "\n",
    "                # T√¨m ID n√†o trong d√≤ng h·ªç c√≥ rank l√† 'species'\n",
    "                species_id = None\n",
    "                for anc_id in reversed(lineage):   # g·∫ßn nh·∫•t tr∆∞·ªõc\n",
    "                  if lineage_ranks.get(anc_id) == \"species\":\n",
    "                      species_id = anc_id\n",
    "                      break\n",
    "\n",
    "                if species_id:\n",
    "                    mapping[str(tid)] = str(species_id)\n",
    "                else:\n",
    "                    # Tr∆∞·ªùng h·ª£p kh√¥ng t√¨m th·∫•y species (v√≠ d·ª• c·∫•p Genus), gi·ªØ nguy√™n ID g·ªëc\n",
    "                    mapping[str(tid)] = str(tid)\n",
    "\n",
    "            except ValueError:\n",
    "                # ID kh√¥ng c√≥ trong database NCBI\n",
    "                mapping[str(tid)] = str(tid)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è L·ªói NCBI Taxa: {e}\")\n",
    "        # Fallback: gi·ªØ nguy√™n\n",
    "        for tid in unique_taxids:\n",
    "            mapping[str(tid)] = str(tid)\n",
    "\n",
    "    return mapping\n",
    "\n",
    "def analyze_for_report_final():\n",
    "    print(\" PH√ÇN T√çCH TAXONOMY (HIERARCHICAL ROLL-UP VERSION)\\n\")\n",
    "\n",
    "    # 1. LOAD DATA\n",
    "    df = pd.read_csv(CONFIG[\"TAXON_FILE\"], sep=\"\\t\", header=None, names=[\"Protein\", \"Taxon\"], dtype=str)\n",
    "\n",
    "    # --- B∆Ø·ªöC M·ªöI: ROLL-UP ---\n",
    "    unique_taxa = df[\"Taxon\"].unique()\n",
    "\n",
    "    # T·∫°o map t·ª´ ID g·ªëc -> ID lo√†i\n",
    "    # (B·∫°n c·∫ßn c√†i ete3 v√† c√≥ internet l·∫ßn ƒë·∫ßu ƒë·ªÉ t·∫£i DB)\n",
    "    taxon_map = get_species_level_mapping(unique_taxa)\n",
    "\n",
    "    # √Åp d·ª•ng map v√†o dataframe\n",
    "    df[\"Taxon_Original\"] = df[\"Taxon\"]\n",
    "    df[\"Taxon\"] = df[\"Taxon\"].map(taxon_map).fillna(df[\"Taxon\"]) # Fillna ph√≤ng h·ªù\n",
    "\n",
    "    print(f\" ƒê√£ quy ho·∫°ch t·ª´ {len(unique_taxa)} ID g·ªëc xu·ªëng c√≤n {df['Taxon'].nunique()} ID (ch·ªß y·∫øu l√† Species).\")\n",
    "\n",
    "    # --- T·ª™ ƒê√ÇY TR·ªû ƒêI L√Ä LOGIC C≈® NH∆ØNG TR√äN D·ªÆ LI·ªÜU ƒê√É G·ªòP ---\n",
    "\n",
    "    min_freq = CONFIG[\"MIN_SAMPLES_REQUIRED\"]\n",
    "\n",
    "    tax_counts = df[\"Taxon\"].value_counts()\n",
    "    new_dead_zone = (tax_counts < min_freq).sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # L·∫•y top taxon\n",
    "    top_taxa_series = tax_counts[tax_counts >= min_freq]\n",
    "    K_FINAL = len(top_taxa_series)\n",
    "    UNK_IDX = K_FINAL\n",
    "\n",
    "    print(\"\\n--- T·∫†O MAPPING ---\")\n",
    "    print(f\"K_FINAL (Mapped Species ‚â• {min_freq}) = {K_FINAL}\")\n",
    "\n",
    "    # Map: Species ID -> index model\n",
    "    # L∆∞u √Ω: Taxon ·ªü ƒë√¢y l√† Species ID\n",
    "    species_to_idx = {tid: i for i, tid in enumerate(top_taxa_series.index)}\n",
    "\n",
    "    # Pre-calculate full chain mapping for speed\n",
    "    full_chain_map = {} # Original ID -> Model Index\n",
    "    for orig_id in unique_taxa:\n",
    "        species_id = taxon_map.get(orig_id, orig_id)\n",
    "        if species_id in species_to_idx:\n",
    "            full_chain_map[orig_id] = species_to_idx[species_id]\n",
    "        else:\n",
    "            full_chain_map[orig_id] = UNK_IDX\n",
    "\n",
    "    # Apply\n",
    "    mapped_series = (\n",
    "        df[\"Taxon_Original\"]\n",
    "        .map(full_chain_map)\n",
    "        .fillna(UNK_IDX)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    # T·∫°o Series s·∫°ch s·∫Ω (x·ª≠ l√Ω NaN v√† √©p ki·ªÉu Int)\n",
    "    mapped_series = df[\"Taxon_Original\"].map(full_chain_map).fillna(UNK_IDX).astype(int)\n",
    "\n",
    "    # Zip c·ªôt Protein v·ªõi Series s·∫°ch ƒë√≥\n",
    "    prot_to_taxon_idx = dict(zip(df[\"Protein\"], mapped_series))\n",
    "\n",
    "    # ===== COVERAGE STATS =====\n",
    "    unk_count = sum(v == UNK_IDX for v in prot_to_taxon_idx.values())\n",
    "    coverage = 100 * (1 - unk_count / len(prot_to_taxon_idx))\n",
    "\n",
    "    print(f\"Taxonomy coverage after roll-up: {coverage:.2f}%\")\n",
    "    print(f\"UNK proteins: {unk_count:,} / {len(prot_to_taxon_idx):,}\")\n",
    "\n",
    "    # L∆∞u file\n",
    "    data_to_save = {\n",
    "        \"taxon_to_idx\": species_to_idx,\n",
    "        \"prot_to_taxon_idx\": prot_to_taxon_idx,\n",
    "        \"original_id_to_species_id\": taxon_map,\n",
    "        \"num_taxa_classes\": K_FINAL + 1,\n",
    "        \"min_freq_cutoff\": min_freq\n",
    "    }\n",
    "\n",
    "    with open(CONFIG[\"OUTPUT_PKL\"], \"wb\") as f:\n",
    "        pickle.dump(data_to_save, f)\n",
    "\n",
    "    # In th·ªëng k√™ so s√°nh\n",
    "    orig_counts = df[\"Taxon_Original\"].value_counts()\n",
    "    original_dead_zone = (orig_counts < min_freq).sum()\n",
    "\n",
    "    print(\"\\n--- HI·ªÜU QU·∫¢ C·ª¶A ROLL-UP ---\")\n",
    "    print(f\"S·ªë l∆∞·ª£ng ID b·ªã lo·∫°i b·ªè (Dead Zone) TR∆Ø·ªöC khi g·ªôp: {original_dead_zone}\")\n",
    "    print(f\"S·ªë l∆∞·ª£ng ID b·ªã lo·∫°i b·ªè (Dead Zone) SAU khi g·ªôp   : {new_dead_zone}\")\n",
    "    print(f\"üëâ Ch√∫ng ta ƒë√£ c·ª©u ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ c√°c taxon hi·∫øm b·∫±ng c√°ch g·ªôp v·ªÅ cha!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_for_report_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b776c",
   "metadata": {
    "id": "3e1b776c",
    "outputId": "75e702b5-4a20-42ba-b194-ca18255a47ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ƒêang ƒë·ªçc file: taxon_mapping_K_Species.pkl...\n",
      "\n",
      "--- C·∫§U TR√öC D·ªÆ LI·ªÜU ---\n",
      "C√°c keys c√≥ trong dictionary: ['taxon_to_idx', 'prot_to_taxon_idx', 'original_id_to_species_id', 'num_taxa_classes', 'min_freq_cutoff']\n",
      "\n",
      "--- TH·ªêNG K√ä C∆† B·∫¢N ---\n",
      "üîπ T·ªïng s·ªë l·ªõp (Classes) cho Model : 135\n",
      "üîπ Index c·ªßa l·ªõp UNK (Unknown)     : 134\n",
      "üîπ Ng∆∞·ª°ng t·∫ßn su·∫•t t·ªëi thi·ªÉu       : 10\n",
      "\n",
      "--- SAMPLE: PROTEIN -> MODEL INDEX ---\n",
      "T·ªïng s·ªë Protein ƒë√£ map: 82,404\n",
      "  Protein Q07523          -> Index 4     (‚úÖ H·ªçc ƒë∆∞·ª£c)\n",
      "  Protein P39084          -> Index 134   (‚ö†Ô∏è R∆°i v√†o UNK)\n",
      "  Protein P77245          -> Index 6     (‚úÖ H·ªçc ƒë∆∞·ª£c)\n",
      "  Protein P19073          -> Index 3     (‚úÖ H·ªçc ƒë∆∞·ª£c)\n",
      "  Protein P84831          -> Index 78    (‚úÖ H·ªçc ƒë∆∞·ª£c)\n",
      "üëâ T·ªïng s·ªë protein b·ªã g√°n nh√£n UNK: 2,587 (3.14%)\n",
      "\n",
      "--- SAMPLE: SPECIES ID -> MODEL INDEX ---\n",
      "  Taxon ID 9606       -> Index 0\n",
      "  Taxon ID 10090      -> Index 1\n",
      "  Taxon ID 3702       -> Index 2\n",
      "  Taxon ID 4932       -> Index 3\n",
      "  Taxon ID 10116      -> Index 4\n",
      "\n",
      "--- SAMPLE: TRA C·ª®U (Original -> Species) ---\n",
      "  üîÅ ƒê√£ g·ªôp: ID g·ªëc 39947 -> Species cha 4530\n",
      "  üîÅ ƒê√£ g·ªôp: ID g·ªëc 224308 -> Species cha 1423\n",
      "  üîÅ ƒê√£ g·ªôp: ID g·ªëc 559292 -> Species cha 4932\n",
      "  üîÅ ƒê√£ g·ªôp: ID g·ªëc 83333 -> Species cha 562\n",
      "  üîÅ ƒê√£ g·ªôp: ID g·ªëc 208964 -> Species cha 287\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n file b·∫°n v·ª´a l∆∞u\n",
    "PKL_PATH = \"taxon_mapping_K_Species.pkl\"\n",
    "\n",
    "def inspect_pickle_file(path):\n",
    "    print(f\"üìÇ ƒêang ƒë·ªçc file: {path}...\\n\")\n",
    "\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # 1. Ki·ªÉm tra c√°c Key c√≥ trong file\n",
    "    print(\"--- C·∫§U TR√öC D·ªÆ LI·ªÜU ---\")\n",
    "    print(f\"C√°c keys c√≥ trong dictionary: {list(data.keys())}\")\n",
    "\n",
    "    # 2. L·∫•y th√¥ng tin th·ªëng k√™\n",
    "    num_classes = data['num_taxa_classes']\n",
    "    cutoff = data['min_freq_cutoff']\n",
    "\n",
    "    # Index c·ªßa l·ªõp UNK lu√¥n l√† class cu·ªëi c√πng (num_classes - 1)\n",
    "    unk_idx = num_classes - 1\n",
    "\n",
    "    print(f\"\\n--- TH·ªêNG K√ä C∆† B·∫¢N ---\")\n",
    "    print(f\"üîπ T·ªïng s·ªë l·ªõp (Classes) cho Model : {num_classes}\")\n",
    "    print(f\"üîπ Index c·ªßa l·ªõp UNK (Unknown)     : {unk_idx}\")\n",
    "    print(f\"üîπ Ng∆∞·ª°ng t·∫ßn su·∫•t t·ªëi thi·ªÉu       : {cutoff}\")\n",
    "\n",
    "    # 3. Soi d·ªØ li·ªáu mapping: Protein -> Index\n",
    "    prot_map = data['prot_to_taxon_idx']\n",
    "    print(f\"\\n--- SAMPLE: PROTEIN -> MODEL INDEX ---\")\n",
    "    print(f\"T·ªïng s·ªë Protein ƒë√£ map: {len(prot_map):,}\")\n",
    "\n",
    "    # L·∫•y ng·∫´u nhi√™n 5 protein ƒë·ªÉ xem\n",
    "    sample_prots = random.sample(list(prot_map.items()), 5)\n",
    "    for prot, idx in sample_prots:\n",
    "        status = \"‚úÖ H·ªçc ƒë∆∞·ª£c\" if idx != unk_idx else \"‚ö†Ô∏è R∆°i v√†o UNK\"\n",
    "        print(f\"  Protein {prot:<15} -> Index {idx:<5} ({status})\")\n",
    "\n",
    "    # ƒê·∫øm nhanh s·ªë l∆∞·ª£ng UNK trong map\n",
    "    unk_count = list(prot_map.values()).count(unk_idx)\n",
    "    print(f\"üëâ T·ªïng s·ªë protein b·ªã g√°n nh√£n UNK: {unk_count:,} ({unk_count/len(prot_map)*100:.2f}%)\")\n",
    "\n",
    "    # 4. Soi d·ªØ li·ªáu mapping: Species ID -> Index\n",
    "    tax_map = data['taxon_to_idx']\n",
    "    print(f\"\\n--- SAMPLE: SPECIES ID -> MODEL INDEX ---\")\n",
    "    # L·∫•y 5 item ƒë·∫ßu ti√™n\n",
    "    first_5_taxa = list(tax_map.items())[:5]\n",
    "    for tax_id, idx in first_5_taxa:\n",
    "        print(f\"  Taxon ID {tax_id:<10} -> Index {idx}\")\n",
    "\n",
    "    # 5. Soi d·ªØ li·ªáu truy v·∫øt: ID G·ªëc -> Species ID\n",
    "    trace_map = data['original_id_to_species_id']\n",
    "    print(f\"\\n--- SAMPLE: TRA C·ª®U (Original -> Species) ---\")\n",
    "    # T√¨m th·ª≠ v√†i tr∆∞·ªùng h·ª£p ID g·ªëc kh√°c ID species (ƒë√£ ƒë∆∞·ª£c roll-up)\n",
    "    found = 0\n",
    "    for orig, species in trace_map.items():\n",
    "        if orig != species:\n",
    "            print(f\"  üîÅ ƒê√£ g·ªôp: ID g·ªëc {orig} -> Species cha {species}\")\n",
    "            found += 1\n",
    "            if found >= 5: break\n",
    "\n",
    "    if found == 0:\n",
    "        print(\"  (Kh√¥ng t√¨m th·∫•y sample n√†o kh√°c bi·ªát trong 5 l·∫ßn th·ª≠ ƒë·∫ßu - c√≥ th·ªÉ data to√†n species chu·∫©n)\")\n",
    "\n",
    "# Ch·∫°y h√†m\n",
    "try:\n",
    "    inspect_pickle_file(PKL_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file .pkl! H√£y ch·∫Øc ch·∫Øn b·∫°n ƒë√£ ch·∫°y b∆∞·ªõc t·∫°o file tr∆∞·ªõc ƒë√≥.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6039c5e",
   "metadata": {
    "id": "c6039c5e",
    "outputId": "e0c397cd-e85b-480b-86ec-ce72a2e50f19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ƒêang ƒë·ªçc file C≈®: /root/CAFA6data/cafa6-embeds/taxon_mapping_K140.pkl...\n",
      "\n",
      "--- C·∫§U TR√öC D·ªÆ LI·ªÜU ---\n",
      "C√°c keys: ['taxon_to_idx', 'prot_to_taxon_idx', 'num_taxa_classes', 'min_freq_cutoff']\n",
      "\n",
      "--- TH·ªêNG K√ä ---\n",
      "üîπ S·ªë l·ªõp (Classes) : 141\n",
      "üîπ Index UNK        : 140\n",
      "üîπ Cutoff t·∫ßn su·∫•t  : 10\n",
      "\n",
      "--- HI·ªÜU SU·∫§T MAPPING (C≈®) ---\n",
      "T·ªïng Protein: 82,404\n",
      "‚õî R∆°i v√†o UNK: 2,719 (3.30%)\n",
      "‚úÖ Coverage   : 96.70%\n",
      "   (So v·ªõi b·∫£n m·ªõi: B·∫£n m·ªõi UNK ch·ªâ kho·∫£ng 3%)\n",
      "\n",
      "--- SOI TAXON ID (Top 10) ---\n",
      "  ID 9606       -> Index 0\n",
      "  ID 10090      -> Index 1\n",
      "  ID 3702       -> Index 2\n",
      "  ID 559292     -> Index 3\n",
      "  ID 10116      -> Index 4\n",
      "  ID 284812     -> Index 5\n",
      "  ID 83333      -> Index 6\n",
      "  ID 7227       -> Index 7\n",
      "  ID 6239       -> Index 8\n",
      "  ID 83332      -> Index 9\n",
      "  ID 7955       -> Index 10\n",
      "  ID 44689      -> Index 11\n",
      "  ID 39947      -> Index 12\n",
      "  ID 9913       -> Index 13\n",
      "  ID 9031       -> Index 14\n",
      "\n",
      "‚ö†Ô∏è File n√†y KH√îNG c√≥ mapping 'original_id_to_species_id'.\n",
      "üëâ Nghƒ©a l√†: Model ƒëang h·ªçc ID g·ªëc th√¥ (Raw ID). E.coli K12 v√† E.coli g·ªëc b·ªã coi l√† 2 lo√†i kh√°c nhau.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "# ƒê∆Ø·ªúNG D·∫™N FILE C≈® (S·ª≠a t√™n file n·∫øu c·∫ßn)\n",
    "OLD_PKL_PATH = \"/root/CAFA6data/cafa6-embeds/taxon_mapping_K140.pkl\"\n",
    "\n",
    "def inspect_old_pickle(path):\n",
    "    print(f\"üìÇ ƒêang ƒë·ªçc file C≈®: {path}...\\n\")\n",
    "\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file: {path}\")\n",
    "        return\n",
    "\n",
    "    # 1. Ki·ªÉm tra Keys\n",
    "    print(\"--- C·∫§U TR√öC D·ªÆ LI·ªÜU ---\")\n",
    "    print(f\"C√°c keys: {list(data.keys())}\")\n",
    "\n",
    "    # 2. Th·ªëng k√™ c∆° b·∫£n\n",
    "    # D√πng .get() ƒë·ªÉ tr√°nh l·ªói n·∫øu file c≈© c·∫•u tr√∫c kh√°c\n",
    "    num_classes = data.get('num_taxa_classes', 'N/A')\n",
    "    cutoff = data.get('min_freq_cutoff', 'N/A')\n",
    "\n",
    "    unk_idx = num_classes - 1 if isinstance(num_classes, int) else -1\n",
    "\n",
    "    print(f\"\\n--- TH·ªêNG K√ä ---\")\n",
    "    print(f\"üîπ S·ªë l·ªõp (Classes) : {num_classes}\")\n",
    "    print(f\"üîπ Index UNK        : {unk_idx}\")\n",
    "    print(f\"üîπ Cutoff t·∫ßn su·∫•t  : {cutoff}\")\n",
    "\n",
    "    # 3. Ph√¢n t√≠ch ƒë·ªô ph·ªß (Coverage)\n",
    "    if 'prot_to_taxon_idx' in data:\n",
    "        prot_map = data['prot_to_taxon_idx']\n",
    "        total_prots = len(prot_map)\n",
    "        unk_count = list(prot_map.values()).count(unk_idx)\n",
    "        coverage = (total_prots - unk_count) / total_prots * 100\n",
    "\n",
    "        print(f\"\\n--- HI·ªÜU SU·∫§T MAPPING (C≈®) ---\")\n",
    "        print(f\"T·ªïng Protein: {total_prots:,}\")\n",
    "        print(f\"‚õî R∆°i v√†o UNK: {unk_count:,} ({unk_count/total_prots*100:.2f}%)\")\n",
    "        print(f\"‚úÖ Coverage   : {coverage:.2f}%\")\n",
    "\n",
    "        # So s√°nh nhanh (Mental check)\n",
    "        print(f\"   (So v·ªõi b·∫£n m·ªõi: B·∫£n m·ªõi UNK ch·ªâ kho·∫£ng 3%)\")\n",
    "\n",
    "    # 4. SOIG G∆Ø∆†NG M·∫∂T \"TH·ª™A TH√ÉI\" (Redundancy Check)\n",
    "    # ƒê√¢y l√† ph·∫ßn quan tr·ªçng ƒë·ªÉ th·∫•y t·∫°i sao c√°ch c≈© d·ªü\n",
    "    if 'taxon_to_idx' in data:\n",
    "        tax_map = data['taxon_to_idx']\n",
    "        print(f\"\\n--- SOI TAXON ID (Top 10) ---\")\n",
    "        # In ra 10 c√°i ƒë·∫ßu ti√™n ƒë·ªÉ xem c√≥ b·ªã tr√πng l·∫∑p strain kh√¥ng\n",
    "        for i, (tid, idx) in enumerate(list(tax_map.items())[:15]):\n",
    "            print(f\"  ID {tid:<10} -> Index {idx}\")\n",
    "\n",
    "    # 5. Ki·ªÉm tra xem c√≥ mapping Roll-up kh√¥ng (Ch·∫Øc ch·∫Øn l√† kh√¥ng)\n",
    "    if 'original_id_to_species_id' not in data:\n",
    "        print(f\"\\n‚ö†Ô∏è File n√†y KH√îNG c√≥ mapping 'original_id_to_species_id'.\")\n",
    "        print(\"üëâ Nghƒ©a l√†: Model ƒëang h·ªçc ID g·ªëc th√¥ (Raw ID). E.coli K12 v√† E.coli g·ªëc b·ªã coi l√† 2 lo√†i kh√°c nhau.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inspect_old_pickle(OLD_PKL_PATH)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
