{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"},{"sourceId":13989811,"sourceType":"datasetVersion","datasetId":8916743},{"sourceId":13990958,"sourceType":"datasetVersion","datasetId":8917132},{"sourceId":13990998,"sourceType":"datasetVersion","datasetId":8917141},{"sourceId":13991073,"sourceType":"datasetVersion","datasetId":8917158},{"sourceId":14054871,"sourceType":"datasetVersion","datasetId":8919436}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # Model 2 - ƒëc 0.63\n\n# import os, sys, math, time, pickle, gc\n\n# import numpy as np\n\n# import pandas as pd\n\n# from tqdm import tqdm\n\n# import torch\n\n# import torch.nn as nn\n\n# import torch.optim as optim\n\n# from torch.utils.data import Dataset, DataLoader\n\n\n# class WideProteinMLP(nn.Module):\n#     def __init__(self, input_dim, num_classes, hidden_dims=[2048, 4096], dropout=0.3):\n\n#         super().__init__()\n\n#         layers = []\n\n#         # Input Norm: Gi√∫p ·ªïn ƒë·ªãnh ƒë·∫ßu v√†o t·ª´ Embeddings\n\n#         layers.append(nn.LayerNorm(input_dim))\n\n#         prev = input_dim\n\n#         for h in hidden_dims:\n\n#             layers.append(nn.Linear(prev, h))\n\n#             layers.append(nn.GELU())  # Activation hi·ªán ƒë·∫°i\n\n#             layers.append(nn.Dropout(dropout))\n\n#             prev = h\n\n#         layers.append(nn.Linear(prev, num_classes))\n\n#         self.net = nn.Sequential(*layers)\n\n#     def forward(self, x):\n\n#         return self.net(x)\n\n\n# # ============================================================================\n\n# # 1. C·∫§U H√åNH (ƒê√É KH·ªöP V·ªöI ·∫¢NH DATASET C·ª¶A B·∫†N)\n\n# # ============================================================================\n\n# CONFIG = {\n#     # --- Input Embeddings ---\n#     \"EMBED_DIR\": \"/kaggle/input/cafa6-embeds\",\n#     # --- Input Labels & Metadata (Dataset c95-cafa6) ---\n#     \"LABEL_DIR\": \"/kaggle/input/c99-cafa6\",\n#     \"VOCAB_FILE\": \"vocab_C99_remove.csv\",\n#     \"TARGET_FILE\": \"train_targets_C99.pkl\",\n#     \"TRAIN_IDS\": \"train_ids_C99_split.npy\",\n#     \"VAL_IDS\": \"val_ids_C99_split.npy\",\n#     \"IA_FILE\": \"/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\",\n#     # --- Model Params ---\n#     \"input_dim\": 1280,\n#     \"hidden_dims\": [2048, 4096],\n#     \"dropout\": 0.3,\n#     \"batch_size\": 32,\n#     \"lr\": 2e-4,\n#     \"weight_decay\": 1e-4,\n#     \"epochs\": 20,\n#     \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n# }\n\n\n# # ============================================================================\n\n# # 2. DATASET (S·ª¨A L·∫†I ƒê·ªÇ ƒê·ªåC ƒê√öNG PATH)\n\n# # ============================================================================\n\n\n# class CAFA6Dataset(Dataset):\n#     def __init__(self, ids_file, targets_file, embed_dir, num_classes):\n\n#         self.ids = np.load(ids_file)\n\n#         with open(targets_file, \"rb\") as f:\n\n#             self.labels_dict = pickle.load(f)\n\n#         self.num_classes = num_classes\n\n#         # Mapping ID -> Index (D·ª±a v√†o file train_ids.txt trong folder embeds)\n\n#         self.id_to_embed_idx = {}\n\n#         with open(os.path.join(embed_dir, \"train_ids.txt\"), \"r\") as f:\n\n#             for idx, line in enumerate(f):\n\n#                 self.id_to_embed_idx[line.strip()] = idx\n\n#         # Mmap embedding\n\n#         self.embed_matrix = np.load(\n#             os.path.join(embed_dir, \"train_embeds.npy\"), mmap_mode=\"r\"\n#         )\n\n#     def __len__(self):\n\n#         return len(self.ids)\n\n#     def __getitem__(self, idx):\n\n#         prot_id = self.ids[idx]\n\n#         embed_idx = self.id_to_embed_idx.get(prot_id)\n\n#         # Load X\n\n#         if embed_idx is None:\n\n#             feat = torch.zeros(1280, dtype=torch.float)\n\n#         else:\n\n#             feat = torch.from_numpy(self.embed_matrix[embed_idx].copy()).float()\n\n#         # Load Y\n\n#         target = torch.zeros(self.num_classes, dtype=torch.float)\n\n#         indices = self.labels_dict.get(prot_id, [])\n\n#         if len(indices) > 0:\n\n#             target[indices] = 1.0\n\n#         return feat, target\n\n\n# # ============================================================================\n\n# # 3. MODEL: WIDE MLP\n\n# # ============================================================================\n\n\n# class WideProteinMLP(nn.Module):\n#     def __init__(self, input_dim, num_classes, hidden_dims=[2048, 4096], dropout=0.3):\n\n#         super().__init__()\n\n#         layers = []\n\n#         layers.append(nn.LayerNorm(input_dim))\n\n#         prev = input_dim\n\n#         for h in hidden_dims:\n\n#             layers.append(nn.Linear(prev, h))\n\n#             layers.append(nn.GELU())\n\n#             layers.append(nn.Dropout(dropout))\n\n#             prev = h\n\n#         layers.append(nn.Linear(prev, num_classes))\n\n#         self.net = nn.Sequential(*layers)\n\n#     def forward(self, x):\n\n#         return self.net(x)\n\n\n# # ============================================================================\n\n# # 4. LOSS: ASL OPTIMIZED (KH·ªöP C√îNG TH·ª®C ·∫¢NH)\n\n# # ============================================================================\n\n\n# class AsymmetricLossOptimized(nn.Module):\n#     def __init__(\n#         self,\n#         gamma_neg=4,\n#         gamma_pos=0,\n#         clip=0.05,\n#         eps=1e-8,\n#         disable_torch_grad_focal_loss=True,\n#     ):\n\n#         super(AsymmetricLossOptimized, self).__init__()\n\n#         self.gamma_neg = gamma_neg\n\n#         self.gamma_pos = gamma_pos\n\n#         self.clip = clip\n\n#         self.eps = eps\n\n#         # T·∫Øt gradient ·ªü ph·∫ßn t√≠nh weight ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ (M·∫πo t·ªëi ∆∞u c·ªßa Top Kaggler)\n\n#         self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n\n#     def forward(self, x, y):\n\n#         # x: logits\n\n#         # y: labels (0/1)\n\n#         # 1. T√≠nh x√°c su·∫•t P\n\n#         x_sigmoid = torch.sigmoid(x)\n\n#         xs_pos = x_sigmoid\n\n#         xs_neg = 1 - x_sigmoid\n\n#         # 2. Asymmetric Clipping (Shift x√°c su·∫•t √¢m)\n\n#         if self.clip is not None and self.clip > 0:\n\n#             xs_neg = (xs_neg + self.clip).clamp(max=1)\n\n#         # 3. T√≠nh Pt (X√°c su·∫•t c·ªßa nh√£n ƒë√∫ng)\n\n#         # pt = p n·∫øu y=1, pt = p_shifted n·∫øu y=0\n\n#         pt = y * xs_pos + (1 - y) * xs_neg\n\n#         # 4. T√≠nh Log Pt (Base Cross Entropy)\n\n#         # log(pt + eps)\n\n#         log_pt = torch.log(pt.clamp(min=self.eps))\n\n#         # 5. T√≠nh h·ªá s·ªë ƒëi·ªÅu ch·ªânh (Modulating Factor)\n\n#         # T·∫Øt gradient ph·∫ßn n√†y ƒë·ªÉ model ch·ªâ t·∫≠p trung t·ªëi ∆∞u x√°c su·∫•t, kh√¥ng t·ªëi ∆∞u c√°i weight\n\n#         if self.disable_torch_grad_focal_loss:\n\n#             torch.set_grad_enabled(False)\n\n#         one_sided_gamma = self.gamma_pos * y + self.gamma_neg * (1 - y)\n\n#         mod_factor = (1 - pt) ** one_sided_gamma\n\n#         if self.disable_torch_grad_focal_loss:\n\n#             torch.set_grad_enabled(True)\n\n#         # 6. Final Loss\n\n#         loss = -mod_factor * log_pt\n\n#         # QUAN TR·ªåNG: D√πng sum() thay v√¨ mean() cho b√†i to√°n sparse multi-label\n\n#         return loss.sum()\n\n\n# # ============================================================================\n\n# # 5. METRIC & TRAINING LOOP\n\n# # ============================================================================\n\n\n# def calculate_fmax_subset_fast(preds, targets, ia_weights):\n\n#     # L·ªçc d√≤ng valid\n\n#     true_sum = np.sum(targets * ia_weights, axis=1)\n\n#     mask = true_sum > 0\n\n#     if mask.sum() == 0:\n#         return 0.0\n\n#     p = preds[mask]\n\n#     t = targets[mask]\n\n#     w_sum_t = true_sum[mask]\n\n#     w_broad = ia_weights.reshape(1, -1)\n\n#     best_f1 = 0.0\n\n#     thresholds = np.linspace(0.0, 1.0, 101)\n\n#     for tau in thresholds:\n\n#         cut = (p >= tau).astype(int)\n\n#         tp = np.sum((cut * t) * w_broad, axis=1)\n\n#         pred_sum = np.sum(cut * w_broad, axis=1)\n\n#         prec = np.divide(tp, pred_sum, out=np.zeros_like(tp), where=pred_sum != 0)\n\n#         rec = np.divide(tp, w_sum_t, out=np.zeros_like(tp), where=w_sum_t != 0)\n\n#         f1 = 0.0\n\n#         if (prec.mean() + rec.mean()) > 0:\n\n#             f1 = 2 * prec.mean() * rec.mean() / (prec.mean() + rec.mean())\n\n#         if f1 > best_f1:\n#             best_f1 = f1\n\n#     return best_f1\n\n\n# def main():\n\n#     print(\n#         f\"Loading vocab from {os.path.join(CONFIG['LABEL_DIR'], CONFIG['VOCAB_FILE'])}...\"\n#     )\n\n#     vocab_df = pd.read_csv(os.path.join(CONFIG[\"LABEL_DIR\"], CONFIG[\"VOCAB_FILE\"]))\n\n#     num_classes = len(vocab_df)\n\n#     # Load IA Weights (Gi·∫£ ƒë·ªãnh file IA c√≥ s·∫µn, n·∫øu ko d√πng ones)\n\n#     try:\n\n#         ia_df = pd.read_csv(\n#             CONFIG[\"IA_FILE\"], sep=\"\\t\", names=[\"term\", \"ia\"], header=None\n#         )\n\n#         ia_dict = ia_df.set_index(\"term\")[\"ia\"].to_dict()\n\n#         ia_weights = np.array([ia_dict.get(t, 1.0) for t in vocab_df[\"term\"].values])\n\n#         print(\"‚úÖ Loaded Real IA Weights.\")\n\n#     except:\n\n#         print(\"‚ö†Ô∏è Warning: IA file not found. Using weights=1.0\")\n\n#         ia_weights = np.ones(num_classes)\n\n#     # Datasets\n\n#     train_ds = CAFA6Dataset(\n#         os.path.join(CONFIG[\"LABEL_DIR\"], CONFIG[\"TRAIN_IDS\"]),\n#         os.path.join(CONFIG[\"LABEL_DIR\"], CONFIG[\"TARGET_FILE\"]),\n#         CONFIG[\"EMBED_DIR\"],\n#         num_classes,\n#     )\n\n#     val_ds = CAFA6Dataset(\n#         os.path.join(CONFIG[\"LABEL_DIR\"], CONFIG[\"VAL_IDS\"]),\n#         os.path.join(CONFIG[\"LABEL_DIR\"], CONFIG[\"TARGET_FILE\"]),\n#         CONFIG[\"EMBED_DIR\"],\n#         num_classes,\n#     )\n\n#     train_loader = DataLoader(\n#         train_ds,\n#         batch_size=CONFIG[\"batch_size\"],\n#         shuffle=True,\n#         num_workers=2,\n#         pin_memory=True,\n#     )\n\n#     val_loader = DataLoader(\n#         val_ds,\n#         batch_size=CONFIG[\"batch_size\"] * 2,\n#         shuffle=False,\n#         num_workers=2,\n#         pin_memory=True,\n#     )\n\n#     # Model & Training\n\n#     model = WideProteinMLP(\n#         CONFIG[\"input_dim\"], num_classes, CONFIG[\"hidden_dims\"], CONFIG[\"dropout\"]\n#     ).to(CONFIG[\"device\"])\n\n#     if torch.cuda.device_count() > 1:\n\n#         print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n\n#         model = nn.DataParallel(model)\n\n#     model = model.to(CONFIG[\"device\"])\n\n#     criterion = AsymmetricLossOptimized(gamma_neg=4, gamma_pos=0.25, clip=0.05)\n\n#     optimizer = optim.AdamW(\n#         model.parameters(), lr=CONFIG[\"lr\"], weight_decay=CONFIG[\"weight_decay\"]\n#     )\n\n#     scheduler = optim.lr_scheduler.OneCycleLR(\n#         optimizer,\n#         max_lr=CONFIG[\"lr\"],\n#         steps_per_epoch=len(train_loader),\n#         epochs=CONFIG[\"epochs\"],\n#     )\n\n#     print(f\"\\nüöÄ Start Training WideMLP ({num_classes} classes)...\")\n\n#     best_score = 0.0\n\n#     for epoch in range(CONFIG[\"epochs\"]):\n\n#         model.train()\n\n#         total_loss = 0\n\n#         pbar = tqdm(train_loader, desc=f\"Ep {epoch+1}\", leave=False)\n\n#         for x, y in pbar:\n\n#             x, y = x.to(CONFIG[\"device\"]), y.to(CONFIG[\"device\"])\n\n#             optimizer.zero_grad()\n\n#             logits = model(x)\n\n#             loss = criterion(logits, y)\n\n#             loss.backward()\n\n#             optimizer.step()\n\n#             scheduler.step()\n\n#             total_loss += loss.item()\n\n#             pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n#         # Validation Phase\n\n#         model.eval()\n\n#         all_preds, all_targets = [], []\n\n#         with torch.no_grad():\n\n#             for x, y in val_loader:\n\n#                 logits = model(x.to(CONFIG[\"device\"]))\n\n#                 all_preds.append(torch.sigmoid(logits).cpu().numpy())\n\n#                 all_targets.append(y.numpy())\n\n#         Y_p = np.vstack(all_preds)\n\n#         Y_t = np.vstack(all_targets)\n\n#         scores = {}\n\n#         for aspect in [\"MFO\", \"BPO\", \"CCO\"]:\n\n#             cols = vocab_df.index[vocab_df[\"aspect\"] == aspect].tolist()\n\n#             if cols:\n\n#                 scores[aspect] = calculate_fmax_subset_fast(\n#                     Y_p[:, cols], Y_t[:, cols], ia_weights[cols]\n#                 )\n\n#         avg_fmax = np.mean(list(scores.values()))\n\n#         print(\n#             f\"Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f} | Val F-max={avg_fmax:.4f} {scores}\"\n#         )\n\n#         if avg_fmax > best_score:\n\n#             best_score = avg_fmax\n\n#             torch.save(model.state_dict(), \"best_model_wide.pth\")\n\n#             print(\"   üèÜ Saved Best Model!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-07T17:19:10.145973Z","iopub.execute_input":"2025-12-07T17:19:10.146749Z","iopub.status.idle":"2025-12-07T17:19:10.178598Z","shell.execute_reply.started":"2025-12-07T17:19:10.146711Z","shell.execute_reply":"2025-12-07T17:19:10.177790Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport sys\nimport gc\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\n# ============================================================================\n# 1. C·∫§U H√åNH C99 (TUNED FOR TAIL)\n# ============================================================================\nCONFIG = {\n    'EMBED_DIR': \"/kaggle/input/cafa6-embeds\",\n    'LABEL_DIR': \"/kaggle/input/c99-cafa6\", # [QUAN TR·ªåNG] ƒê·ªïi folder n·∫øu c·∫ßn\n    'WORK_DIR': \"/kaggle/working\",\n    \n    # --- ƒê·ªîI SANG FILE C99 ---\n    'VOCAB_FILE': \"vocab_C99_remove.csv\",\n    'TARGET_FILE': \"train_targets_C99.pkl\",\n    'TRAIN_IDS': \"train_ids_C99_split.npy\",\n    'VAL_IDS': \"val_ids_C99_split.npy\",\n    \n    'IA_FILE': \"/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\",\n    \n    # --- Model Params (Gi·ªØ nguy√™n) ---\n    'input_dim': 1280,\n    'hidden_dims': [2048, 4096], # ƒê·ªß m·∫°nh cho C99\n    'dropout': 0.3,\n    # num_classes s·∫Ω t·ª± ƒë·ªông load t·ª´ vocab\n    \n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n    \n    # --- Training Config ---\n    'batch_size': 16,    \n    'lr': 2e-4,          # Gi·ªØ nguy√™n t·ªëc ƒë·ªô chu·∫©n\n    'weight_decay': 1e-4,\n    'epochs': 25         # 20 Epoch l√† ƒë·ªß cho Standard\n}\n\n# ============================================================================\n# 2. MODEL & DATASET (GI·ªÆ NGUY√äN B·∫¢N G·ªêC)\n# ============================================================================\nclass WideProteinMLP(nn.Module):\n    def __init__(self, input_dim, num_classes, hidden_dims=[2048, 4096], dropout=0.3):\n        super().__init__()\n        layers = [nn.LayerNorm(input_dim)]\n        prev = input_dim\n        for h in hidden_dims:\n            layers += [nn.Linear(prev, h), nn.GELU(), nn.Dropout(dropout)]\n            prev = h\n        layers.append(nn.Linear(prev, num_classes))\n        self.net = nn.Sequential(*layers)\n    def forward(self, x): return self.net(x)\n\nclass CAFA6Dataset(Dataset):\n    def __init__(self, ids_file, targets_file, embed_dir, num_classes):\n        # Auto-detect path\n        path = os.path.join(CONFIG['LABEL_DIR'], ids_file)\n        if not os.path.exists(path): path = os.path.join(CONFIG['WORK_DIR'], ids_file)\n        self.ids = np.load(path)\n        \n        t_path = os.path.join(CONFIG['LABEL_DIR'], targets_file)\n        if not os.path.exists(t_path): t_path = os.path.join(CONFIG['WORK_DIR'], targets_file)\n        with open(t_path, 'rb') as f: self.labels_dict = pickle.load(f)\n            \n        self.num_classes = num_classes\n        self.id_to_embed_idx = {}\n        with open(os.path.join(embed_dir, \"train_ids.txt\"), 'r') as f:\n            for idx, line in enumerate(f): self.id_to_embed_idx[line.strip()] = idx\n        self.embed_matrix = np.load(os.path.join(embed_dir, \"train_embeds.npy\"), mmap_mode='r')\n\n    def __len__(self): return len(self.ids)\n    def __getitem__(self, idx):\n        prot_id = self.ids[idx]\n        embed_idx = self.id_to_embed_idx.get(prot_id)\n        feat = torch.from_numpy(self.embed_matrix[embed_idx].copy()).float() if embed_idx is not None else torch.zeros(1280)\n        target = torch.zeros(self.num_classes, dtype=torch.float)\n        indices = self.labels_dict.get(prot_id, [])\n        if len(indices) > 0: target[indices] = 1.0\n        return feat, target\n\n# ============================================================================\n# 3. LOSS FUNCTION (ASL)\n# ============================================================================\nclass AsymmetricLossOptimized(nn.Module):\n    def __init__(self, gamma_neg=4, gamma_pos=0, clip=0.05, eps=1e-8):\n        super().__init__()\n        self.gamma_neg = gamma_neg\n        self.gamma_pos = gamma_pos\n        self.clip = clip\n        self.eps = eps\n\n    def forward(self, x, y):\n        x_sigmoid = torch.sigmoid(x)\n        xs_pos = x_sigmoid\n        xs_neg = 1 - x_sigmoid\n        if self.clip > 0: xs_neg = (xs_neg + self.clip).clamp(max=1)\n        \n        pt = y * xs_pos + (1 - y) * xs_neg\n        log_pt = torch.log(pt.clamp(min=self.eps))\n        \n        pos_weight = (1 - xs_pos) ** self.gamma_pos\n        neg_weight = (1 - xs_neg) ** self.gamma_neg\n        \n        loss = - (pos_weight * log_pt * y + neg_weight * log_pt * (1-y))\n        return loss.sum()\n\n# ============================================================================\n# 4. METRIC & MAIN (D√ôNG H√ÄM METRIC CHU·∫®N C·ª¶A B·∫†N)\n# ============================================================================\ndef calculate_fmax_subset(preds, targets, ia_weights):\n    w = ia_weights.reshape(1, -1)\n    true_sum = np.sum(targets * w, axis=1)\n    valid_mask = true_sum > 0\n    if valid_mask.sum() == 0: return 0.0\n    \n    p_sub = preds[valid_mask]\n    t_sub = targets[valid_mask]\n    w_sub = w \n    w_true_sub = true_sum[valid_mask]\n    \n    best_f1 = 0.0\n    # Qu√©t 51 ng∆∞·ª°ng cho nhanh\n    thresholds = np.linspace(0.0, 1.0, 51) \n    \n    for tau in thresholds:\n        cut = (p_sub >= tau).astype(int)\n        tp = np.sum((cut * t_sub) * w_sub, axis=1)\n        pred_sum = np.sum(cut * w_sub, axis=1)\n        \n        prec = np.divide(tp, pred_sum, out=np.zeros_like(tp), where=pred_sum!=0)\n        rec = np.divide(tp, w_true_sub, out=np.zeros_like(tp), where=w_true_sub!=0)\n        \n        avg_p = np.mean(prec)\n        avg_r = np.mean(rec)\n        \n        if (avg_p + avg_r) > 0:\n            f1 = 2 * avg_p * avg_r / (avg_p + avg_r)\n        else:\n            f1 = 0.0\n            \n        if f1 > best_f1: best_f1 = f1\n            \n    return best_f1\n\ndef validate_detailed(model, loader, vocab_df, ia_weights, device):\n    model.eval()\n    all_preds, all_targets = [], []\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            with autocast():\n                logits = model(x)\n            all_preds.append(torch.sigmoid(logits).cpu().numpy())\n            all_targets.append(y.numpy())\n    Y_p = np.vstack(all_preds)\n    Y_t = np.vstack(all_targets)\n    \n    scores = {}\n    # L∆∞u √Ω: C99 c√≥ nhi·ªÅu nh√£n h∆°n, n√™n vi·ªác loop aspect v·∫´n ƒë√∫ng\n    for aspect in ['MFO', 'BPO', 'CCO']:\n        col_indices = vocab_df.index[vocab_df['aspect'] == aspect].tolist()\n        if not col_indices: continue\n        scores[aspect] = calculate_fmax_subset(Y_p[:, col_indices], Y_t[:, col_indices], ia_weights[col_indices])\n    \n    avg_fmax = np.mean(list(scores.values()))\n    return avg_fmax, scores\n\ndef main():\n    print(f\"üöÄ START TRAINING C99 STANDARD (WIDE MLP)...\")\n    \n    # 1. Load Vocab C99\n    path = os.path.join(CONFIG['LABEL_DIR'], CONFIG['VOCAB_FILE'])\n    if not os.path.exists(path): path = os.path.join(CONFIG['WORK_DIR'], CONFIG['VOCAB_FILE'])\n    vocab_df = pd.read_csv(path)\n    num_classes = len(vocab_df)\n    print(f\"   Num Classes: {num_classes}\")\n    \n    # Load IA Weights (Map theo Vocab C99)\n    try:\n        ia_df = pd.read_csv(CONFIG['IA_FILE'], sep='\\t', names=['term', 'ia'], header=None)\n        ia_map = dict(zip(ia_df.term, ia_df.ia))\n        ia_weights = np.array([ia_map.get(t, 1.0) for t in vocab_df.term.values])\n    except: ia_weights = np.ones(num_classes)\n    \n    # 2. Loaders\n    train_ds = CAFA6Dataset(CONFIG['TRAIN_IDS'], CONFIG['TARGET_FILE'], CONFIG['EMBED_DIR'], num_classes)\n    val_ds = CAFA6Dataset(CONFIG['VAL_IDS'], CONFIG['TARGET_FILE'], CONFIG['EMBED_DIR'], num_classes)\n    train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size']*2, shuffle=False, num_workers=2)\n    \n    # 3. Model\n    model = WideProteinMLP(CONFIG['input_dim'], num_classes, CONFIG['hidden_dims'], CONFIG['dropout']).to(CONFIG['device'])\n    if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n    \n    # 4. Config T·ªëi ∆∞u cho C99 (GammaPos=1.0)\n    # [QUAN TR·ªåNG]: TƒÉng gamma_pos l√™n 1.0 ƒë·ªÉ b·∫Øt nh√£n hi·∫øm (C99 nhi·ªÅu nh√£n hi·∫øm)\n    criterion = AsymmetricLossOptimized(gamma_neg=2.0, gamma_pos=2.0, clip=0.05)\n    \n    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=CONFIG['lr'], steps_per_epoch=len(train_loader), epochs=CONFIG['epochs'])\n    scaler = GradScaler()\n\n    best_score = 0.0\n    \n    # 5. Training Loop\n    for epoch in range(CONFIG['epochs']):\n        model.train()\n        loss_sum = 0\n        pbar = tqdm(train_loader, desc=f\"Ep {epoch+1}\", leave=False)\n        \n        for x, y in pbar:\n            x, y = x.to(CONFIG['device']), y.to(CONFIG['device'])\n            optimizer.zero_grad()\n            with autocast():\n                logits = model(x)\n                loss = criterion(logits, y)\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            loss_sum += loss.item()\n            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n            \n        # Validate\n        val_fmax, val_details = validate_detailed(model, val_loader, vocab_df, ia_weights, CONFIG['device'])\n        print(f\"Epoch {epoch+1}: Loss={loss_sum/len(train_loader):.4f} | Val F-max={val_fmax:.4f} {val_details}\")\n        \n        if val_fmax > best_score:\n            best_score = val_fmax\n            torch.save(model.state_dict(), \"best_model_c99_extreme.pth\")\n            print(\"   üèÜ Saved Best C99 Model!\")\n\n    print(f\"\\n‚úÖ DONE! Best F-max: {best_score:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T19:50:14.358724Z","iopub.execute_input":"2025-12-07T19:50:14.359379Z","iopub.status.idle":"2025-12-07T21:36:17.577965Z","shell.execute_reply.started":"2025-12-07T19:50:14.359348Z","shell.execute_reply":"2025-12-07T21:36:17.576968Z"}},"outputs":[{"name":"stdout","text":"üöÄ START TRAINING C99 STANDARD (WIDE MLP)...\n   Num Classes: 15582\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/2595228453.py:208: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\nEp 1:   0%|          | 0/2318 [00:00<?, ?it/s]/tmp/ipykernel_47/2595228453.py:221: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n/tmp/ipykernel_47/2595228453.py:158: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Loss=1349.6962 | Val F-max=0.3839 {'MFO': 0.4326490736805841, 'BPO': 0.28492136867520496, 'CCO': 0.4339804102579939}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Loss=930.5789 | Val F-max=0.4441 {'MFO': 0.49905937145915946, 'BPO': 0.3285555580871629, 'CCO': 0.5046982517380255}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Loss=862.5929 | Val F-max=0.4717 {'MFO': 0.5263946454431395, 'BPO': 0.351633652256163, 'CCO': 0.5369894031558298}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Loss=827.2162 | Val F-max=0.4908 {'MFO': 0.5615143718812048, 'BPO': 0.36296729317801996, 'CCO': 0.5479288220261652}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Loss=804.4565 | Val F-max=0.5044 {'MFO': 0.5840844464315254, 'BPO': 0.36943047515829536, 'CCO': 0.5596521413202947}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Loss=783.8063 | Val F-max=0.5138 {'MFO': 0.5960765999505689, 'BPO': 0.3838128597470661, 'CCO': 0.5616414201262553}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Loss=759.7694 | Val F-max=0.5261 {'MFO': 0.6093392299987254, 'BPO': 0.39581672509360155, 'CCO': 0.573057568786175}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Loss=737.1406 | Val F-max=0.5338 {'MFO': 0.6196968224654886, 'BPO': 0.402920740828982, 'CCO': 0.5787298791261012}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Loss=712.9398 | Val F-max=0.5396 {'MFO': 0.6229208783969501, 'BPO': 0.41052037568723365, 'CCO': 0.585403048646168}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Loss=687.6631 | Val F-max=0.5467 {'MFO': 0.6320124278885052, 'BPO': 0.4201713858573886, 'CCO': 0.5879567007183772}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Loss=668.0043 | Val F-max=0.5519 {'MFO': 0.6342534718870584, 'BPO': 0.4250480323377007, 'CCO': 0.5962617806234629}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Loss=640.8180 | Val F-max=0.5568 {'MFO': 0.6405668769593882, 'BPO': 0.430398062735776, 'CCO': 0.599346049619873}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Loss=619.9566 | Val F-max=0.5575 {'MFO': 0.6366836541859197, 'BPO': 0.4318343979194323, 'CCO': 0.6039114608562418}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Loss=592.7325 | Val F-max=0.5615 {'MFO': 0.6426285554247204, 'BPO': 0.43627063375035247, 'CCO': 0.6057143241729923}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Loss=572.1674 | Val F-max=0.5638 {'MFO': 0.6435681703589546, 'BPO': 0.4408671054700486, 'CCO': 0.6070298055941425}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Loss=562.3977 | Val F-max=0.5650 {'MFO': 0.6457414387138274, 'BPO': 0.43982078554072934, 'CCO': 0.6094999180016677}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Loss=551.7101 | Val F-max=0.5655 {'MFO': 0.6453957461929413, 'BPO': 0.44214246873513435, 'CCO': 0.6088716858662978}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Loss=528.6237 | Val F-max=0.5660 {'MFO': 0.6465341191956316, 'BPO': 0.44229320178259657, 'CCO': 0.6092493178172143}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Loss=521.6768 | Val F-max=0.5667 {'MFO': 0.647140731363098, 'BPO': 0.44288150943292076, 'CCO': 0.6101694024449723}\n   üèÜ Saved Best C99 Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Loss=519.3324 | Val F-max=0.5668 {'MFO': 0.6473101011950865, 'BPO': 0.44370479902528787, 'CCO': 0.6095005918401467}\n   üèÜ Saved Best C99 Model!\n\n‚úÖ DONE! Best F-max: 0.5668\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# !pip install obonet networkx --quiet\n\n# import obonet\n# import networkx as nx\n# import pandas as pd\n# import numpy as np\n# import pickle\n# from collections import deque\n# import os\n\n# CONFIG = {\n#     'OBO_FILE': \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\",\n#     'VOCAB_FILE': \"/kaggle/input/c95-cafa6/vocab_C95_remove.csv\", \n#     'TRAIN_TARGETS': \"/kaggle/input/c95-cafa6/train_targets_C95.pkl\", # C·∫ßn cho t√≠nh IC\n#     'OUTPUT_PKL': \"hierarchy_metadata_v2.pkl\"\n# }\n\n# def build_training_metadata_full():\n#     print(\"üöÄ Building Training Metadata (Depth, IC & HMC Map)...\")\n    \n#     # 1. Load Graph & Vocab C95\n#     graph = obonet.read_obo(CONFIG['OBO_FILE'])\n#     vocab_df = pd.read_csv(CONFIG['VOCAB_FILE'])\n#     vocab_terms = vocab_df['term'].tolist()\n#     go_to_idx = {go: i for i, go in enumerate(vocab_terms)}\n#     n_classes = len(vocab_terms)\n    \n#     # ----------------------------------------------------\n#     # 2. T√çNH DEPTH (BFS) - Logic c·ªßa b·∫°n\n#     # ----------------------------------------------------\n#     roots = [\"GO:0008150\", \"GO:0003674\", \"GO:0005575\"]\n#     depth_dict = {node: float('inf') for node in graph.nodes()}\n#     queue = deque()\n    \n#     for root in roots:\n#         if root in graph:\n#             depth_dict[root] = 0\n#             queue.append(root)\n            \n#     while queue:\n#         node = queue.popleft()\n#         d = depth_dict[node]\n#         # S·ª≠ d·ª•ng 'is_a' v√† 'part_of' edges (ch·ªâ c·∫ßn l·∫•y edge predecessors)\n#         for child in graph.predecessors(node):\n#             if depth_dict[child] > d + 1:\n#                 depth_dict[child] = d + 1\n#                 queue.append(child)\n                \n#     # Map Depth v√†o Vocab C95\n#     depth_arr = np.array([depth_dict.get(t, 0) for t in vocab_terms], dtype=np.float32)\n#     depth_arr[depth_arr == float('inf')] = 0.0\n    \n#     # Normalize\n#     max_d = depth_arr.max()\n#     depth_norm = depth_arr / max_d if max_d > 0 else depth_arr\n#     print(f\"  ‚úÖ Depth Done. Max Depth: {max_d}\")\n    \n#     # ----------------------------------------------------\n#     # 3. T√çNH INFORMATION CONTENT (IC)\n#     # ----------------------------------------------------\n#     # Load targets ƒë·ªÉ t√≠nh t·∫ßn su·∫•t\n#     with open(CONFIG['TRAIN_TARGETS'], 'rb') as f:\n#         labels_dict = pickle.load(f)\n    \n#     # T√≠nh t·∫ßn su·∫•t (Count)\n#     term_counts = np.zeros(n_classes, dtype=np.int32)\n#     total_proteins = len(labels_dict)\n\n#     # T·ªïng h·ª£p nh√£n t·ª´ dictionary\n#     for pid, indices in labels_dict.items():\n#         # Ch·ªâ c·∫ßn ƒë·∫£m b·∫£o index kh√¥ng v∆∞·ª£t qu√° n_classes (ƒë√£ ƒë∆∞·ª£c l·ªçc C95)\n#         for idx in indices:\n#             if idx < n_classes:\n#                 term_counts[idx] += 1\n    \n#     # C√¥ng th·ª©c IC: IC = -log(P(t))\n#     term_prob = term_counts / total_proteins\n#     # Th√™m epsilon nh·ªè ƒë·ªÉ tr√°nh log(0)\n#     term_prob = np.clip(term_prob, 1e-8, 1.0)\n#     ic_arr = -np.log(term_prob)\n    \n#     # Normalize IC (Optional, nh∆∞ng an to√†n h∆°n)\n#     max_ic = ic_arr.max()\n#     ic_norm = ic_arr / max_ic if max_ic > 0 else ic_arr\n#     print(f\"  ‚úÖ IC Done. Max IC: {max_ic:.2f}\")\n\n#     # ----------------------------------------------------\n#     # 4. CHILD-TO-PARENT INDEX MAP (Cho HMC Loss)\n#     # ----------------------------------------------------\n#     child_to_parent_idx = {}\n    \n#     # Ch·ªâ duy·ªát qua c√°c GO Term C√ì trong vocab C95\n#     for term_go in tqdm(vocab_terms, desc=\"Building Parent Map\"):\n#         if term_go not in graph: continue\n        \n#         child_idx = go_to_idx[term_go]\n#         parent_indices = []\n        \n#         # predecessors = parents trong graph\n#         for parent_go in graph.successors(term_go):\n#             if parent_go in go_to_idx:\n#                 parent_indices.append(go_to_idx[parent_go])\n        \n#         if parent_indices:\n#             child_to_parent_idx[child_idx] = parent_indices\n\n#     print(f\"  ‚úÖ HMC Map Done. Total nodes mapped: {len(child_to_parent_idx):,}\")\n    \n#     # 5. Save\n#     metadata = {\n#         'depth_norm': depth_norm,\n#         'ic_norm': ic_norm,\n#         'child_to_parent_idx': child_to_parent_idx\n#     }\n#     with open(CONFIG['OUTPUT_PKL'], 'wb') as f:\n#         pickle.dump(metadata, f)\n        \n#     print(f\"\\n‚úÖ DONE! Saved all metadata to {CONFIG['OUTPUT_PKL']}\")\n\n# if __name__ == \"__main__\":\n#     # ƒê·∫£m b·∫£o file TRAIN_TARGETS ƒë√£ ƒë∆∞·ª£c mount ho·∫∑c t·∫°o ra\n#     if not os.path.exists(CONFIG['TRAIN_TARGETS']):\n#         print(f\"üö® ERROR: Missing {CONFIG['TRAIN_TARGETS']}. Cannot calculate IC.\")\n#     else:\n#         build_training_metadata_full()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T10:37:39.362953Z","iopub.execute_input":"2025-12-07T10:37:39.363246Z","iopub.status.idle":"2025-12-07T10:37:54.208983Z","shell.execute_reply.started":"2025-12-07T10:37:39.363226Z","shell.execute_reply":"2025-12-07T10:37:54.208130Z"}},"outputs":[{"name":"stdout","text":"üöÄ Building Training Metadata (Depth, IC & HMC Map)...\n  ‚úÖ Depth Done. Max Depth: 11.0\n  ‚úÖ IC Done. Max IC: 7.66\n","output_type":"stream"},{"name":"stderr","text":"Building Parent Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6413/6413 [00:00<00:00, 459748.94it/s]","output_type":"stream"},{"name":"stdout","text":"  ‚úÖ HMC Map Done. Total nodes mapped: 6,375\n\n‚úÖ DONE! Saved all metadata to hierarchy_metadata_v2.pkl\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# !pip install obonet networkx --quiet\n\n# import obonet\n# import networkx as nx\n# import pandas as pd\n# import numpy as np\n# import pickle\n# from collections import deque\n# import os\n\n# CONFIG = {\n#     'OBO_FILE': \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\",\n#     'VOCAB_FILE': \"/kaggle/input/c99-cafa6/vocab_C99_remove.csv\", \n#     'TRAIN_TARGETS': \"/kaggle/input/c99-cafa6/train_targets_C99.pkl\", # C·∫ßn cho t√≠nh IC\n#     'OUTPUT_PKL': \"hierarchy_metadata_C99.pkl\"\n# }\n\n# def build_training_metadata_full():\n#     print(\"üöÄ Building Training Metadata (Depth, IC & HMC Map)...\")\n    \n#     # 1. Load Graph & Vocab C95\n#     graph = obonet.read_obo(CONFIG['OBO_FILE'])\n#     vocab_df = pd.read_csv(CONFIG['VOCAB_FILE'])\n#     vocab_terms = vocab_df['term'].tolist()\n#     go_to_idx = {go: i for i, go in enumerate(vocab_terms)}\n#     n_classes = len(vocab_terms)\n    \n#     # ----------------------------------------------------\n#     # 2. T√çNH DEPTH (BFS) - Logic c·ªßa b·∫°n\n#     # ----------------------------------------------------\n#     roots = [\"GO:0008150\", \"GO:0003674\", \"GO:0005575\"]\n#     depth_dict = {node: float('inf') for node in graph.nodes()}\n#     queue = deque()\n    \n#     for root in roots:\n#         if root in graph:\n#             depth_dict[root] = 0\n#             queue.append(root)\n            \n#     while queue:\n#         node = queue.popleft()\n#         d = depth_dict[node]\n#         # S·ª≠ d·ª•ng 'is_a' v√† 'part_of' edges (ch·ªâ c·∫ßn l·∫•y edge predecessors)\n#         for child in graph.predecessors(node):\n#             if depth_dict[child] > d + 1:\n#                 depth_dict[child] = d + 1\n#                 queue.append(child)\n                \n#     # Map Depth v√†o Vocab C95\n#     depth_arr = np.array([depth_dict.get(t, 0) for t in vocab_terms], dtype=np.float32)\n#     depth_arr[depth_arr == float('inf')] = 0.0\n    \n#     # Normalize\n#     max_d = depth_arr.max()\n#     depth_norm = depth_arr / max_d if max_d > 0 else depth_arr\n#     print(f\"  ‚úÖ Depth Done. Max Depth: {max_d}\")\n    \n#     # ----------------------------------------------------\n#     # 3. T√çNH INFORMATION CONTENT (IC)\n#     # ----------------------------------------------------\n#     # Load targets ƒë·ªÉ t√≠nh t·∫ßn su·∫•t\n#     with open(CONFIG['TRAIN_TARGETS'], 'rb') as f:\n#         labels_dict = pickle.load(f)\n    \n#     # T√≠nh t·∫ßn su·∫•t (Count)\n#     term_counts = np.zeros(n_classes, dtype=np.int32)\n#     total_proteins = len(labels_dict)\n\n#     # T·ªïng h·ª£p nh√£n t·ª´ dictionary\n#     for pid, indices in labels_dict.items():\n#         # Ch·ªâ c·∫ßn ƒë·∫£m b·∫£o index kh√¥ng v∆∞·ª£t qu√° n_classes (ƒë√£ ƒë∆∞·ª£c l·ªçc C95)\n#         for idx in indices:\n#             if idx < n_classes:\n#                 term_counts[idx] += 1\n    \n#     # C√¥ng th·ª©c IC: IC = -log(P(t))\n#     term_prob = term_counts / total_proteins\n#     # Th√™m epsilon nh·ªè ƒë·ªÉ tr√°nh log(0)\n#     term_prob = np.clip(term_prob, 1e-8, 1.0)\n#     ic_arr = -np.log(term_prob)\n    \n#     # Normalize IC (Optional, nh∆∞ng an to√†n h∆°n)\n#     max_ic = ic_arr.max()\n#     ic_norm = ic_arr / max_ic if max_ic > 0 else ic_arr\n#     print(f\"  ‚úÖ IC Done. Max IC: {max_ic:.2f}\")\n\n#     # ----------------------------------------------------\n#     # 4. CHILD-TO-PARENT INDEX MAP (Cho HMC Loss)\n#     # ----------------------------------------------------\n#     child_to_parent_idx = {}\n    \n#     # Ch·ªâ duy·ªát qua c√°c GO Term C√ì trong vocab C95\n#     for term_go in tqdm(vocab_terms, desc=\"Building Parent Map\"):\n#         if term_go not in graph: continue\n        \n#         child_idx = go_to_idx[term_go]\n#         parent_indices = []\n        \n#         # predecessors = parents trong graph\n#         for parent_go in graph.successors(term_go):\n#             if parent_go in go_to_idx:\n#                 parent_indices.append(go_to_idx[parent_go])\n        \n#         if parent_indices:\n#             child_to_parent_idx[child_idx] = parent_indices\n\n#     print(f\"  ‚úÖ HMC Map Done. Total nodes mapped: {len(child_to_parent_idx):,}\")\n    \n#     # 5. Save\n#     metadata = {\n#         'depth_norm': depth_norm,\n#         'ic_norm': ic_norm,\n#         'child_to_parent_idx': child_to_parent_idx\n#     }\n#     with open(CONFIG['OUTPUT_PKL'], 'wb') as f:\n#         pickle.dump(metadata, f)\n        \n#     print(f\"\\n‚úÖ DONE! Saved all metadata to {CONFIG['OUTPUT_PKL']}\")\n\n# if __name__ == \"__main__\":\n#     # ƒê·∫£m b·∫£o file TRAIN_TARGETS ƒë√£ ƒë∆∞·ª£c mount ho·∫∑c t·∫°o ra\n#     if not os.path.exists(CONFIG['TRAIN_TARGETS']):\n#         print(f\"üö® ERROR: Missing {CONFIG['TRAIN_TARGETS']}. Cannot calculate IC.\")\n#     else:\n#         build_training_metadata_full()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:15:46.067255Z","iopub.execute_input":"2025-12-07T12:15:46.067790Z","iopub.status.idle":"2025-12-07T12:16:01.800416Z","shell.execute_reply.started":"2025-12-07T12:15:46.067760Z","shell.execute_reply":"2025-12-07T12:16:01.799378Z"}},"outputs":[{"name":"stdout","text":"üöÄ Building Training Metadata (Depth, IC & HMC Map)...\n  ‚úÖ Depth Done. Max Depth: 11.0\n  ‚úÖ IC Done. Max IC: 9.37\n","output_type":"stream"},{"name":"stderr","text":"Building Parent Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15582/15582 [00:00<00:00, 415075.07it/s]","output_type":"stream"},{"name":"stdout","text":"  ‚úÖ HMC Map Done. Total nodes mapped: 15,535\n\n‚úÖ DONE! Saved all metadata to hierarchy_metadata_C99.pkl\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# import os\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import Dataset, DataLoader\n# from torch.cuda.amp import autocast, GradScaler\n# import numpy as np\n# import pandas as pd\n# import pickle\n# from tqdm import tqdm\n# from collections import OrderedDict\n\n# # ============================================================================\n# # 1. C·∫§U H√åNH C99 (FIX L·ªñI PATH)\n# # ============================================================================\n# CONFIG = {\n#     # --- PATHS ---\n#     'EMBED_DIR': \"/kaggle/input/cafa6-embeds\",\n    \n#     # [FIX 1] Tr·ªè ƒë√∫ng v√†o th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu C99\n#     'LABEL_DIR': \"/kaggle/input/c99-cafa6\", \n#     'WORK_DIR': \"/kaggle/working\",\n    \n#     # Files C99\n#     'VOCAB_FILE': \"vocab_C99_remove.csv\",       \n#     'TARGET_FILE': \"train_targets_C99.pkl\",     \n#     'TRAIN_IDS': \"train_ids_C99_split.npy\",     \n#     'VAL_IDS': \"val_ids_C99_split.npy\",         \n#     'METADATA_PKL': \"hierarchy_metadata_C99.pkl\", \n#     'IA_FILE': \"/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\",\n    \n#     # --- MODEL SURGERY ---\n#     'PRETRAINED_MODEL': \"model_v3_hmc_warmup.pth\", \n#     'OLD_VOCAB_FILE': \"/kaggle/input/c95-cafa6/vocab_C95_remove.csv\",\n    \n#     # --- MODEL SPECS ---\n#     'input_dim': 1280,\n#     'hidden_dims': [2048, 4096],\n#     'dropout': 0.3,\n#     'num_classes': 15582,  \n    \n#     # --- TRAINING ---\n#     'device': 'cuda',\n#     'batch_size': 32,      \n#     'lr': 1e-4,             \n#     'weight_decay': 1e-4,\n#     'epochs': 20,           \n#     'hmc_lambda': 0.01,    \n#     'warmup_epochs': 3      \n# }\n\n# # (Gi·ªØ nguy√™n CAFA6Dataset, WideProteinMLP)\n# class CAFA6Dataset(Dataset):\n#     def __init__(self, ids_file, targets_file, embed_dir, num_classes):\n#         path = os.path.join(CONFIG['LABEL_DIR'], ids_file)\n#         if not os.path.exists(path): path = os.path.join(CONFIG['WORK_DIR'], ids_file)\n#         self.ids = np.load(path)\n        \n#         t_path = os.path.join(CONFIG['LABEL_DIR'], targets_file)\n#         if not os.path.exists(t_path): t_path = os.path.join(CONFIG['WORK_DIR'], targets_file)\n#         with open(t_path, 'rb') as f: self.labels_dict = pickle.load(f)\n            \n#         self.num_classes = num_classes\n#         self.id_to_embed_idx = {}\n#         with open(os.path.join(embed_dir, \"train_ids.txt\"), 'r') as f:\n#             for idx, line in enumerate(f): self.id_to_embed_idx[line.strip()] = idx\n#         self.embed_matrix = np.load(os.path.join(embed_dir, \"train_embeds.npy\"), mmap_mode='r')\n\n#     def __len__(self): return len(self.ids)\n#     def __getitem__(self, idx):\n#         prot_id = self.ids[idx]\n#         embed_idx = self.id_to_embed_idx.get(prot_id)\n#         feat = torch.from_numpy(self.embed_matrix[embed_idx].copy()).float() if embed_idx is not None else torch.zeros(1280)\n#         target = torch.zeros(self.num_classes, dtype=torch.float)\n#         indices = self.labels_dict.get(prot_id, [])\n#         if len(indices) > 0: target[indices] = 1.0\n#         return feat, target\n\n# class WideProteinMLP(nn.Module):\n#     def __init__(self, input_dim, num_classes, hidden_dims=[2048, 4096], dropout=0.3):\n#         super().__init__()\n#         layers = [nn.LayerNorm(input_dim)]\n#         prev = input_dim\n#         for h in hidden_dims:\n#             layers += [nn.Linear(prev, h), nn.GELU(), nn.Dropout(dropout)]\n#             prev = h\n#         layers.append(nn.Linear(prev, num_classes))\n#         self.net = nn.Sequential(*layers)\n#     def forward(self, x): return self.net(x)\n\n# # ============================================================================\n# # 2. MODEL SURGERY (FIX R·ª¶I RO)\n# # ============================================================================\n# def load_pretrained_and_expand(model, pretrained_path, old_vocab_path, new_vocab_path):\n#     print(\"üè• Starting Model Surgery (C95 -> C99)...\")\n    \n#     ckpt = torch.load(pretrained_path, map_location='cpu')\n#     old_state = ckpt['model_state'] if isinstance(ckpt, dict) and 'model_state' in ckpt else ckpt\n    \n#     clean_state = OrderedDict()\n#     for k, v in old_state.items(): clean_state[k.replace(\"module.\", \"\")] = v\n    \n#     old_vocab = pd.read_csv(old_vocab_path)['term'].tolist()\n#     new_vocab = pd.read_csv(new_vocab_path)['term'].tolist()\n#     old_term_to_idx = {t: i for i, t in enumerate(old_vocab)}\n    \n#     classifier_weight_key = None\n#     for k, v in clean_state.items():\n#         # [FIX 2] Ki·ªÉm tra k·ªπ h∆°n (ndim=2 v√† shape match)\n#         if v.ndim == 2 and v.shape[0] == len(old_vocab):\n#             classifier_weight_key = k\n#             break\n            \n#     if classifier_weight_key is None:\n#         print(\"‚ùå ERROR: Kh√¥ng t√¨m th·∫•y layer classifier c≈©.\")\n#         return model\n        \n#     classifier_bias_key = classifier_weight_key.replace(\"weight\", \"bias\")\n#     print(f\"   - Found classifier layers: {classifier_weight_key}\")\n    \n#     new_state = model.state_dict()\n#     new_weight = new_state[classifier_weight_key].clone()\n#     new_bias = new_state[classifier_bias_key].clone()\n    \n#     old_weight = clean_state[classifier_weight_key]\n#     old_bias = clean_state[classifier_bias_key]\n    \n#     print(\"   - Copying weights...\")\n#     copied_count = 0\n#     for new_idx, term in enumerate(new_vocab):\n#         if term in old_term_to_idx:\n#             old_idx = old_term_to_idx[term]\n#             new_weight[new_idx] = old_weight[old_idx]\n#             new_bias[new_idx] = old_bias[old_idx]\n#             copied_count += 1\n            \n#     print(f\"   - Copied {copied_count}/{len(new_vocab)} terms from C95.\")\n    \n#     # Update state dict m·ªõi\n#     final_state = OrderedDict()\n#     for k in new_state: # Duy·ªát tr√™n c·∫•u tr√∫c M·ªöI\n#         if k == classifier_weight_key:\n#             final_state[k] = new_weight\n#         elif k == classifier_bias_key:\n#             final_state[k] = new_bias\n#         elif k in clean_state:\n#             final_state[k] = clean_state[k]\n#         else:\n#             # Layer m·ªõi (v√≠ d·ª• dropout th√™m v√†o) -> gi·ªØ random init\n#             final_state[k] = new_state[k]\n\n#     # [FIX 2] Load Strict=False v√† ki·ªÉm tra log\n#     missing, unexpected = model.load_state_dict(final_state, strict=False)\n#     print(f\"   - Missing keys: {len(missing)}\")\n#     print(f\"   - Unexpected keys: {len(unexpected)}\")\n    \n#     print(\"‚úÖ Surgery Complete.\")\n#     return model\n\n# # ============================================================================\n# # 3. LOSS FUNCTION (FIXED: SUM SCALE & SAFE HMC)\n# # ============================================================================\n# class HybridLossC99(nn.Module):\n#     def __init__(self, ia_weights, hierarchy_map, gamma_neg=4, gamma_pos=0, hmc_lambda=0.01, clip=0.05,):\n#         super().__init__()\n#         self.gamma_neg = gamma_neg\n#         self.gamma_pos = gamma_pos\n#         # üö® TH√äM D√íNG N√ÄY (Clip ban ƒë·∫ßu l√† 0.05 theo ASL chu·∫©n)\n#         self.clip = clip\n        \n#         self.hmc_lambda = hmc_lambda\n        \n#         # IA Weights\n#         ia_tensor = torch.tensor(ia_weights, dtype=torch.float32)\n#         ia_tensor = torch.clamp(ia_tensor, 0.5, 3.0)\n#         # Normalize mean=1\n#         ia_tensor = ia_tensor / ia_tensor.mean()\n#         self.register_buffer('ia_weights', ia_tensor)\n        \n#         # HMC Edges\n#         edges = []\n#         if hierarchy_map:\n#             for child, parents in hierarchy_map.items():\n#                 for p in parents: edges.append([int(child), int(p)])\n#         if edges:\n#             self.register_buffer('edge_index', torch.tensor(edges, dtype=torch.long).t())\n#         else:\n#             self.edge_index = None\n\n#     def forward(self, x, y):\n#         x_sig = torch.sigmoid(x)\n#         xs_pos = x_sig\n#         xs_neg = 1 - x_sig\n        \n#         # K·∫πp gi√° tr·ªã an to√†n\n#         if self.clip > 0: xs_neg = (xs_neg + self.clip).clamp(max=1)\n            \n#         pt = y * xs_pos + (1 - y) * xs_neg\n#         log_pt = torch.log(pt.clamp(min=1e-8))\n        \n#         pos_W = (1 - xs_pos) ** self.gamma_pos\n#         neg_W = (1 - xs_neg) ** self.gamma_neg\n        \n#         base_loss = - (pos_W * log_pt * y + neg_W * log_pt * (1-y))\n        \n#         # IA Weighted\n#         w_loss = base_loss * (y * self.ia_weights + (1-y))\n        \n#         # [FIX] D√πng .sum() / batch_size ƒë·ªÉ v·ªÅ scale c≈© (d·ªÖ tune h∆°n mean c·ª±c nh·ªè)\n#         # K·∫øt qu·∫£ Loss s·∫Ω t·∫ßm 1000-2000\n#         asl_loss = w_loss.sum() / x.size(0)\n        \n#         # HMC Loss\n#         if self.hmc_lambda > 0 and self.edge_index is not None:\n#             c_probs = x_sig[:, self.edge_index[0]]\n#             p_probs = x_sig[:, self.edge_index[1]]\n#             diff = torch.relu(c_probs - p_probs)\n            \n#             # [FIX] HMC c≈©ng scale theo batch\n#             hmc_val = diff.sum() / x.size(0)\n            \n#             # [FIX] Safety Cap cho HMC (kh√¥ng cho n√≥ qu√° l·ªõn l√†m n·ªï gradient)\n#             # N·∫øu HMC l·ªõn qu√°, k·∫πp n√≥ l·∫°i\n#             # hmc_val = torch.clamp(hmc_val, max=1000.0) \n            \n#             return asl_loss + (self.hmc_lambda * hmc_val)\n            \n#         return asl_loss\n\n# # ============================================================================\n# # 4. UTILS VALIDATION (B·ªî SUNG QUAN TR·ªåNG)\n# # ============================================================================\n# def calculate_fmax_fast_c99(preds, targets, ia_weights):\n#     # L·ªçc d√≤ng valid\n#     w = ia_weights.reshape(1, -1)\n#     true_sum = np.sum(targets * w, axis=1)\n#     valid_mask = true_sum > 0\n#     if valid_mask.sum() == 0: return 0.0\n    \n#     p = preds[valid_mask]\n#     t = targets[valid_mask]\n#     w_sub = w \n#     w_true_sub = true_sum[valid_mask]\n    \n#     best_f1 = 0.0\n#     # Th·ª≠ qu√©t ng∆∞·ª°ng th·∫•p h∆°n: 0.01 ƒë·∫øn 0.5\n#     thresholds = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5] \n    \n#     for tau in thresholds:\n#         cut = (p >= tau).astype(int)\n#         tp = np.sum((cut * t) * w_sub, axis=1)\n#         pred_sum = np.sum(cut * w_sub, axis=1)\n        \n#         prec = np.divide(tp, pred_sum, out=np.zeros_like(tp), where=pred_sum!=0)\n#         rec = np.divide(tp, w_true_sub, out=np.zeros_like(tp), where=w_true_sub!=0)\n        \n#         # Micro hay Macro kh√¥ng quan tr·ªçng l√∫c debug, mi·ªÖn l√† n√≥ > 0\n#         f1_arr = np.divide(2*prec*rec, prec+rec, out=np.zeros_like(prec), where=(prec+rec)!=0)\n#         f1 = np.mean(f1_arr)\n#         if f1 > best_f1: best_f1 = f1\n            \n#     return best_f1\n\n# # ============================================================================\n# # 5. TRAINING LOOP\n# # ============================================================================\n# def train_c99_finetune_fixed():\n#     print(\"üöÄ STARTING C99 FINE-TUNING (FIXED)...\")\n    \n#     # 1. Resources\n#     print(\"   Loading Vocab C99...\")\n#     path = os.path.join(CONFIG['LABEL_DIR'], CONFIG['VOCAB_FILE'])\n#     if not os.path.exists(path): path = os.path.join(CONFIG['WORK_DIR'], CONFIG['VOCAB_FILE'])\n#     vocab_df = pd.read_csv(path)\n#     CONFIG['num_classes'] = len(vocab_df)\n    \n#     try:\n#         ia_df = pd.read_csv(CONFIG['IA_FILE'], sep='\\t', names=['t','w'], header=None)\n#         ia_map = dict(zip(ia_df.t, ia_df.w))\n#         ia_weights = np.array([ia_map.get(t, 1.0) for t in vocab_df.term.values])\n#     except: ia_weights = np.ones(CONFIG['num_classes'])\n    \n#     # Metadata (HMC Map)\n#     path = os.path.join(CONFIG['WORK_DIR'], CONFIG['METADATA_PKL'])\n#     if not os.path.exists(path): path = os.path.join(CONFIG['LABEL_DIR'], CONFIG['METADATA_PKL'])\n#     with open(path, 'rb') as f: meta = pickle.load(f)\n    \n#     # [FIX 4] L·∫•y ƒë√∫ng key cho HMC (ki·ªÉm tra l·∫°i file metadata c·ªßa b·∫°n d√πng key n√†o)\n#     # Th∆∞·ªùng l√† 'child_to_parent' ho·∫∑c 'child_to_parent_idx'\n#     if 'child_to_parent' in meta:\n#         parent_map = meta['child_to_parent']\n#     else:\n#         parent_map = meta.get('child_to_parent_idx', {})\n\n#     # 2. Datasets\n#     train_ds = CAFA6Dataset(CONFIG['TRAIN_IDS'], CONFIG['TARGET_FILE'], CONFIG['EMBED_DIR'], CONFIG['num_classes'])\n#     val_ds = CAFA6Dataset(CONFIG['VAL_IDS'], CONFIG['TARGET_FILE'], CONFIG['EMBED_DIR'], CONFIG['num_classes'])\n#     train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\n#     # Val loader (Sample nh·ªè ƒë·ªÉ check nhanh)\n#     val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size']*2, shuffle=False, num_workers=2)\n    \n#     # 3. Model Surgery\n#     model = WideProteinMLP(CONFIG['input_dim'], CONFIG['num_classes']).to(CONFIG['device'])\n    \n#     if os.path.exists(CONFIG['PRETRAINED_MODEL']):\n#         old_vocab_path = os.path.join(\"/kaggle/input/c95-cafa6\", \"vocab_C95_remove.csv\") # Path c·ª©ng C95\n#         new_vocab_path = os.path.join(CONFIG['LABEL_DIR'], CONFIG['VOCAB_FILE'])\n#         model = load_pretrained_and_expand(model, CONFIG['PRETRAINED_MODEL'], old_vocab_path, new_vocab_path)\n#     else:\n#         print(\"‚ö†Ô∏è Warning: Train from Scratch!\")\n        \n#     if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n    \n#     # 4. Config\n#     # [FIX] Config Loss & Optim\n#     criterion = HybridLossC99(ia_weights, parent_map, gamma_neg=4, gamma_pos=0.5, hmc_lambda=0.0).to(CONFIG['device'])\n    \n#     # [FIX] Quay v·ªÅ LR chu·∫©n 2e-4 (v√¨ d√πng sum scale)\n#     optimizer = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=2e-4, steps_per_epoch=len(train_loader), epochs=CONFIG['epochs'])\n#     scaler = torch.amp.GradScaler('cuda')\n\n#     best_score = 0.0\n\n#     # 5. Train Loop\n#     for epoch in range(CONFIG['epochs']):\n#         model.train()\n        \n#         if epoch < CONFIG['warmup_epochs']:\n#             criterion.hmc_lambda = 0.0\n#             print(f\"   üî• Epoch {epoch+1}: HMC OFF\")\n#         else:\n#             criterion.hmc_lambda = CONFIG['hmc_lambda']\n#             print(f\"   üîí Epoch {epoch+1}: HMC ON\")\n            \n#         loss_sum = 0\n#         pbar = tqdm(train_loader, desc=f\"Ep {epoch+1}\", leave=False)\n        \n#         for x, y in pbar:\n#             x, y = x.to(CONFIG['device']), y.to(CONFIG['device'])\n#             optimizer.zero_grad()\n#             with autocast():\n#                 logits = model(x)\n#                 loss = criterion(logits, y)\n            \n#             # Clipping 1.0 b·∫Øt bu·ªôc\n#             scaler.scale(loss).backward()\n#             scaler.unscale_(optimizer)\n#             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            \n#             scaler.step(optimizer)\n#             scaler.update()\n#             scheduler.step()\n#             loss_sum += loss.item()\n#             pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n            \n#         # Validation\n#         if (epoch + 1) % 2 == 0:\n#             model.eval()\n#             preds, targets = [], []\n#             with torch.no_grad():\n#                 for i, (x, y) in enumerate(val_loader):\n#                     if i > 50: break \n#                     logits = model(x.to(CONFIG['device']))\n#                     preds.append(torch.sigmoid(logits).cpu().numpy())\n#                     targets.append(y.numpy())\n#             Y_p = np.vstack(preds)\n#             Y_t = np.vstack(targets)\n#             # D√πng h√†m F-max chu·∫©n\n#             val_fmax = calculate_fmax_subset(Y_p, Y_t, ia_weights)\n#             print(f\"Epoch {epoch+1}: Loss={loss_sum/len(train_loader):.4f} | Val F-max={val_fmax:.4f}\")\n#             torch.save(model.state_dict(), \"best_model_c99.pth\")\n#         else:\n#             print(f\"Epoch {epoch+1}: Loss={loss_sum/len(train_loader):.4f}\")\n\n# if __name__ == \"__main__\":\n#     train_c99_finetune_fixed()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:17:28.035361Z","iopub.execute_input":"2025-12-07T13:17:28.036110Z","execution_failed":"2025-12-07T13:28:24.568Z"}},"outputs":[{"name":"stdout","text":"üöÄ STARTING C99 FINE-TUNING (FIXED)...\n   Loading Vocab C99...\nüè• Starting Model Surgery (C95 -> C99)...\n   - Found classifier layers: net.7.weight\n   - Copying weights...\n   - Copied 6413/15582 terms from C95.\n   - Missing keys: 0\n   - Unexpected keys: 0\n‚úÖ Surgery Complete.\n   üî• Epoch 1: HMC OFF\n","output_type":"stream"},{"name":"stderr","text":"Ep 1:   0%|          | 0/2318 [00:00<?, ?it/s]/tmp/ipykernel_47/2401601771.py:342: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Loss=135.9607\n   üî• Epoch 2: HMC OFF\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/2123168177.py:167: RuntimeWarning: invalid value encountered in greater_equal\n  cut = (p_sub >= tau).astype(int)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Loss=nan | Val F-max=0.0000\n   üî• Epoch 3: HMC OFF\n","output_type":"stream"},{"name":"stderr","text":"Ep 3:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1907/2318 [02:24<00:31, 13.21it/s, loss=nan]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}