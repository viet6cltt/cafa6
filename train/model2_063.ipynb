{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab43461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, time, pickle, gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class WideProteinMLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 num_classes,\n",
    "                 hidden_dims=[2048, 4096],\n",
    "                 dropout=0.3):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        layers.append(nn.LayerNorm(input_dim))\n",
    "\n",
    "        prev = input_dim\n",
    "\n",
    "        for h in hidden_dims:\n",
    "\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "\n",
    "            layers.append(nn.GELU()) \n",
    "\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "            prev = h\n",
    "\n",
    "        layers.append(nn.Linear(prev, num_classes))\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# 1. CONFIG\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "\n",
    "    # --- Input Embeddings ---\n",
    "    \"EMBED_DIR\": \"/kaggle/input/cafa6-embeds\",\n",
    "\n",
    "    # --- Input Labels & Metadata (Dataset c95-cafa6) ---\n",
    "    \"LABEL_DIR\": \"/kaggle/input/c95-cafa6\",\n",
    "    \"VOCAB_FILE\": \"vocab_C95_remove.csv\",\n",
    "    \"TARGET_FILE\": \"train_targets_C95.pkl\",\n",
    "    \"TRAIN_IDS\": \"train_ids_C95_split.npy\",\n",
    "    \"VAL_IDS\": \"val_ids_C95_split.npy\",\n",
    "    \"IA_FILE\": \"/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\",\n",
    "\n",
    "    # --- Model Params ---\n",
    "    \"input_dim\": 1280,\n",
    "    \"hidden_dims\": [2048, 4096],\n",
    "    \"dropout\": 0.3,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"epochs\": 20,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# 2. DATASET \n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class CAFA6Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, ids_file, targets_file, embed_dir, num_classes):\n",
    "\n",
    "        self.ids = np.load(ids_file)\n",
    "\n",
    "        with open(targets_file, \"rb\") as f:\n",
    "\n",
    "            self.labels_dict = pickle.load(f)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Mapping ID -> Index \n",
    "\n",
    "        self.id_to_embed_idx = {}\n",
    "\n",
    "        with open(os.path.join(embed_dir, \"train_ids.txt\"), \"r\") as f:\n",
    "\n",
    "            for idx, line in enumerate(f):\n",
    "\n",
    "                self.id_to_embed_idx[line.strip()] = idx\n",
    "\n",
    "        # Mmap embedding\n",
    "\n",
    "        self.embed_matrix = np.load(os.path.join(embed_dir,\n",
    "                                                 \"train_embeds.npy\"),\n",
    "                                    mmap_mode=\"r\")\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        prot_id = self.ids[idx]\n",
    "\n",
    "        embed_idx = self.id_to_embed_idx.get(prot_id)\n",
    "\n",
    "        # Load X\n",
    "\n",
    "        if embed_idx is None:\n",
    "\n",
    "            feat = torch.zeros(1280, dtype=torch.float)\n",
    "\n",
    "        else:\n",
    "\n",
    "            feat = torch.from_numpy(\n",
    "                self.embed_matrix[embed_idx].copy()).float()\n",
    "\n",
    "        # Load Y\n",
    "\n",
    "        target = torch.zeros(self.num_classes, dtype=torch.float)\n",
    "\n",
    "        indices = self.labels_dict.get(prot_id, [])\n",
    "\n",
    "        if len(indices) > 0:\n",
    "\n",
    "            target[indices] = 1.0\n",
    "\n",
    "        return feat, target\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# 3. MODEL: WIDE MLP\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class WideProteinMLP(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 num_classes,\n",
    "                 hidden_dims=[2048, 4096],\n",
    "                 dropout=0.3):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        layers.append(nn.LayerNorm(input_dim))\n",
    "\n",
    "        prev = input_dim\n",
    "\n",
    "        for h in hidden_dims:\n",
    "\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "\n",
    "            layers.append(nn.GELU())\n",
    "\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "            prev = h\n",
    "\n",
    "        layers.append(nn.Linear(prev, num_classes))\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# 4. LOSS: ASL OPTIMIZED \n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class AsymmetricLossOptimized(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        gamma_neg=4,\n",
    "        gamma_pos=0,\n",
    "        clip=0.05,\n",
    "        eps=1e-8,\n",
    "        disable_torch_grad_focal_loss=True,\n",
    "    ):\n",
    "\n",
    "        super(AsymmetricLossOptimized, self).__init__()\n",
    "\n",
    "        self.gamma_neg = gamma_neg\n",
    "\n",
    "        self.gamma_pos = gamma_pos\n",
    "\n",
    "        self.clip = clip\n",
    "\n",
    "        self.eps = eps\n",
    "\n",
    "        # Táº¯t gradient á»Ÿ pháº§n tÃ­nh weight Ä‘á»ƒ tiáº¿t kiá»‡m bá»™ nhá»›\n",
    "\n",
    "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        # x: logits\n",
    "\n",
    "        # y: labels (0/1)\n",
    "\n",
    "        # 1. TÃ­nh xÃ¡c suáº¥t P\n",
    "\n",
    "        x_sigmoid = torch.sigmoid(x)\n",
    "\n",
    "        xs_pos = x_sigmoid\n",
    "\n",
    "        xs_neg = 1 - x_sigmoid\n",
    "\n",
    "        # 2. Asymmetric Clipping (Shift xÃ¡c suáº¥t Ã¢m)\n",
    "\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "\n",
    "            xs_neg = (xs_neg + self.clip).clamp(max=1)\n",
    "\n",
    "        # 3. TÃ­nh Pt (XÃ¡c suáº¥t cá»§a nhÃ£n Ä‘Ãºng)\n",
    "\n",
    "        # pt = p náº¿u y=1, pt = p_shifted náº¿u y=0\n",
    "\n",
    "        pt = y * xs_pos + (1 - y) * xs_neg\n",
    "\n",
    "        # 4. TÃ­nh Log Pt (Base Cross Entropy)\n",
    "\n",
    "        # log(pt + eps)\n",
    "\n",
    "        log_pt = torch.log(pt.clamp(min=self.eps))\n",
    "\n",
    "        # 5. TÃ­nh há»‡ sá»‘ Ä‘iá»u chá»‰nh (Modulating Factor)\n",
    "\n",
    "        if self.disable_torch_grad_focal_loss:\n",
    "\n",
    "            torch.set_grad_enabled(False)\n",
    "\n",
    "        one_sided_gamma = self.gamma_pos * y + self.gamma_neg * (1 - y)\n",
    "\n",
    "        mod_factor = (1 - pt)**one_sided_gamma\n",
    "\n",
    "        if self.disable_torch_grad_focal_loss:\n",
    "\n",
    "            torch.set_grad_enabled(True)\n",
    "\n",
    "        # 6. Final Loss\n",
    "\n",
    "        loss = -mod_factor * log_pt\n",
    "\n",
    "        return loss.sum()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# 5. METRIC & TRAINING LOOP\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def calculate_fmax_subset(preds, targets, ia_weights):\n",
    "    w = ia_weights.reshape(1, -1)\n",
    "    true_sum = np.sum(targets * w, axis=1)\n",
    "    valid_mask = true_sum > 0\n",
    "    if valid_mask.sum() == 0: return 0.0\n",
    "    \n",
    "    p_sub = preds[valid_mask]\n",
    "    t_sub = targets[valid_mask]\n",
    "    w_sub = w \n",
    "    w_true_sub = true_sum[valid_mask]\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    # QuÃ©t 51 ngÆ°á»¡ng cho nhanh\n",
    "    thresholds = np.linspace(0.0, 1.0, 51) \n",
    "    \n",
    "    for tau in thresholds:\n",
    "        cut = (p_sub >= tau).astype(int)\n",
    "        tp = np.sum((cut * t_sub) * w_sub, axis=1)\n",
    "        pred_sum = np.sum(cut * w_sub, axis=1)\n",
    "        \n",
    "        prec = np.divide(tp, pred_sum, out=np.zeros_like(tp), where=pred_sum!=0)\n",
    "        rec = np.divide(tp, w_true_sub, out=np.zeros_like(tp), where=w_true_sub!=0)\n",
    "        \n",
    "        avg_p = np.mean(prec)\n",
    "        avg_r = np.mean(rec)\n",
    "        \n",
    "        if (avg_p + avg_r) > 0:\n",
    "            f1 = 2 * avg_p * avg_r / (avg_p + avg_r)\n",
    "        else:\n",
    "            f1 = 0.0\n",
    "            \n",
    "        if f1 > best_f1: best_f1 = f1\n",
    "            \n",
    "    return best_f1\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    print(\n",
    "        f\"Loading vocab from {os.path.join(CONFIG['LABEL_DIR'], CONFIG['VOCAB_FILE'])}...\"\n",
    "    )\n",
    "\n",
    "    vocab_df = pd.read_csv(\n",
    "        os.path.join(CONFIG[\"LABEL_DIR\"], CONFIG[\"VOCAB_FILE\"]))\n",
    "\n",
    "    num_classes = len(vocab_df)\n",
    "\n",
    "    try:\n",
    "\n",
    "        ia_df = pd.read_csv(CONFIG[\"IA_FILE\"],\n",
    "                            sep=\"\\t\",\n",
    "                            names=[\"term\", \"ia\"],\n",
    "                            header=None)\n",
    "\n",
    "        ia_dict = ia_df.set_index(\"term\")[\"ia\"].to_dict()\n",
    "\n",
    "        ia_weights = np.array(\n",
    "            [ia_dict.get(t, 1.0) for t in vocab_df[\"term\"].values])\n",
    "\n",
    "        print(\"âœ… Loaded Real IA Weights.\")\n",
    "\n",
    "    except:\n",
    "\n",
    "        print(\"âš ï¸ Warning: IA file not found. Using weights=1.0\")\n",
    "\n",
    "        ia_weights = np.ones(num_classes)\n",
    "\n",
    "    # Datasets\n",
    "\n",
    "    train_ds = CAFA6Dataset(\n",
    "        os.path.join(CONFIG[\"LABEL_DIR\"], CONFIG[\"TRAIN_IDS\"]),\n",
    "        os.path.join(CONFIG[\"LABEL_DIR\"], CONFIG[\"TARGET_FILE\"]),\n",
    "        CONFIG[\"EMBED_DIR\"],\n",
    "        num_classes,\n",
    "    )\n",
    "\n",
    "    val_ds = CAFA6Dataset(\n",
    "        os.path.join(CONFIG[\"LABEL_DIR\"], CONFIG[\"VAL_IDS\"]),\n",
    "        os.path.join(CONFIG[\"LABEL_DIR\"], CONFIG[\"TARGET_FILE\"]),\n",
    "        CONFIG[\"EMBED_DIR\"],\n",
    "        num_classes,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CONFIG[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=CONFIG[\"batch_size\"] * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Model & Training\n",
    "\n",
    "    model = WideProteinMLP(CONFIG[\"input_dim\"], num_classes,\n",
    "                           CONFIG[\"hidden_dims\"],\n",
    "                           CONFIG[\"dropout\"]).to(CONFIG[\"device\"])\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "\n",
    "        print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    model = model.to(CONFIG[\"device\"])\n",
    "\n",
    "    criterion = AsymmetricLossOptimized(gamma_neg=4, gamma_pos=0, clip=0.05)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(),\n",
    "                            lr=CONFIG[\"lr\"],\n",
    "                            weight_decay=CONFIG[\"weight_decay\"])\n",
    "\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=CONFIG[\"lr\"],\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        epochs=CONFIG[\"epochs\"],\n",
    "    )\n",
    "\n",
    "    print(f\"\\nğŸš€ Start Training WideMLP ({num_classes} classes)...\")\n",
    "\n",
    "    best_score = 0.0\n",
    "\n",
    "    for epoch in range(CONFIG[\"epochs\"]):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Ep {epoch+1}\", leave=False)\n",
    "\n",
    "        for x, y in pbar:\n",
    "\n",
    "            x, y = x.to(CONFIG[\"device\"]), y.to(CONFIG[\"device\"])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(x)\n",
    "\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        # Validation Phase\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        all_preds, all_targets = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for x, y in val_loader:\n",
    "\n",
    "                logits = model(x.to(CONFIG[\"device\"]))\n",
    "\n",
    "                all_preds.append(torch.sigmoid(logits).cpu().numpy())\n",
    "\n",
    "                all_targets.append(y.numpy())\n",
    "\n",
    "        Y_p = np.vstack(all_preds)\n",
    "\n",
    "        Y_t = np.vstack(all_targets)\n",
    "\n",
    "        scores = {}\n",
    "\n",
    "        for aspect in [\"MFO\", \"BPO\", \"CCO\"]:\n",
    "\n",
    "            cols = vocab_df.index[vocab_df[\"aspect\"] == aspect].tolist()\n",
    "\n",
    "            if cols:\n",
    "\n",
    "                scores[aspect] = calculate_fmax_subset(\n",
    "                    Y_p[:, cols], Y_t[:, cols], ia_weights[cols])\n",
    "\n",
    "        avg_fmax = np.mean(list(scores.values()))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f} | Val F-max={avg_fmax:.4f} {scores}\"\n",
    "        )\n",
    "\n",
    "        if avg_fmax > best_score:\n",
    "\n",
    "            best_score = avg_fmax\n",
    "\n",
    "            torch.save(model.state_dict(), \"best_model_wide.pth\")\n",
    "\n",
    "            print(\"   ğŸ† Saved Best Model!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
