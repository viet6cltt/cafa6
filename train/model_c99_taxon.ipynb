{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T16:52:51.105370Z",
     "iopub.status.busy": "2025-12-09T16:52:51.105133Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ START TRAINING C95 WITH TAXONOMY (SCRATCH)...\n",
      "   Num Taxa Classes: 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1:   0%|          | 0/4636 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=48.4454 | Val Loss=36.7104 | Val F-max=0.4164 {'MFO': 0.4511074268174924, 'BPO': 0.31310458641950534, 'CCO': 0.4850624851299321}\n",
      "   üèÜ Saved Best Model (With Taxon)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=34.8172 | Val Loss=33.5167 | Val F-max=0.4672 {'MFO': 0.5152544827787097, 'BPO': 0.34940452879138123, 'CCO': 0.5369393371142823}\n",
      "   üèÜ Saved Best Model (With Taxon)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=32.6078 | Val Loss=32.5096 | Val F-max=0.4915 {'MFO': 0.5421428515222344, 'BPO': 0.3721202659385291, 'CCO': 0.5601608724794943}\n",
      "   üèÜ Saved Best Model (With Taxon)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=31.7620 | Val Loss=32.2804 | Val F-max=0.5053 {'MFO': 0.5642427842241584, 'BPO': 0.3810004365061482, 'CCO': 0.5705577846251438}\n",
      "   üèÜ Saved Best Model (With Taxon)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=31.3417 | Val Loss=31.9218 | Val F-max=0.5102 {'MFO': 0.5746176124717595, 'BPO': 0.38347273142023247, 'CCO': 0.5725637372579707}\n",
      "   üèÜ Saved Best Model (With Taxon)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss=31.3864 | Val Loss=-1.0000 | Val F-max=-1.0000 {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss=33.1419 | Val Loss=32.0459 | Val F-max=0.5146 {'MFO': 0.583799879140645, 'BPO': 0.38771372510229934, 'CCO': 0.5723037899843055}\n",
      "   üèÜ Saved Best Model (With Taxon)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss=32.2791 | Val Loss=-1.0000 | Val F-max=-1.0000 {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss=32.2891 | Val Loss=31.5388 | Val F-max=0.5322 {'MFO': 0.5947825768375721, 'BPO': 0.4073040161484241, 'CCO': 0.5943963012799065}\n",
      "   üèÜ Saved Best Model (With Taxon)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss=29.3301 | Val Loss=-1.0000 | Val F-max=-1.0000 {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss=28.8322 | Val Loss=31.0693 | Val F-max=0.5453 {'MFO': 0.6089648974788927, 'BPO': 0.42326259102916397, 'CCO': 0.6035403968267343}\n",
      "   üèÜ Saved Best Model (With Taxon)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss=26.5012 | Val Loss=-1.0000 | Val F-max=-1.0000 {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss=28.1675 | Val Loss=30.7357 | Val F-max=0.5549 {'MFO': 0.6204340760214903, 'BPO': 0.43573848103639556, 'CCO': 0.6084855125742561}\n",
      "   üèÜ Saved Best Model (With Taxon)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss=25.2765 | Val Loss=-1.0000 | Val F-max=-1.0000 {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss=23.3206 | Val Loss=36.1480 | Val F-max=0.5637 {'MFO': 0.6314958137001555, 'BPO': 0.4448282732837725, 'CCO': 0.6146354826207328}\n",
      "   üèÜ Saved Best Model (With Taxon)!\n"
     ]
    }
   ],
   "source": [
    "import os, sys, math, time, pickle, gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# 1. CONFIG\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "\n",
    "    # --- Input Embeddings ---\n",
    "    \"EMBED_DIR\": \"/kaggle/input/cafa6-embeds\",\n",
    "\n",
    "    # --- Input Labels & Metadata ---\n",
    "    \"WORK_DIR\": \"/kaggle/working\",\n",
    "    \n",
    "    \"LABEL_DIR\": \"/kaggle/input/c99-cafa6\",\n",
    "\n",
    "    'VOCAB_FILE': \"vocab_C99_remove.csv\",\n",
    "    'TARGET_FILE': \"train_targets_C99.pkl\",\n",
    "    'TRAIN_IDS': \"train_ids_C99_split.npy\",\n",
    "    'VAL_IDS': \"val_ids_C99_split.npy\",\n",
    "    \n",
    "    \"IA_FILE\": \"/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\",\n",
    "\n",
    "    'TAXON_PKL': \"/kaggle/input/cafa6-embeds/taxon_mapping_K140.pkl\",\n",
    "\n",
    "    # --- Model Params ---\n",
    "    \"input_dim\": 1280,\n",
    "    \"hidden_dims\": [2048, 4096],\n",
    "    \"dropout\": 0.3,\n",
    "    'taxon_embed_dim': 64,\n",
    "    \n",
    "    \"batch_size\": 16,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"epochs\": 25,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# 2. MODEL(C√ì TAXONOMY)\n",
    "# ============================================================================\n",
    "\n",
    "class WideProteinMLP_WithTaxon(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_taxa, taxon_dim=64, hidden_dims=[2048, 4096], dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Nh√°nh Protein (ESM-2) \n",
    "        self.bn_input = nn.LayerNorm(input_dim)\n",
    "        \n",
    "        # 2. Nh√°nh Taxonomy \n",
    "        self.taxon_embedding = nn.Embedding(num_taxa, taxon_dim)\n",
    "        self.taxon_norm = nn.LayerNorm(taxon_dim)  \n",
    "\n",
    "        self.unk_idx = num_taxa - 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.taxon_embedding.weight[self.unk_idx].zero_()\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"unk_fixed_vector\",\n",
    "            self.taxon_embedding.weight[self.unk_idx].clone()\n",
    "        )\n",
    "        \n",
    "        # 3. T·ªïng h·ª£p \n",
    "        combined_dim = input_dim + taxon_dim\n",
    "        \n",
    "        layers = []\n",
    "        prev = combined_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "            \n",
    "        layers.append(nn.Linear(prev, num_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x_seq, x_tax):\n",
    "        \n",
    "        feat_seq = self.bn_input(x_seq)\n",
    "        feat_tax = self.taxon_embedding(x_tax)\n",
    "        feat_tax = self.taxon_norm(feat_tax)  \n",
    "        \n",
    "        # [Batch, 1280 + 64]\n",
    "        combined = torch.cat([feat_seq, feat_tax], dim=1)\n",
    "        \n",
    "        return self.net(combined)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. DATASET (LOAD TAXON)\n",
    "# ============================================================================\n",
    "class CAFA6Dataset(Dataset):\n",
    "    def __init__(self, ids_file, targets_file, embed_dir, num_classes, taxon_pkl):\n",
    "        # Load ID & Labels n\n",
    "        path = os.path.join(CONFIG['LABEL_DIR'], ids_file)\n",
    "        if not os.path.exists(path): path = os.path.join(CONFIG['WORK_DIR'], ids_file)\n",
    "        self.ids = np.load(path)\n",
    "        \n",
    "        t_path = os.path.join(CONFIG['LABEL_DIR'], targets_file)\n",
    "        if not os.path.exists(t_path): t_path = os.path.join(CONFIG['WORK_DIR'], targets_file)\n",
    "        with open(t_path, 'rb') as f: self.labels_dict = pickle.load(f)\n",
    "            \n",
    "        self.num_classes = num_classes\n",
    "        self.id_to_embed_idx = {}\n",
    "        with open(os.path.join(embed_dir, \"train_ids.txt\"), 'r') as f:\n",
    "            for idx, line in enumerate(f): self.id_to_embed_idx[line.strip()] = idx\n",
    "        self.embed_matrix = np.load(os.path.join(embed_dir, \"train_embeds.npy\"), mmap_mode='r')\n",
    "        \n",
    "        # Load Taxon Mapping\n",
    "        \n",
    "        if os.path.exists(taxon_pkl):\n",
    "            tax_path = taxon_pkl\n",
    "        else:\n",
    "            tax_path = os.path.join(CONFIG['WORK_DIR'], os.path.basename(taxon_pkl))\n",
    "            print(\"‚ö†Ô∏è USING TAXON PKL FROM WORK_DIR:\", tax_path)\n",
    "            \n",
    "        with open(tax_path, 'rb') as f:\n",
    "            tax_data = pickle.load(f)\n",
    "        \n",
    "        self.prot_to_taxon = tax_data['prot_to_taxon_idx'] \n",
    "        self.default_tax = tax_data['num_taxa_classes'] - 1 \n",
    "\n",
    "    def __len__(self): return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prot_id = self.ids[idx]\n",
    "        \n",
    "        # 1. Embed\n",
    "        embed_idx = self.id_to_embed_idx.get(prot_id)\n",
    "        if embed_idx is None:\n",
    "            feat = torch.zeros(CONFIG[\"input_dim\"], dtype=torch.float32)\n",
    "        else:\n",
    "            feat = torch.from_numpy(self.embed_matrix[embed_idx].copy()).float()\n",
    "        \n",
    "        # 2. Target\n",
    "        target = torch.zeros(self.num_classes, dtype=torch.float)\n",
    "        indices = self.labels_dict.get(prot_id, [])\n",
    "        if len(indices) > 0: target[indices] = 1.0\n",
    "            \n",
    "        # 3. Taxon ID\n",
    "        taxon_idx = self.prot_to_taxon.get(prot_id, self.default_tax)\n",
    "        \n",
    "        return feat, torch.tensor(taxon_idx, dtype=torch.long), target\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# 4. LOSS: ASL OPTIMIZED \n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class AsymmetricLossOptimized(nn.Module):\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=0, clip=0.05, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.eps = eps\n",
    "    def forward(self, x, y):\n",
    "        x_sigmoid = torch.sigmoid(x)\n",
    "        xs_pos = x_sigmoid\n",
    "        xs_neg = 1 - x_sigmoid\n",
    "        if self.clip > 0: xs_neg = (xs_neg + self.clip).clamp(max=1)\n",
    "        pt = y * xs_pos + (1 - y) * xs_neg\n",
    "        log_pt = torch.log(pt.clamp(min=self.eps))\n",
    "        pos_weight = (1 - xs_pos) ** self.gamma_pos\n",
    "        neg_weight = (1 - xs_neg) ** self.gamma_neg\n",
    "        weighted_loss = - (pos_weight * log_pt * y + neg_weight * log_pt * (1-y))\n",
    "        return weighted_loss.sum() / x.size(0)\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# 5. METRIC & TRAINING LOOP\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def calculate_fmax_subset(preds, targets, ia_weights):\n",
    "    w = ia_weights.reshape(1, -1)\n",
    "    true_sum = np.sum(targets * w, axis=1)\n",
    "    valid_mask = true_sum > 0\n",
    "    if valid_mask.sum() == 0: return 0.0\n",
    "    p_sub = preds[valid_mask]; t_sub = targets[valid_mask]; w_sub = w; w_true_sub = true_sum[valid_mask]\n",
    "    best_f1 = 0.0\n",
    "    thresholds = np.linspace(0.0, 1.0, 51) \n",
    "    for tau in thresholds:\n",
    "        cut = (p_sub >= tau).astype(int)\n",
    "        tp = np.sum((cut * t_sub) * w_sub, axis=1)\n",
    "        pred_sum = np.sum(cut * w_sub, axis=1)\n",
    "        prec = np.divide(tp, pred_sum, out=np.zeros_like(tp), where=pred_sum!=0)\n",
    "        rec = np.divide(tp, w_true_sub, out=np.zeros_like(tp), where=w_true_sub!=0)\n",
    "        avg_p = np.mean(prec); avg_r = np.mean(rec)\n",
    "        if (avg_p + avg_r) > 0: f1 = 2 * avg_p * avg_r / (avg_p + avg_r)\n",
    "        else: f1 = 0.0\n",
    "        if f1 > best_f1: best_f1 = f1\n",
    "    return best_f1\n",
    "\n",
    "def validate_with_loss(model, loader, vocab_df, ia_weights, device, criterion):\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    val_loss_sum = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_seq, x_tax, y in loader:\n",
    "            x_seq = x_seq.to(device)\n",
    "            x_tax = x_tax.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                logits = model(x_seq, x_tax)\n",
    "                loss = criterion(logits, y)\n",
    "\n",
    "            val_loss_sum += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "            all_preds.append(torch.sigmoid(logits).cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "\n",
    "    val_loss = val_loss_sum / n_batches\n",
    "\n",
    "    Y_p = np.vstack(all_preds)\n",
    "    Y_t = np.vstack(all_targets)\n",
    "\n",
    "    scores = {}\n",
    "    for aspect in ['MFO', 'BPO', 'CCO']:\n",
    "        col_indices = vocab_df.index[vocab_df['aspect'] == aspect].tolist()\n",
    "        if not col_indices:\n",
    "            continue\n",
    "        scores[aspect] = calculate_fmax_subset(\n",
    "            Y_p[:, col_indices], \n",
    "            Y_t[:, col_indices], \n",
    "            ia_weights[col_indices]\n",
    "        )\n",
    "\n",
    "    avg_fmax = np.mean(list(scores.values()))\n",
    "    return val_loss, avg_fmax, scores\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. MAIN TRAINING LOOP\n",
    "# ============================================================================\n",
    "def train_c99_taxon():\n",
    "    print(\"üöÄ START TRAINING C95 WITH TAXONOMY (SCRATCH)...\")\n",
    "    \n",
    "    # 1. Load Resources\n",
    "    vocab_df = pd.read_csv(os.path.join(CONFIG['LABEL_DIR'], CONFIG['VOCAB_FILE']))\n",
    "    num_classes = len(vocab_df)\n",
    "    \n",
    "    try:\n",
    "        ia_df = pd.read_csv(CONFIG['IA_FILE'], sep='\\t', names=['term', 'ia'], header=None)\n",
    "        ia_map = dict(zip(ia_df.term, ia_df.ia))\n",
    "        ia_weights = np.array([ia_map.get(t, 1.0) for t in vocab_df.term.values])\n",
    "    except: ia_weights = np.ones(num_classes)\n",
    "    \n",
    "    # 2. Dataset & Model\n",
    "    tax_pkl_path = CONFIG['TAXON_PKL'] if os.path.exists(CONFIG['TAXON_PKL']) else os.path.join(CONFIG['WORK_DIR'], CONFIG['TAXON_PKL'])\n",
    "    with open(tax_pkl_path, 'rb') as f: tax_data = pickle.load(f)\n",
    "    num_taxa = tax_data['num_taxa_classes']\n",
    "    print(f\"   Num Taxa Classes: {num_taxa}\")\n",
    "\n",
    "    train_ds = CAFA6Dataset(CONFIG['TRAIN_IDS'], CONFIG['TARGET_FILE'], CONFIG['EMBED_DIR'], num_classes, CONFIG['TAXON_PKL'])\n",
    "    val_ds = CAFA6Dataset(CONFIG['VAL_IDS'], CONFIG['TARGET_FILE'], CONFIG['EMBED_DIR'], num_classes, CONFIG['TAXON_PKL'])\n",
    "    train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size']*2, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Init Model\n",
    "    model = WideProteinMLP_WithTaxon(\n",
    "        input_dim=CONFIG['input_dim'], \n",
    "        num_classes=num_classes, \n",
    "        num_taxa=num_taxa, \n",
    "        taxon_dim=CONFIG['taxon_embed_dim'],\n",
    "        hidden_dims=CONFIG['hidden_dims'], \n",
    "        dropout=CONFIG['dropout']\n",
    "    ).to(CONFIG['device'])\n",
    "    \n",
    "    if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n",
    "    \n",
    "    criterion = AsymmetricLossOptimized(gamma_neg=3.0, gamma_pos=1.2, clip=0.05)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=CONFIG['lr'], steps_per_epoch=len(train_loader), epochs=CONFIG['epochs'])\n",
    "    scaler = GradScaler(\"cuda\")\n",
    "\n",
    "    best_score = 0.0\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        model.train()\n",
    "        loss_sum = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Ep {epoch+1}\", leave=False)\n",
    "        \n",
    "        for x_seq, x_tax, y in pbar:\n",
    "            x_seq = x_seq.to(CONFIG['device'])\n",
    "            x_tax = x_tax.to(CONFIG['device'])\n",
    "            y = y.to(CONFIG['device'])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                logits = model(x_seq, x_tax) \n",
    "                loss = criterion(logits, y)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            #  CH·∫∂N UNK H·ªåC L·∫†I SAU M·ªñI B∆Ø·ªöC UPDATE\n",
    "            with torch.no_grad():\n",
    "                emb = model.module.taxon_embedding if hasattr(model, \"module\") else model.taxon_embedding\n",
    "                unk_idx = model.module.unk_idx if hasattr(model, \"module\") else model.unk_idx\n",
    "                unk_vec = model.module.unk_fixed_vector if hasattr(model, \"module\") else model.unk_fixed_vector\n",
    "                emb.weight[unk_idx].copy_(unk_vec)\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            \n",
    "\n",
    "            loss_sum += loss.item()\n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "        if epoch < 4 or epoch % 2 == 0:\n",
    "            val_loss, val_fmax, val_details = validate_with_loss(\n",
    "                model, val_loader, vocab_df, ia_weights, CONFIG['device'], criterion\n",
    "            )\n",
    "        else:\n",
    "            val_loss = -1\n",
    "            val_fmax = -1\n",
    "            val_details = {}\n",
    "        \n",
    "        print(\n",
    "            f\"Epoch {epoch+1}: \"\n",
    "            f\"Train Loss={loss_sum/len(train_loader):.4f} | \"\n",
    "            f\"Val Loss={val_loss:.4f} | \"\n",
    "            f\"Val F-max={val_fmax:.4f} {val_details}\"\n",
    "        )\n",
    "        \n",
    "        if val_fmax > best_score:\n",
    "            best_score = val_fmax\n",
    "            torch.save(model.state_dict(), \"best_model_c99_taxon.pth\")\n",
    "            print(\"   üèÜ Saved Best Model (With Taxon)!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_c99_taxon()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14084779,
     "sourceId": 116062,
     "sourceType": "competition"
    },
    {
     "datasetId": 8917132,
     "sourceId": 13990958,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8917141,
     "sourceId": 13990998,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8917158,
     "sourceId": 13991073,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8919436,
     "sourceId": 14054871,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8916743,
     "sourceId": 14073266,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
