{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94b14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, time, pickle, gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# 1. CONFIG\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "\n",
    "    # --- Input Embeddings ---\n",
    "    \"EMBED_DIR\": \"/kaggle/input/cafa6-embeds\",\n",
    "\n",
    "    # --- Input Labels & Metadata (Dataset c95-cafa6) ---\n",
    "    \"WORK_DIR\": \"/kaggle/working\",\n",
    "    \n",
    "    \"LABEL_DIR\": \"/kaggle/input/c95-cafa6\",\n",
    "\n",
    "    'VOCAB_FILE': \"vocab_C95_remove.csv\",\n",
    "    'TARGET_FILE': \"train_targets_C95.pkl\",\n",
    "    'TRAIN_IDS': \"train_ids_C95_split.npy\",\n",
    "    'VAL_IDS': \"val_ids_C95_split.npy\",\n",
    "    \n",
    "    \"IA_FILE\": \"/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\",\n",
    "\n",
    "    'TAXON_PKL': \"/kaggle/input/cafa6-embeds/taxon_mapping_K140.pkl\",\n",
    "\n",
    "    # --- Model Params ---\n",
    "    \"input_dim\": 1280,\n",
    "    \"hidden_dims\": [2048, 4096],\n",
    "    \"dropout\": 0.3,\n",
    "    'taxon_embed_dim': 64,\n",
    "    \n",
    "    \"batch_size\": 16,\n",
    "    \"lr\": 2e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"epochs\": 25,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# 2. MODEL M·ªöI (C√ì TAXONOMY)\n",
    "# ============================================================================\n",
    "\n",
    "class WideProteinMLP_WithTaxon(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_taxa, taxon_dim=64, hidden_dims=[2048, 4096], dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Nh√°nh Protein (ESM-2) \n",
    "        self.bn_input = nn.LayerNorm(input_dim)\n",
    "        \n",
    "        # 2. Nh√°nh Taxonomy - Embedding h·ªçc ƒë∆∞·ª£c\n",
    "        self.taxon_embedding = nn.Embedding(num_taxa, taxon_dim)\n",
    "        self.taxon_norm = nn.LayerNorm(taxon_dim)  \n",
    "\n",
    "        self.unk_idx = num_taxa - 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.taxon_embedding.weight[self.unk_idx].zero_()\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"unk_fixed_vector\",\n",
    "            self.taxon_embedding.weight[self.unk_idx].clone()\n",
    "        )\n",
    "        \n",
    "        # 3. T·ªïng h·ª£p\n",
    "        combined_dim = input_dim + taxon_dim\n",
    "        \n",
    "        layers = []\n",
    "        prev = combined_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "            \n",
    "        layers.append(nn.Linear(prev, num_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x_seq, x_tax):\n",
    "        # x_seq: [batch, 1280]\n",
    "        # x_tax: [batch] (Int IDs)\n",
    "        \n",
    "        feat_seq = self.bn_input(x_seq)\n",
    "        feat_tax = self.taxon_embedding(x_tax)\n",
    "        feat_tax = self.taxon_norm(feat_tax)  \n",
    "        \n",
    "        # [Batch, 1280 + 64]\n",
    "        combined = torch.cat([feat_seq, feat_tax], dim=1)\n",
    "        \n",
    "        return self.net(combined)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. DATASET C·∫¨P NH·∫¨T (LOAD TAXON)\n",
    "# ============================================================================\n",
    "class CAFA6Dataset(Dataset):\n",
    "    def __init__(self, ids_file, targets_file, embed_dir, num_classes, taxon_pkl):\n",
    "        path = os.path.join(CONFIG['LABEL_DIR'], ids_file)\n",
    "        if not os.path.exists(path): path = os.path.join(CONFIG['WORK_DIR'], ids_file)\n",
    "        self.ids = np.load(path)\n",
    "        \n",
    "        t_path = os.path.join(CONFIG['LABEL_DIR'], targets_file)\n",
    "        if not os.path.exists(t_path): t_path = os.path.join(CONFIG['WORK_DIR'], targets_file)\n",
    "        with open(t_path, 'rb') as f: self.labels_dict = pickle.load(f)\n",
    "            \n",
    "        self.num_classes = num_classes\n",
    "        self.id_to_embed_idx = {}\n",
    "        with open(os.path.join(embed_dir, \"train_ids.txt\"), 'r') as f:\n",
    "            for idx, line in enumerate(f): self.id_to_embed_idx[line.strip()] = idx\n",
    "        self.embed_matrix = np.load(os.path.join(embed_dir, \"train_embeds.npy\"), mmap_mode='r')\n",
    "        \n",
    "        # T√¨m file ·ªü work dir ho·∫∑c input dir\n",
    "        \n",
    "        if os.path.exists(taxon_pkl):\n",
    "            tax_path = taxon_pkl\n",
    "        else:\n",
    "            tax_path = os.path.join(CONFIG['WORK_DIR'], os.path.basename(taxon_pkl))\n",
    "            print(\"‚ö†Ô∏è USING TAXON PKL FROM WORK_DIR:\", tax_path)\n",
    "            \n",
    "        with open(tax_path, 'rb') as f:\n",
    "            tax_data = pickle.load(f)\n",
    "        \n",
    "        self.prot_to_taxon = tax_data['prot_to_taxon_idx'] \n",
    "        self.default_tax = tax_data['num_taxa_classes'] - 1 \n",
    "\n",
    "    def __len__(self): return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prot_id = self.ids[idx]\n",
    "        \n",
    "        # 1. Embed\n",
    "        embed_idx = self.id_to_embed_idx.get(prot_id)\n",
    "        if embed_idx is None:\n",
    "            feat = torch.zeros(CONFIG[\"input_dim\"], dtype=torch.float32)\n",
    "        else:\n",
    "            feat = torch.from_numpy(self.embed_matrix[embed_idx].copy()).float()\n",
    "        \n",
    "        # 2. Target\n",
    "        target = torch.zeros(self.num_classes, dtype=torch.float)\n",
    "        indices = self.labels_dict.get(prot_id, [])\n",
    "        if len(indices) > 0: target[indices] = 1.0\n",
    "            \n",
    "        # 3. [M·ªöI] Taxon ID\n",
    "        # L·∫•y Taxon Index, n·∫øu kh√¥ng c√≥ th√¨ tr·∫£ v·ªÅ default (UNK)\n",
    "        taxon_idx = self.prot_to_taxon.get(prot_id, self.default_tax)\n",
    "        \n",
    "        # Tr·∫£ v·ªÅ 3 gi√° tr·ªã\n",
    "        return feat, torch.tensor(taxon_idx, dtype=torch.long), target\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# 4. LOSS: ASL OPTIMIZED \n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class AsymmetricLossOptimized(nn.Module):\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=0, clip=0.05, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.eps = eps\n",
    "    def forward(self, x, y):\n",
    "        x_sigmoid = torch.sigmoid(x)\n",
    "        xs_pos = x_sigmoid\n",
    "        xs_neg = 1 - x_sigmoid\n",
    "        if self.clip > 0: xs_neg = (xs_neg + self.clip).clamp(max=1)\n",
    "        pt = y * xs_pos + (1 - y) * xs_neg\n",
    "        log_pt = torch.log(pt.clamp(min=self.eps))\n",
    "        pos_weight = (1 - xs_pos) ** self.gamma_pos\n",
    "        neg_weight = (1 - xs_neg) ** self.gamma_neg\n",
    "        weighted_loss = - (pos_weight * log_pt * y + neg_weight * log_pt * (1-y))\n",
    "        return weighted_loss.sum()\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# 5. METRIC & TRAINING LOOP\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def calculate_fmax_subset(preds, targets, ia_weights):\n",
    "    w = ia_weights.reshape(1, -1)\n",
    "    true_sum = np.sum(targets * w, axis=1)\n",
    "    valid_mask = true_sum > 0\n",
    "    if valid_mask.sum() == 0: return 0.0\n",
    "    p_sub = preds[valid_mask]; t_sub = targets[valid_mask]; w_sub = w; w_true_sub = true_sum[valid_mask]\n",
    "    best_f1 = 0.0\n",
    "    thresholds = np.linspace(0.0, 1.0, 51) \n",
    "    for tau in thresholds:\n",
    "        cut = (p_sub >= tau).astype(int)\n",
    "        tp = np.sum((cut * t_sub) * w_sub, axis=1)\n",
    "        pred_sum = np.sum(cut * w_sub, axis=1)\n",
    "        prec = np.divide(tp, pred_sum, out=np.zeros_like(tp), where=pred_sum!=0)\n",
    "        rec = np.divide(tp, w_true_sub, out=np.zeros_like(tp), where=w_true_sub!=0)\n",
    "        avg_p = np.mean(prec); avg_r = np.mean(rec)\n",
    "        if (avg_p + avg_r) > 0: f1 = 2 * avg_p * avg_r / (avg_p + avg_r)\n",
    "        else: f1 = 0.0\n",
    "        if f1 > best_f1: best_f1 = f1\n",
    "    return best_f1\n",
    "\n",
    "def validate_detailed(model, loader, vocab_df, ia_weights, device):\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for x_seq, x_tax, y in loader:\n",
    "            x_seq = x_seq.to(device)\n",
    "            x_tax = x_tax.to(device)\n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                logits = model(x_seq, x_tax)\n",
    "            all_preds.append(torch.sigmoid(logits).cpu().numpy())\n",
    "            all_targets.append(y.numpy())\n",
    "    Y_p = np.vstack(all_preds); Y_t = np.vstack(all_targets)\n",
    "    scores = {}\n",
    "    for aspect in ['MFO', 'BPO', 'CCO']:\n",
    "        col_indices = vocab_df.index[vocab_df['aspect'] == aspect].tolist()\n",
    "        if not col_indices: continue\n",
    "        scores[aspect] = calculate_fmax_subset(Y_p[:, col_indices], Y_t[:, col_indices], ia_weights[col_indices])\n",
    "    avg_fmax = np.mean(list(scores.values()))\n",
    "    return avg_fmax, scores\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. MAIN TRAINING LOOP\n",
    "# ============================================================================\n",
    "def train_c95_taxon():\n",
    "    print(\"üöÄ START TRAINING C95 WITH TAXONOMY (SCRATCH)...\")\n",
    "    \n",
    "    # 1. Load Resources\n",
    "    vocab_df = pd.read_csv(os.path.join(CONFIG['LABEL_DIR'], CONFIG['VOCAB_FILE']))\n",
    "    num_classes = len(vocab_df)\n",
    "    \n",
    "    try:\n",
    "        ia_df = pd.read_csv(CONFIG['IA_FILE'], sep='\\t', names=['term', 'ia'], header=None)\n",
    "        ia_map = dict(zip(ia_df.term, ia_df.ia))\n",
    "        ia_weights = np.array([ia_map.get(t, 1.0) for t in vocab_df.term.values])\n",
    "    except: ia_weights = np.ones(num_classes)\n",
    "    \n",
    "    # 2. Dataset & Model\n",
    "    tax_pkl_path = CONFIG['TAXON_PKL'] if os.path.exists(CONFIG['TAXON_PKL']) else os.path.join(CONFIG['WORK_DIR'], CONFIG['TAXON_PKL'])\n",
    "    with open(tax_pkl_path, 'rb') as f: tax_data = pickle.load(f)\n",
    "    num_taxa = tax_data['num_taxa_classes']\n",
    "    print(f\"   Num Taxa Classes: {num_taxa}\")\n",
    "\n",
    "    train_ds = CAFA6Dataset(CONFIG['TRAIN_IDS'], CONFIG['TARGET_FILE'], CONFIG['EMBED_DIR'], num_classes, CONFIG['TAXON_PKL'])\n",
    "    val_ds = CAFA6Dataset(CONFIG['VAL_IDS'], CONFIG['TARGET_FILE'], CONFIG['EMBED_DIR'], num_classes, CONFIG['TAXON_PKL'])\n",
    "    train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size']*2, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Init Model \n",
    "    model = WideProteinMLP_WithTaxon(\n",
    "        input_dim=CONFIG['input_dim'], \n",
    "        num_classes=num_classes, \n",
    "        num_taxa=num_taxa, \n",
    "        taxon_dim=CONFIG['taxon_embed_dim'],\n",
    "        hidden_dims=CONFIG['hidden_dims'], \n",
    "        dropout=CONFIG['dropout']\n",
    "    ).to(CONFIG['device'])\n",
    "    \n",
    "    if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n",
    "    \n",
    "    criterion = AsymmetricLossOptimized(gamma_neg=2.5, gamma_pos=0, clip=0.05)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=CONFIG['lr'], steps_per_epoch=len(train_loader), epochs=CONFIG['epochs'])\n",
    "    scaler = GradScaler(\"cuda\")\n",
    "\n",
    "    best_score = 0.0\n",
    "    \n",
    "    # Loop\n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        model.train()\n",
    "        loss_sum = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Ep {epoch+1}\", leave=False)\n",
    "        \n",
    "        for x_seq, x_tax, y in pbar: \n",
    "            x_seq = x_seq.to(CONFIG['device'])\n",
    "            x_tax = x_tax.to(CONFIG['device'])\n",
    "            y = y.to(CONFIG['device'])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                logits = model(x_seq, x_tax)\n",
    "                loss = criterion(logits, y)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                emb = model.module.taxon_embedding if hasattr(model, \"module\") else model.taxon_embedding\n",
    "                unk_idx = model.module.unk_idx if hasattr(model, \"module\") else model.unk_idx\n",
    "                unk_vec = model.module.unk_fixed_vector if hasattr(model, \"module\") else model.unk_fixed_vector\n",
    "                emb.weight[unk_idx].copy_(unk_vec)\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            \n",
    "\n",
    "            loss_sum += loss.item()\n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "            \n",
    "        val_fmax, val_details = validate_detailed(model, val_loader, vocab_df, ia_weights, CONFIG['device'])\n",
    "        print(f\"Epoch {epoch+1}: Loss={loss_sum/len(train_loader):.4f} | Val F-max={val_fmax:.4f} {val_details}\")\n",
    "        \n",
    "        if val_fmax > best_score:\n",
    "            best_score = val_fmax\n",
    "            torch.save(model.state_dict(), \"best_model_c95_taxon.pth\")\n",
    "            print(\"   üèÜ Saved Best Model (With Taxon)!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_c95_taxon()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a37c0b",
   "metadata": {},
   "source": [
    "Epoch 1: Loss=2234.6272 | Val F-max=0.4149 {'MFO': 0.4521210724417265, 'BPO': 0.3122263168422698, 'CCO': 0.48028865099381846}\n",
    "\n",
    "üèÜ Saved Best Model (With Taxon)!\n",
    "\n",
    "\n",
    "Epoch 2: Loss=1718.1856 | Val F-max=0.4646 {'MFO': 0.5139700758043899, 'BPO': 0.3504591698159337, 'CCO': 0.5294352775050785}\n",
    "\n",
    "üèÜ Saved Best Model (With Taxon)!\n",
    "\n",
    "\n",
    "Epoch 3: Loss=1629.6295 | Val F-max=0.4949 {'MFO': 0.5528053533263427, 'BPO': 0.3677360049264319, 'CCO': 0.5640186982491494}\n",
    "\n",
    "üèÜ Saved Best Model (With Taxon)!\n",
    "\n",
    "\n",
    "Epoch 4: Loss=1584.2367 | Val F-max=0.5090 {'MFO': 0.5695660534038522, 'BPO': 0.38302653231172723, 'CCO': 0.5744884560189405}\n",
    "\n",
    "üèÜ Saved Best Model (With Taxon)!\n",
    "\n",
    "\n",
    "Epoch 5: Loss=1555.7461 | Val F-max=0.5242 {'MFO': 0.5934636678977137, 'BPO': 0.3967266141178558, 'CCO': 0.5825291143062825}\n",
    "\n",
    "üèÜ Saved Best Model (With Taxon)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff186f72",
   "metadata": {},
   "source": [
    "#### ƒê√°nh gi√° l√† th·∫•t b·∫°i so v·ªõi b·∫£n g·ªëc c·ªßa c95 (k d√πng taxon) => c√≥ l·∫Ω v√¨ b·∫£n embeds ESM2 650M ƒë√£ ƒë·ªß m·∫°nh v·ªõi C95 -> ƒë·ªß nh·∫≠n bi·∫øt c√°c nh√£n trong t·∫≠p C95 -> vi·ªác th√™m th√¥ng tin lo√†i v√†o khi·∫øn b·ªã lo·∫°n h∆°n."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
