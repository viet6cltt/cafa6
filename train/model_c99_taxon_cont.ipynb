{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"},{"sourceId":13990958,"sourceType":"datasetVersion","datasetId":8917132},{"sourceId":13990998,"sourceType":"datasetVersion","datasetId":8917141},{"sourceId":13991073,"sourceType":"datasetVersion","datasetId":8917158},{"sourceId":14073266,"sourceType":"datasetVersion","datasetId":8916743},{"sourceId":14077609,"sourceType":"datasetVersion","datasetId":8919436}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import os\n# import sys\n# import gc\n# import pickle\n# import numpy as np\n# import pandas as pd\n# from tqdm import tqdm\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import Dataset, DataLoader\n# from torch.cuda.amp import autocast, GradScaler\n\n# # ============================================================================\n# # 1. C·∫§U H√åNH C99 (TUNED FOR TAIL)\n# # ============================================================================\n# CONFIG = {\n#     'EMBED_DIR': \"/kaggle/input/cafa6-embeds\",\n#     'LABEL_DIR': \"/kaggle/input/c99-cafa6\", # [QUAN TR·ªåNG] ƒê·ªïi folder n·∫øu c·∫ßn\n#     'WORK_DIR': \"/kaggle/working\",\n    \n#     # --- ƒê·ªîI SANG FILE C99 ---\n#     'VOCAB_FILE': \"vocab_C99_remove.csv\",\n#     'TARGET_FILE': \"train_targets_C99.pkl\",\n#     'TRAIN_IDS': \"train_ids_C99_split.npy\",\n#     'VAL_IDS': \"val_ids_C99_split.npy\",\n    \n#     'IA_FILE': \"/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\",\n    \n#     # --- Model Params (Gi·ªØ nguy√™n) ---\n#     'input_dim': 1280,\n#     'hidden_dims': [2048, 4096], # ƒê·ªß m·∫°nh cho C99\n#     'dropout': 0.3,\n#     # num_classes s·∫Ω t·ª± ƒë·ªông load t·ª´ vocab\n    \n#     'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n    \n#     # --- Training Config ---\n#     'batch_size': 16,    \n#     'lr': 2e-4,          # Gi·ªØ nguy√™n t·ªëc ƒë·ªô chu·∫©n\n#     'weight_decay': 1e-4,\n#     'epochs': 25         # 20 Epoch l√† ƒë·ªß cho Standard\n# }\n\n# # ============================================================================\n# # 2. MODEL & DATASET (GI·ªÆ NGUY√äN B·∫¢N G·ªêC)\n# # ============================================================================\n# class WideProteinMLP(nn.Module):\n#     def __init__(self, input_dim, num_classes, hidden_dims=[2048, 4096], dropout=0.3):\n#         super().__init__()\n#         layers = [nn.LayerNorm(input_dim)]\n#         prev = input_dim\n#         for h in hidden_dims:\n#             layers += [nn.Linear(prev, h), nn.GELU(), nn.Dropout(dropout)]\n#             prev = h\n#         layers.append(nn.Linear(prev, num_classes))\n#         self.net = nn.Sequential(*layers)\n#     def forward(self, x): return self.net(x)\n\n# class CAFA6Dataset(Dataset):\n#     def __init__(self, ids_file, targets_file, embed_dir, num_classes):\n#         # Auto-detect path\n#         path = os.path.join(CONFIG['LABEL_DIR'], ids_file)\n#         if not os.path.exists(path): path = os.path.join(CONFIG['WORK_DIR'], ids_file)\n#         self.ids = np.load(path)\n        \n#         t_path = os.path.join(CONFIG['LABEL_DIR'], targets_file)\n#         if not os.path.exists(t_path): t_path = os.path.join(CONFIG['WORK_DIR'], targets_file)\n#         with open(t_path, 'rb') as f: self.labels_dict = pickle.load(f)\n            \n#         self.num_classes = num_classes\n#         self.id_to_embed_idx = {}\n#         with open(os.path.join(embed_dir, \"train_ids.txt\"), 'r') as f:\n#             for idx, line in enumerate(f): self.id_to_embed_idx[line.strip()] = idx\n#         self.embed_matrix = np.load(os.path.join(embed_dir, \"train_embeds.npy\"), mmap_mode='r')\n\n#     def __len__(self): return len(self.ids)\n#     def __getitem__(self, idx):\n#         prot_id = self.ids[idx]\n#         embed_idx = self.id_to_embed_idx.get(prot_id)\n#         feat = torch.from_numpy(self.embed_matrix[embed_idx].copy()).float() if embed_idx is not None else torch.zeros(1280)\n#         target = torch.zeros(self.num_classes, dtype=torch.float)\n#         indices = self.labels_dict.get(prot_id, [])\n#         if len(indices) > 0: target[indices] = 1.0\n#         return feat, target\n\n# # ============================================================================\n# # 3. LOSS FUNCTION (ASL)\n# # ============================================================================\n# class AsymmetricLossOptimized(nn.Module):\n#     def __init__(self, gamma_neg=4, gamma_pos=0, clip=0.05, eps=1e-8):\n#         super().__init__()\n#         self.gamma_neg = gamma_neg\n#         self.gamma_pos = gamma_pos\n#         self.clip = clip\n#         self.eps = eps\n\n#     def forward(self, x, y):\n#         x_sigmoid = torch.sigmoid(x)\n#         xs_pos = x_sigmoid\n#         xs_neg = 1 - x_sigmoid\n#         if self.clip > 0: xs_neg = (xs_neg + self.clip).clamp(max=1)\n        \n#         pt = y * xs_pos + (1 - y) * xs_neg\n#         log_pt = torch.log(pt.clamp(min=self.eps))\n        \n#         pos_weight = (1 - xs_pos) ** self.gamma_pos\n#         neg_weight = (1 - xs_neg) ** self.gamma_neg\n        \n#         loss = - (pos_weight * log_pt * y + neg_weight * log_pt * (1-y))\n#         return loss.sum()\n\n# # ============================================================================\n# # 4. METRIC & MAIN (D√ôNG H√ÄM METRIC CHU·∫®N C·ª¶A B·∫†N)\n# # ============================================================================\n# def calculate_fmax_subset(preds, targets, ia_weights):\n#     w = ia_weights.reshape(1, -1)\n#     true_sum = np.sum(targets * w, axis=1)\n#     valid_mask = true_sum > 0\n#     if valid_mask.sum() == 0: return 0.0\n    \n#     p_sub = preds[valid_mask]\n#     t_sub = targets[valid_mask]\n#     w_sub = w \n#     w_true_sub = true_sum[valid_mask]\n    \n#     best_f1 = 0.0\n#     # Qu√©t 51 ng∆∞·ª°ng cho nhanh\n#     thresholds = np.linspace(0.0, 1.0, 51) \n    \n#     for tau in thresholds:\n#         cut = (p_sub >= tau).astype(int)\n#         tp = np.sum((cut * t_sub) * w_sub, axis=1)\n#         pred_sum = np.sum(cut * w_sub, axis=1)\n        \n#         prec = np.divide(tp, pred_sum, out=np.zeros_like(tp), where=pred_sum!=0)\n#         rec = np.divide(tp, w_true_sub, out=np.zeros_like(tp), where=w_true_sub!=0)\n        \n#         avg_p = np.mean(prec)\n#         avg_r = np.mean(rec)\n        \n#         if (avg_p + avg_r) > 0:\n#             f1 = 2 * avg_p * avg_r / (avg_p + avg_r)\n#         else:\n#             f1 = 0.0\n            \n#         if f1 > best_f1: best_f1 = f1\n            \n#     return best_f1\n\n# def validate_detailed(model, loader, vocab_df, ia_weights, device):\n#     model.eval()\n#     all_preds, all_targets = [], []\n#     with torch.no_grad():\n#         for x, y in loader:\n#             x = x.to(device)\n#             with autocast():\n#                 logits = model(x)\n#             all_preds.append(torch.sigmoid(logits).cpu().numpy())\n#             all_targets.append(y.numpy())\n#     Y_p = np.vstack(all_preds)\n#     Y_t = np.vstack(all_targets)\n    \n#     scores = {}\n#     # L∆∞u √Ω: C99 c√≥ nhi·ªÅu nh√£n h∆°n, n√™n vi·ªác loop aspect v·∫´n ƒë√∫ng\n#     for aspect in ['MFO', 'BPO', 'CCO']:\n#         col_indices = vocab_df.index[vocab_df['aspect'] == aspect].tolist()\n#         if not col_indices: continue\n#         scores[aspect] = calculate_fmax_subset(Y_p[:, col_indices], Y_t[:, col_indices], ia_weights[col_indices])\n    \n#     avg_fmax = np.mean(list(scores.values()))\n#     return avg_fmax, scores\n\n# def main():\n#     print(f\"üöÄ START TRAINING C99 STANDARD (WIDE MLP)...\")\n    \n#     # 1. Load Vocab C99\n#     path = os.path.join(CONFIG['LABEL_DIR'], CONFIG['VOCAB_FILE'])\n#     if not os.path.exists(path): path = os.path.join(CONFIG['WORK_DIR'], CONFIG['VOCAB_FILE'])\n#     vocab_df = pd.read_csv(path)\n#     num_classes = len(vocab_df)\n#     print(f\"   Num Classes: {num_classes}\")\n    \n#     # Load IA Weights (Map theo Vocab C99)\n#     try:\n#         ia_df = pd.read_csv(CONFIG['IA_FILE'], sep='\\t', names=['term', 'ia'], header=None)\n#         ia_map = dict(zip(ia_df.term, ia_df.ia))\n#         ia_weights = np.array([ia_map.get(t, 1.0) for t in vocab_df.term.values])\n#     except: ia_weights = np.ones(num_classes)\n    \n#     # 2. Loaders\n#     train_ds = CAFA6Dataset(CONFIG['TRAIN_IDS'], CONFIG['TARGET_FILE'], CONFIG['EMBED_DIR'], num_classes)\n#     val_ds = CAFA6Dataset(CONFIG['VAL_IDS'], CONFIG['TARGET_FILE'], CONFIG['EMBED_DIR'], num_classes)\n#     train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\n#     val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size']*2, shuffle=False, num_workers=2)\n    \n#     # 3. Model\n#     model = WideProteinMLP(CONFIG['input_dim'], num_classes, CONFIG['hidden_dims'], CONFIG['dropout']).to(CONFIG['device'])\n#     if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n    \n#     # 4. Config T·ªëi ∆∞u cho C99 (GammaPos=1.0)\n#     # [QUAN TR·ªåNG]: TƒÉng gamma_pos l√™n 1.0 ƒë·ªÉ b·∫Øt nh√£n hi·∫øm (C99 nhi·ªÅu nh√£n hi·∫øm)\n#     criterion = AsymmetricLossOptimized(gamma_neg=2.0, gamma_pos=2.0, clip=0.05)\n    \n#     optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=CONFIG['lr'], steps_per_epoch=len(train_loader), epochs=CONFIG['epochs'])\n#     scaler = GradScaler()\n\n#     best_score = 0.0\n    \n#     # 5. Training Loop\n#     for epoch in range(CONFIG['epochs']):\n#         model.train()\n#         loss_sum = 0\n#         pbar = tqdm(train_loader, desc=f\"Ep {epoch+1}\", leave=False)\n        \n#         for x, y in pbar:\n#             x, y = x.to(CONFIG['device']), y.to(CONFIG['device'])\n#             optimizer.zero_grad()\n#             with autocast():::\n#                 logits = model(x)\n#                 loss = criterion(logits, y)\n            \n#             scaler.scale(loss).backward()\n#             scaler.step(optimizer)\n#             scaler.update()\n#             scheduler.step()\n#             loss_sum += loss.item()\n#             pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n            \n#         # Validate\n#         val_fmax, val_details = validate_detailed(model, val_loader, vocab_df, ia_weights, CONFIG['device'])\n#         print(f\"Epoch {epoch+1}: Loss={loss_sum/len(train_loader):.4f} | Val F-max={val_fmax:.4f} {val_details}\")\n        \n#         if val_fmax > best_score:\n#             best_score = val_fmax\n#             torch.save(model.state_dict(), \"best_model_c99_extreme.pth\")\n#             print(\"   üèÜ Saved Best C99 Model!\")\n\n#     print(f\"\\n‚úÖ DONE! Best F-max: {best_score:.4f}\")\n\n# if __name__ == \"__main__\":\n#     main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T13:27:48.669518Z","iopub.execute_input":"2025-12-09T13:27:48.669721Z","iopub.status.idle":"2025-12-09T13:27:48.685540Z","shell.execute_reply.started":"2025-12-09T13:27:48.669706Z","shell.execute_reply":"2025-12-09T13:27:48.685007Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# !pip install obonet networkx --quiet\n\n# import obonet\n# import networkx as nx\n# import pandas as pd\n# import numpy as np\n# import pickle\n# from collections import deque\n# import os\n\n# CONFIG = {\n#     'OBO_FILE': \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\",\n#     'VOCAB_FILE': \"/kaggle/input/c99-cafa6/vocab_C99_remove.csv\", \n#     'TRAIN_TARGETS': \"/kaggle/input/c99-cafa6/train_targets_C99.pkl\", # C·∫ßn cho t√≠nh IC\n#     'OUTPUT_PKL': \"hierarchy_metadata_C99.pkl\"\n# }\n\n# def build_training_metadata_full():\n#     print(\"üöÄ Building Training Metadata (Depth, IC & HMC Map)...\")\n    \n#     # 1. Load Graph & Vocab C95\n#     graph = obonet.read_obo(CONFIG['OBO_FILE'])\n#     vocab_df = pd.read_csv(CONFIG['VOCAB_FILE'])\n#     vocab_terms = vocab_df['term'].tolist()\n#     go_to_idx = {go: i for i, go in enumerate(vocab_terms)}\n#     n_classes = len(vocab_terms)\n    \n#     # ----------------------------------------------------\n#     # 2. T√çNH DEPTH (BFS) - Logic c·ªßa b·∫°n\n#     # ----------------------------------------------------\n#     roots = [\"GO:0008150\", \"GO:0003674\", \"GO:0005575\"]\n#     depth_dict = {node: float('inf') for node in graph.nodes()}\n#     queue = deque()\n    \n#     for root in roots:\n#         if root in graph:\n#             depth_dict[root] = 0\n#             queue.append(root)\n            \n#     while queue:\n#         node = queue.popleft()\n#         d = depth_dict[node]\n#         # S·ª≠ d·ª•ng 'is_a' v√† 'part_of' edges (ch·ªâ c·∫ßn l·∫•y edge predecessors)\n#         for child in graph.predecessors(node):\n#             if depth_dict[child] > d + 1:\n#                 depth_dict[child] = d + 1\n#                 queue.append(child)\n                \n#     # Map Depth v√†o Vocab C95\n#     depth_arr = np.array([depth_dict.get(t, 0) for t in vocab_terms], dtype=np.float32)\n#     depth_arr[depth_arr == float('inf')] = 0.0\n    \n#     # Normalize\n#     max_d = depth_arr.max()\n#     depth_norm = depth_arr / max_d if max_d > 0 else depth_arr\n#     print(f\"  ‚úÖ Depth Done. Max Depth: {max_d}\")\n    \n#     # ----------------------------------------------------\n#     # 3. T√çNH INFORMATION CONTENT (IC)\n#     # ----------------------------------------------------\n#     # Load targets ƒë·ªÉ t√≠nh t·∫ßn su·∫•t\n#     with open(CONFIG['TRAIN_TARGETS'], 'rb') as f:\n#         labels_dict = pickle.load(f)\n    \n#     # T√≠nh t·∫ßn su·∫•t (Count)\n#     term_counts = np.zeros(n_classes, dtype=np.int32)\n#     total_proteins = len(labels_dict)\n\n#     # T·ªïng h·ª£p nh√£n t·ª´ dictionary\n#     for pid, indices in labels_dict.items():\n#         # Ch·ªâ c·∫ßn ƒë·∫£m b·∫£o index kh√¥ng v∆∞·ª£t qu√° n_classes (ƒë√£ ƒë∆∞·ª£c l·ªçc C95)\n#         for idx in indices:\n#             if idx < n_classes:\n#                 term_counts[idx] += 1\n    \n#     # C√¥ng th·ª©c IC: IC = -log(P(t))\n#     term_prob = term_counts / total_proteins\n#     # Th√™m epsilon nh·ªè ƒë·ªÉ tr√°nh log(0)\n#     term_prob = np.clip(term_prob, 1e-8, 1.0)\n#     ic_arr = -np.log(term_prob)\n    \n#     # Normalize IC (Optional, nh∆∞ng an to√†n h∆°n)\n#     max_ic = ic_arr.max()\n#     ic_norm = ic_arr / max_ic if max_ic > 0 else ic_arr\n#     print(f\"  ‚úÖ IC Done. Max IC: {max_ic:.2f}\")\n\n#     # ----------------------------------------------------\n#     # 4. CHILD-TO-PARENT INDEX MAP (Cho HMC Loss)\n#     # ----------------------------------------------------\n#     child_to_parent_idx = {}\n    \n#     # Ch·ªâ duy·ªát qua c√°c GO Term C√ì trong vocab C95\n#     for term_go in tqdm(vocab_terms, desc=\"Building Parent Map\"):\n#         if term_go not in graph: continue\n        \n#         child_idx = go_to_idx[term_go]\n#         parent_indices = []\n        \n#         # predecessors = parents trong graph\n#         for parent_go in graph.successors(term_go):\n#             if parent_go in go_to_idx:\n#                 parent_indices.append(go_to_idx[parent_go])\n        \n#         if parent_indices:\n#             child_to_parent_idx[child_idx] = parent_indices\n\n#     print(f\"  ‚úÖ HMC Map Done. Total nodes mapped: {len(child_to_parent_idx):,}\")\n    \n#     # 5. Save\n#     metadata = {\n#         'depth_norm': depth_norm,\n#         'ic_norm': ic_norm,\n#         'child_to_parent_idx': child_to_parent_idx\n#     }\n#     with open(CONFIG['OUTPUT_PKL'], 'wb') as f:\n#         pickle.dump(metadata, f)\n        \n#     print(f\"\\n‚úÖ DONE! Saved all metadata to {CONFIG['OUTPUT_PKL']}\")\n\n# if __name__ == \"__main__\":\n#     # ƒê·∫£m b·∫£o file TRAIN_TARGETS ƒë√£ ƒë∆∞·ª£c mount ho·∫∑c t·∫°o ra\n#     if not os.path.exists(CONFIG['TRAIN_TARGETS']):\n#         print(f\"üö® ERROR: Missing {CONFIG['TRAIN_TARGETS']}. Cannot calculate IC.\")\n#     else:\n#         build_training_metadata_full()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T13:27:48.702016Z","iopub.execute_input":"2025-12-09T13:27:48.702451Z","iopub.status.idle":"2025-12-09T13:27:48.713530Z","shell.execute_reply.started":"2025-12-09T13:27:48.702433Z","shell.execute_reply":"2025-12-09T13:27:48.713001Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os, sys, math, time, pickle, gc\n\nimport numpy as np\n\nimport pandas as pd\n\nfrom tqdm import tqdm\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.optim as optim\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torch.amp import autocast, GradScaler\n\n\n# ============================================================================\n\n# 1. CONFIG\n\n# ============================================================================\n\nCONFIG = {\n\n    # --- Input Embeddings ---\n    \"EMBED_DIR\": \"/kaggle/input/cafa6-embeds\",\n\n    # --- Input Labels & Metadata (Dataset c95-cafa6) ---\n    \"WORK_DIR\": \"/kaggle/working\",\n    \n    \"LABEL_DIR\": \"/kaggle/input/c99-cafa6\",\n\n    'VOCAB_FILE': \"vocab_C99_remove.csv\",\n    'TARGET_FILE': \"train_targets_C99.pkl\",\n    'TRAIN_IDS': \"train_ids_C99_split.npy\",\n    'VAL_IDS': \"val_ids_C99_split.npy\",\n    \n    \"IA_FILE\": \"/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\",\n\n    'TAXON_PKL': \"/kaggle/input/cafa6-embeds/taxon_mapping_K140.pkl\",\n\n    \"PRETRAINED_PATH\": \"/kaggle/input/model-cafa6/best_model_c99_taxon.pth\",\n\n    # --- Model Params ---\n    \"input_dim\": 1280,\n    \"hidden_dims\": [2048, 4096],\n    \"dropout\": 0.3,\n    'taxon_embed_dim': 64,\n    \n    \"batch_size\": 16,\n    \"lr\": 5e-5,\n    \"weight_decay\": 1e-5,\n    \"epochs\": 10,\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n}\n\n# ============================================================================\n# 2. MODEL M·ªöI (C√ì TAXONOMY)\n# ============================================================================\n\nclass WideProteinMLP_WithTaxon(nn.Module):\n    def __init__(self, input_dim, num_classes, num_taxa, taxon_dim=64, hidden_dims=[2048, 4096], dropout=0.3):\n        super().__init__()\n        \n        # 1. Nh√°nh Protein (ESM-2) - Chu·∫©n h√≥a ƒë·∫ßu v√†o\n        self.bn_input = nn.LayerNorm(input_dim)\n        \n        # 2. Nh√°nh Taxonomy - Embedding h·ªçc ƒë∆∞·ª£c\n        self.taxon_embedding = nn.Embedding(num_taxa, taxon_dim)\n        self.taxon_norm = nn.LayerNorm(taxon_dim)  \n\n        self.unk_idx = num_taxa - 1\n\n        with torch.no_grad():\n            self.taxon_embedding.weight[self.unk_idx].zero_()\n        \n        self.register_buffer(\n            \"unk_fixed_vector\",\n            self.taxon_embedding.weight[self.unk_idx].clone()\n        )\n        \n        # 3. T·ªïng h·ª£p (Concat)\n        combined_dim = input_dim + taxon_dim\n        \n        layers = []\n        prev = combined_dim\n        for h in hidden_dims:\n            layers.append(nn.Linear(prev, h))\n            layers.append(nn.GELU())\n            layers.append(nn.Dropout(dropout))\n            prev = h\n            \n        layers.append(nn.Linear(prev, num_classes))\n        self.net = nn.Sequential(*layers)\n        \n    def forward(self, x_seq, x_tax):\n        # x_seq: [batch, 1280]\n        # x_tax: [batch] (Int IDs)\n        \n        feat_seq = self.bn_input(x_seq)\n        feat_tax = self.taxon_embedding(x_tax)\n        feat_tax = self.taxon_norm(feat_tax)  \n        \n        # N·ªëi l·∫°i: [Batch, 1280 + 64]\n        combined = torch.cat([feat_seq, feat_tax], dim=1)\n        \n        return self.net(combined)\n\n# ============================================================================\n# 3. DATASET C·∫¨P NH·∫¨T (LOAD TAXON)\n# ============================================================================\nclass CAFA6Dataset(Dataset):\n    def __init__(self, ids_file, targets_file, embed_dir, num_classes, taxon_pkl):\n        # Load ID & Labels nh∆∞ c≈©\n        path = os.path.join(CONFIG['LABEL_DIR'], ids_file)\n        if not os.path.exists(path): path = os.path.join(CONFIG['WORK_DIR'], ids_file)\n        self.ids = np.load(path)\n        \n        t_path = os.path.join(CONFIG['LABEL_DIR'], targets_file)\n        if not os.path.exists(t_path): t_path = os.path.join(CONFIG['WORK_DIR'], targets_file)\n        with open(t_path, 'rb') as f: self.labels_dict = pickle.load(f)\n            \n        self.num_classes = num_classes\n        self.id_to_embed_idx = {}\n        with open(os.path.join(embed_dir, \"train_ids.txt\"), 'r') as f:\n            for idx, line in enumerate(f): self.id_to_embed_idx[line.strip()] = idx\n        self.embed_matrix = np.load(os.path.join(embed_dir, \"train_embeds.npy\"), mmap_mode='r')\n        \n        # [M·ªöI] Load Taxon Mapping\n        # T√¨m file ·ªü work dir ho·∫∑c input dir\n        \n        if os.path.exists(taxon_pkl):\n            tax_path = taxon_pkl\n        else:\n            tax_path = os.path.join(CONFIG['WORK_DIR'], os.path.basename(taxon_pkl))\n            print(\"‚ö†Ô∏è USING TAXON PKL FROM WORK_DIR:\", tax_path)\n            \n        with open(tax_path, 'rb') as f:\n            tax_data = pickle.load(f)\n        \n        self.prot_to_taxon = tax_data['prot_to_taxon_idx'] # Key trong file pkl c·ªßa b·∫°n\n        self.default_tax = tax_data['num_taxa_classes'] - 1 # L·ªõp UNK (Index cu·ªëi c√πng)\n\n    def __len__(self): return len(self.ids)\n\n    def __getitem__(self, idx):\n        prot_id = self.ids[idx]\n        \n        # 1. Embed\n        embed_idx = self.id_to_embed_idx.get(prot_id)\n        if embed_idx is None:\n            feat = torch.zeros(CONFIG[\"input_dim\"], dtype=torch.float32)\n        else:\n            feat = torch.from_numpy(self.embed_matrix[embed_idx].copy()).float()\n        \n        # 2. Target\n        target = torch.zeros(self.num_classes, dtype=torch.float)\n        indices = self.labels_dict.get(prot_id, [])\n        if len(indices) > 0: target[indices] = 1.0\n            \n        # 3. [M·ªöI] Taxon ID\n        # L·∫•y Taxon Index, n·∫øu kh√¥ng c√≥ th√¨ tr·∫£ v·ªÅ default (UNK)\n        taxon_idx = self.prot_to_taxon.get(prot_id, self.default_tax)\n        \n        # Tr·∫£ v·ªÅ 3 gi√° tr·ªã\n        return feat, torch.tensor(taxon_idx, dtype=torch.long), target\n\n\n# ============================================================================\n\n# 4. LOSS: ASL OPTIMIZED \n\n# ============================================================================\n\n\nclass AsymmetricLossOptimized(nn.Module):\n    def __init__(self, gamma_neg=4, gamma_pos=0, clip=0.05, eps=1e-8):\n        super().__init__()\n        self.gamma_neg = gamma_neg\n        self.gamma_pos = gamma_pos\n        self.clip = clip\n        self.eps = eps\n    def forward(self, x, y):\n        x_sigmoid = torch.sigmoid(x)\n        xs_pos = x_sigmoid\n        xs_neg = 1 - x_sigmoid\n        if self.clip > 0: xs_neg = (xs_neg + self.clip).clamp(max=1)\n        pt = y * xs_pos + (1 - y) * xs_neg\n        log_pt = torch.log(pt.clamp(min=self.eps))\n        pos_weight = (1 - xs_pos) ** self.gamma_pos\n        neg_weight = (1 - xs_neg) ** self.gamma_neg\n        weighted_loss = - (pos_weight * log_pt * y + neg_weight * log_pt * (1-y))\n        return weighted_loss.sum() / x.size(0)\n\n# ============================================================================\n\n# 5. METRIC & TRAINING LOOP\n\n# ============================================================================\n\n\ndef calculate_fmax_subset(preds, targets, ia_weights):\n    w = ia_weights.reshape(1, -1)\n    true_sum = np.sum(targets * w, axis=1)\n    valid_mask = true_sum > 0\n    if valid_mask.sum() == 0: return 0.0\n    p_sub = preds[valid_mask]; t_sub = targets[valid_mask]; w_sub = w; w_true_sub = true_sum[valid_mask]\n    best_f1 = 0.0\n    thresholds = np.linspace(0.0, 1.0, 51) \n    for tau in thresholds:\n        cut = (p_sub >= tau).astype(int)\n        tp = np.sum((cut * t_sub) * w_sub, axis=1)\n        pred_sum = np.sum(cut * w_sub, axis=1)\n        prec = np.divide(tp, pred_sum, out=np.zeros_like(tp), where=pred_sum!=0)\n        rec = np.divide(tp, w_true_sub, out=np.zeros_like(tp), where=w_true_sub!=0)\n        avg_p = np.mean(prec); avg_r = np.mean(rec)\n        if (avg_p + avg_r) > 0: f1 = 2 * avg_p * avg_r / (avg_p + avg_r)\n        else: f1 = 0.0\n        if f1 > best_f1: best_f1 = f1\n    return best_f1\n\ndef validate_with_loss(model, loader, vocab_df, ia_weights, device, criterion):\n    model.eval()\n    all_preds, all_targets = [], []\n    val_loss_sum = 0.0\n    n_batches = 0\n\n    with torch.no_grad():\n        for x_seq, x_tax, y in loader:\n            x_seq = x_seq.to(device)\n            x_tax = x_tax.to(device)\n            y = y.to(device)\n\n            with autocast(device_type=\"cuda\"):\n                logits = model(x_seq, x_tax)\n                loss = criterion(logits, y)\n\n            val_loss_sum += loss.item()\n            n_batches += 1\n\n            all_preds.append(torch.sigmoid(logits).cpu().numpy())\n            all_targets.append(y.cpu().numpy())\n\n    val_loss = val_loss_sum / n_batches\n\n    Y_p = np.vstack(all_preds)\n    Y_t = np.vstack(all_targets)\n\n    scores = {}\n    for aspect in ['MFO', 'BPO', 'CCO']:\n        col_indices = vocab_df.index[vocab_df['aspect'] == aspect].tolist()\n        if not col_indices:\n            continue\n        scores[aspect] = calculate_fmax_subset(\n            Y_p[:, col_indices], \n            Y_t[:, col_indices], \n            ia_weights[col_indices]\n        )\n\n    avg_fmax = np.mean(list(scores.values()))\n    return val_loss, avg_fmax, scores\n\n\n# ============================================================================\n# 5. RESUME TRAINING LOOP (ƒê√£ s·ª≠a l·∫°i)\n# ============================================================================\ndef resume_c99_taxon():\n    print(\"üöÄ RESUMING C99 TAXON (FINE-TUNE 10 EPOCHS)...\")\n    \n    # 1. Load Resources\n    vocab_df = pd.read_csv(os.path.join(CONFIG['LABEL_DIR'], CONFIG['VOCAB_FILE']))\n    num_classes = len(vocab_df)\n    try:\n        ia_df = pd.read_csv(CONFIG['IA_FILE'], sep='\\t', names=['term', 'ia'], header=None)\n        ia_map = dict(zip(ia_df.term, ia_df.ia))\n        ia_weights = np.array([ia_map.get(t, 1.0) for t in vocab_df.term.values])\n    except: ia_weights = np.ones(num_classes)\n    \n    tax_pkl_path = CONFIG['TAXON_PKL'] if os.path.exists(CONFIG['TAXON_PKL']) else os.path.join(CONFIG['WORK_DIR'], os.path.basename(CONFIG['TAXON_PKL']))\n    with open(tax_pkl_path, 'rb') as f: tax_data = pickle.load(f)\n    num_taxa = tax_data['num_taxa_classes']\n    print(f\"   Num Taxa Classes: {num_taxa}\")\n\n    # 2. Dataset & Loader\n    train_ds = CAFA6Dataset(CONFIG['TRAIN_IDS'], CONFIG['TARGET_FILE'], CONFIG['EMBED_DIR'], num_classes, CONFIG['TAXON_PKL'])\n    val_ds = CAFA6Dataset(CONFIG['VAL_IDS'], CONFIG['TARGET_FILE'], CONFIG['EMBED_DIR'], num_classes, CONFIG['TAXON_PKL'])\n    train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size']*2, shuffle=False, num_workers=2)\n    \n    # 3. Init Model & Load Weights\n    model = WideProteinMLP_WithTaxon(\n        input_dim=CONFIG['input_dim'], \n        num_classes=num_classes, \n        num_taxa=num_taxa,\n        taxon_dim=CONFIG['taxon_embed_dim'],\n        hidden_dims=CONFIG['hidden_dims'], \n        dropout=CONFIG['dropout']\n    ).to(CONFIG['device'])\n    \n    # --- LOAD WEIGHTS T·ª™ EPOCH 15 ---\n    print(f\"üì• Loading weights from: {CONFIG['PRETRAINED_PATH']}\")\n    if os.path.exists(CONFIG['PRETRAINED_PATH']):\n        state_dict = torch.load(CONFIG['PRETRAINED_PATH'], map_location=CONFIG['device'])\n        # Fix l·ªói module n·∫øu train b·∫±ng DataParallel tr∆∞·ªõc ƒë√≥\n        new_state_dict = {}\n        for k, v in state_dict.items():\n            if k.startswith(\"module.\"): new_state_dict[k[7:]] = v\n            else: new_state_dict[k] = v\n        model.load_state_dict(new_state_dict)\n        print(\"‚úÖ Weights loaded successfully!\")\n    else:\n        print(f\"‚ùå ERROR: Kh√¥ng t√¨m th·∫•y file {CONFIG['PRETRAINED_PATH']}!\")\n        return\n\n    if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n    \n    # 4. Setup Optimizer M·ªõi (LR nh·ªè + Cosine Decay)\n    criterion = AsymmetricLossOptimized(gamma_neg=3.0, gamma_pos=1.2, clip=0.05)\n    \n    # Reset Optimizer v·ªõi LR nh·ªè\n    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n    \n    # D√πng CosineAnnealing ƒë·ªÉ h·∫° c√°nh m·ªÅm\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'], eta_min=1e-6)\n    scaler = GradScaler(\"cuda\") if torch.cuda.is_available() else GradScaler()\n\n    best_score = 0.5637 # ƒêi·ªÉm epoch 15 c≈© c·ªßa b·∫°n\n    \n    # 5. Training Loop\n    for epoch in range(CONFIG['epochs']):\n        real_epoch = 15 + epoch + 1 # ƒê·ªÉ log hi·ªÉn th·ªã ƒë√∫ng epoch 16, 17...\n        \n        model.train()\n        loss_sum = 0\n        pbar = tqdm(train_loader, desc=f\"Ep {real_epoch}\", leave=False)\n        \n        for x_seq, x_tax, y in pbar:\n            x_seq = x_seq.to(CONFIG['device'])\n            x_tax = x_tax.to(CONFIG['device'])\n            y = y.to(CONFIG['device'])\n            \n            optimizer.zero_grad()\n            with autocast(device_type=\"cuda\"):\n                logits = model(x_seq, x_tax)\n                loss = criterion(logits, y)\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            # ‚úÖ LOGIC GI·ªÆ NGUY√äN: CH·∫∂N UNK H·ªåC L·∫†I\n            with torch.no_grad():\n                if isinstance(model, nn.DataParallel):\n                    emb = model.module.taxon_embedding\n                    unk_idx = model.module.unk_idx\n                    unk_vec = model.module.unk_fixed_vector\n                else:\n                    emb = model.taxon_embedding\n                    unk_idx = model.unk_idx\n                    unk_vec = model.unk_fixed_vector\n                emb.weight[unk_idx].copy_(unk_vec)\n            \n            # Scheduler Step theo Epoch (v·ªõi CosineAnnealing), KH√îNG step trong batch\n            \n            loss_sum += loss.item()\n            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n\n        # Step scheduler cu·ªëi epoch\n        scheduler.step()\n        \n        # Validate m·ªói epoch (v√¨ ƒëang giai ƒëo·∫°n cu·ªëi quan tr·ªçng)\n        val_loss, val_fmax, val_details = validate_with_loss(\n            model, val_loader, vocab_df, ia_weights, CONFIG['device'], criterion\n        )\n        \n        print(\n            f\"Epoch {real_epoch}: \"\n            f\"Train Loss={loss_sum/len(train_loader):.4f} | \"\n            f\"Val Loss={val_loss:.4f} | \"\n            f\"Val F-max={val_fmax:.4f} {val_details}\"\n        )\n        \n        if val_fmax > best_score:\n            best_score = val_fmax\n            torch.save(model.state_dict(), \"best_model_c99_taxon_final.pth\")\n            print(\"   üèÜ Saved New Best Model!\")\n        \n        # Lu√¥n save checkpoint cu·ªëi c√πng\n        torch.save(model.state_dict(), \"last_checkpoint.pth\")\n\nif __name__ == \"__main__\":\n    resume_c99_taxon()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T19:31:50.264674Z","iopub.execute_input":"2025-12-09T19:31:50.265321Z","execution_failed":"2025-12-09T20:55:17.926Z"}},"outputs":[{"name":"stdout","text":"üöÄ RESUMING C99 TAXON (FINE-TUNE 10 EPOCHS)...\n   Num Taxa Classes: 141\nüì• Loading weights from: /kaggle/input/model-cafa6/best_model_c99_taxon.pth\n‚úÖ Weights loaded successfully!\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=20.3428 | Val Loss=36.2146 | Val F-max=0.5757 {'MFO': 0.6444812960287448, 'BPO': 0.4576959253703782, 'CCO': 0.6247965562551481}\n   üèÜ Saved New Best Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=19.7739 | Val Loss=36.2949 | Val F-max=0.5762 {'MFO': 0.6451744691688489, 'BPO': 0.45844852996905644, 'CCO': 0.6250407320894099}\n   üèÜ Saved New Best Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=19.1909 | Val Loss=31.5091 | Val F-max=0.5772 {'MFO': 0.6459181905668602, 'BPO': 0.4605007760652698, 'CCO': 0.6250537249918373}\n   üèÜ Saved New Best Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=18.3193 | Val Loss=31.8570 | Val F-max=0.5777 {'MFO': 0.64491136375324, 'BPO': 0.4618140515001747, 'CCO': 0.6262926421429607}\n   üèÜ Saved New Best Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=17.6164 | Val Loss=32.1166 | Val F-max=0.5776 {'MFO': 0.6442429224908471, 'BPO': 0.46125390188498583, 'CCO': 0.6271988407147208}\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=16.9935 | Val Loss=32.1445 | Val F-max=0.5783 {'MFO': 0.6461223303097027, 'BPO': 0.46286250282369623, 'CCO': 0.6258642920640908}\n   üèÜ Saved New Best Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=16.7039 | Val Loss=32.3557 | Val F-max=0.5790 {'MFO': 0.6467151929842894, 'BPO': 0.46332864905447296, 'CCO': 0.6268752261281997}\n   üèÜ Saved New Best Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=16.4604 | Val Loss=32.4664 | Val F-max=0.5795 {'MFO': 0.6472463836202468, 'BPO': 0.463818548497588, 'CCO': 0.6275731062879047}\n   üèÜ Saved New Best Model!\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=16.3439 | Val Loss=32.5050 | Val F-max=0.5792 {'MFO': 0.6468590678372205, 'BPO': 0.46410387635460354, 'CCO': 0.6267588582664426}\n","output_type":"stream"},{"name":"stderr","text":"Ep 25:  12%|‚ñà‚ñè        | 551/4636 [00:53<06:37, 10.28it/s, loss=20.1166]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}